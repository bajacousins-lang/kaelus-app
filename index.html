import os
import re
import time
import json
import html
import threading
import logging
import requests
import sys
import unicodedata
from typing import Optional, List
from datetime import datetime, timezone, timedelta, date
from difflib import SequenceMatcher
from hashlib import sha1
from urllib.parse import quote_plus, urlparse
from zoneinfo import ZoneInfo

from flask import Flask, jsonify, request, Response
from flask_cors import CORS
from bs4 import BeautifulSoup
from pymongo import MongoClient, ReturnDocument
import telebot

# ============================================================
#  BCNTV: Bot + API (gunicorn) + Worker (polling + sync)
#  - API: gunicorn bot:app
#  - Worker: python bot.py --worker (polling + sync)
# ============================================================

# ---------------- REGEX (parsing Kaelus) ----------------
# Nota: se definen aqu√≠ para evitar NameError y estandarizar extracci√≥n.
RX_JORNADA = re.compile(r"(?:jornada|fecha)\s*#?\s*(\d+)", re.I)
RX_RONDA = re.compile(r"(?:evento|ronda|round)\s*#?\s*(\d+)", re.I)

# Hora (deportes): requiere am/pm para evitar capturar cosas como "Jornada#26"
RX_TIME_AMPM = re.compile(r"\b(\d{1,2})(?::(\d{2}))?\s*(a\.?m\.?|p\.?m\.?|am|pm)\b", re.I)

# Regex alias: usado para buscar horas en heur√≠sticas (venue/limpieza)
RX_TIME_ANY = RX_TIME_AMPM

# Hora simple legacy: 2, 2:45, 02:45 am/pm, 2 pm, etc. (mantener para otras secciones)
RX_TIME = re.compile(r"\b(\d{1,2})(?::(\d{2}))?\s*(a\.?m\.?|p\.?m\.?|am|pm)?\b", re.I)

# Zonas: Este/Centro/Pac√≠fico + abreviaciones
RX_TZ = re.compile(r"\b(este|centro|pac[i√≠]fico|pacifico|et|ct|pt|eastern|central|pacific)\b", re.I)

# Fechas en espa√±ol
RX_DATE_ES = re.compile(
    r"\b(\d{1,2})\s+de\s+(enero|febrero|marzo|abril|mayo|junio|julio|agosto|septiembre|setiembre|octubre|noviembre|diciembre)\s+de\s+(\d{4})\b",
    re.I,
)
RX_DATE_CAPS = re.compile(
    r"\b(\d{1,2})\s+DE\s+(ENERO|FEBRERO|MARZO|ABRIL|MAYO|JUNIO|JULIO|AGOSTO|SEPTIEMBRE|SETIEMBRE|OCTUBRE|NOVIEMBRE|DICIEMBRE)\s+DE\s+(\d{4})\b",
    re.I,
)

# Series (Temporada(s) / Episodio(s))
# Nota: la fuente puede traer "Temporadas 1 a 3" (plural + rango), no solo "Temporada 1".
# Adem√°s, a veces hay errores tipogr√°ficos (ej. "Temproada", "Temprada"),
# y tambi√©n typos en "episodio" (ej. "epiosdio").
RX_SERIE = re.compile(
    r"\bTem(?:porad|proad|prad)(?:a|as)\s*\d+(?:\s*(?:a|y|,|\-)\s*\d+)*"
    r"(?:\s*(?:episodios?|episodio|epiosdio|epiosdios|cap[i√≠]tulos?|cap[i√≠]tulo)\s*\d+(?:\s*(?:a|y|,|\-|\s)\s*\d+)*)?\b",
    re.I,
)

# A√±o / calidad
RX_YEAR = re.compile(r"\b(19\d{2}|20\d{2})\b")
RX_QUALITY = re.compile(r"\b(?:480p|720p|1080p|2160p|4k|8k|hdr|sd|hd|fhd|uhd)\b", re.I)

WORKER_MODE = "--worker" in sys.argv

logging.basicConfig(level=logging.INFO)
log = logging.getLogger("bcntv")

# ---------------- CONFIG ----------------
TOKEN = os.getenv("TELEGRAM_TOKEN")
MONGO_URI = os.getenv("MONGO_URI") or os.getenv("MONGO_URL")
ADMIN_ID = int((os.getenv("ADMIN_ID") or "0").strip() or "0")

SCRAP_URL = (os.getenv("SCRAP_URL") or os.getenv("SOURCE_URL") or "https://www.kaelustvsoporte.com").strip().rstrip("/")
WEBAPP_URL = (os.getenv("WEBAPP_URL") or os.getenv("URL_WEBAPP") or "https://bajacousins-lang.github.io/kaelus-app/").strip()
WEBAPP_URL = WEBAPP_URL.rstrip("/") + "/"

ASSETS_BASE_URL = os.getenv("ASSETS_BASE_URL", WEBAPP_URL).rstrip("/")
LOGO_URL = os.getenv("LOGO_URL", f"{ASSETS_BASE_URL}/logo.png")

SYNC_SECONDS = int(os.getenv("SYNC_SECONDS", "900"))
BROADCAST_SLEEP = float(os.getenv("BROADCAST_SLEEP", "0.15"))
API_KEY = os.getenv("API_KEY", "").strip()
API_PROTECT_LINKS = os.getenv("API_PROTECT_LINKS", "0").strip() == "1"

AI_MATCH_THRESHOLD = float(os.getenv("AI_MATCH_THRESHOLD", "0.86"))
AI_FUZZY_CACHE_MAX = int(os.getenv("AI_FUZZY_CACHE_MAX", "512"))

# Zuplo posters
POSTER_API_URL = (os.getenv("POSTER_API_URL") or "").strip().rstrip("/")
POSTER_API_KEY = (os.getenv("ZUPLO1_API_KEY") or os.getenv("POSTER_API_KEY") or os.getenv("TMDB_API_KEY") or "").strip()  # Zuplo API Key
POSTER_TIMEOUT = int(os.getenv("POSTER_TIMEOUT", "6"))

# Live rules (seg√∫n lo definido)
LIVE_DURATION_NORMAL_MIN = int(os.getenv("LIVE_DURATION_NORMAL_MIN", "130"))  # 2h 10m
LIVE_DURATION_COMBAT_MIN = int(os.getenv("LIVE_DURATION_COMBAT_MIN", "180"))  # 3h (UFC/Box/etc)

# Timezone user default (para chat)
USER_TZ = os.getenv("USER_TZ", "America/Tijuana")

# ---------------- WEBHOOK MODE (opcional, evita 409 sin rotar token) ----------------
# Si USE_WEBHOOK=1, el bot NO usa getUpdates (polling) y recibe updates por webhook
# en /telegram_webhook/<secret>. Esto evita 409 incluso si existe otro poller viejo.
USE_WEBHOOK = os.getenv('USE_WEBHOOK', '0').strip() == '1'
PUBLIC_URL = (os.getenv('PUBLIC_URL') or os.getenv('RENDER_EXTERNAL_URL') or '').strip().rstrip('/')
TG_WEBHOOK_SECRET = (os.getenv('TG_WEBHOOK_SECRET') or os.getenv('WEBHOOK_SECRET') or '').strip()
if not TG_WEBHOOK_SECRET and TOKEN:
    TG_WEBHOOK_SECRET = sha1(TOKEN.encode('utf-8')).hexdigest()[:16]


# Debug seguro
log.warning(
    f"[BOOT] pid={os.getpid()} WORKER_MODE={WORKER_MODE} "
    f"TOKEN_present={bool(TOKEN)} token_len={len(TOKEN) if TOKEN else None} "
    f"POSTER_API_URL={POSTER_API_URL or '(empty)'} KEY_present={bool(POSTER_API_KEY)} "
    f"SCRAP_URL={SCRAP_URL} WEBAPP_URL={WEBAPP_URL}"
)

if WORKER_MODE and not TOKEN:
    raise RuntimeError("Falta TELEGRAM_TOKEN (worker)")

if not MONGO_URI:
    raise RuntimeError("Falta MONGO_URI/MONGO_URL")

# ---------------- INIT DB ----------------
client = MongoClient(MONGO_URI)
db = client["bcntv_database"]

users_col = db["usuarios"]
cache_col = db["cache_contenido"]

# solicitudes VIP (nuevo, NO elimina nada)
vip_req_col = db["vip_requests"]

# Memoria incremental (mejora #2)
mem_col = db["ai_memory"]  # 1 doc con bad_phrases, league_aliases, channel_aliases

# ============================================================
# FIX ANTI-409 GLOBAL (NUEVO, NO BORRA NADA)
# - Evita 409 cuando Render corre 2 instancias a la vez (deploy solapado)
# - Solo bloquea el POLLING de Telegram; el sync sigue igual
# ============================================================
locks_col = db["locks"]
GLOBAL_LOCK_TTL = int(os.getenv("GLOBAL_LOCK_TTL", "120"))  # segundos
# Si el lock no se puede adquirir (DB moment√°neamente no disponible, race, etc.),
# por defecto FALLA CERRADO (no hace polling) para evitar 409.
# Si necesitas "fail-open" temporalmente, setea GLOBAL_LOCK_FAIL_OPEN=1.
GLOBAL_LOCK_FAIL_OPEN = os.getenv("GLOBAL_LOCK_FAIL_OPEN", "0").strip() == "1"
_global_lock_owner = None


def _global_owner_id() -> str:
    # Identificador √∫nico por proceso/instancia
    return f"{os.getenv('RENDER_INSTANCE_ID', 'local')}-{os.getpid()}-{int(time.time())}"


def acquire_global_polling_lock(ttl_seconds: int | None = None) -> bool:
    """
    Lock global en Mongo para que solo 1 instancia haga polling (evita 409).
    NO elimina ninguna funci√≥n existente; solo a√±ade protecci√≥n extra.
    """
    global _global_lock_owner
    ttl = ttl_seconds or GLOBAL_LOCK_TTL
    owner = _global_owner_id()
    now = datetime.now(timezone.utc)
    exp = now + timedelta(seconds=ttl)

    try:
        from pymongo.errors import DuplicateKeyError

        doc = locks_col.find_one_and_update(
            {
                "_id": "telegram_polling",
                "$or": [
                    {"expires_at": {"$lte": now}},
                    {"expires_at": {"$exists": False}},
                    {"owner": owner},
                ]
            },
            {"$set": {"owner": owner, "expires_at": exp, "ts": now}},
            upsert=True,
            return_document=ReturnDocument.AFTER
        )
        if doc and doc.get("owner") == owner:
            _global_lock_owner = owner
            log.warning(f"‚úÖ GLOBAL LOCK adquirido owner={owner} ttl={ttl}s")
            return True

        log.error(f"‚ö†Ô∏è GLOBAL LOCK NO adquirido. owner_actual={doc.get('owner') if doc else None}")
        return False

    except DuplicateKeyError:
        # Race t√≠pico cuando el doc no exist√≠a y 2 instancias intentan upsert al mismo tiempo.
        # En este caso, NO hacemos polling: otra instancia gan√≥.
        try:
            cur = locks_col.find_one({"_id": "telegram_polling"}) or {}
            log.error(f"‚ö†Ô∏è GLOBAL LOCK race (duplicate key). owner_actual={cur.get('owner')} expires_at={cur.get('expires_at')}")
        except Exception:
            pass
        return False

    except Exception as e:
        # Por defecto, falla cerrado para evitar 409.
        log.warning(f"‚ö†Ô∏è No se pudo adquirir GLOBAL LOCK: {e}. fail_open={GLOBAL_LOCK_FAIL_OPEN}")
        return True if GLOBAL_LOCK_FAIL_OPEN else False


def keepalive_global_polling_lock(ttl_seconds: int | None = None):
    """
    Renueva el lock global mientras el proceso siga vivo.
    """
    global _global_lock_owner
    ttl = ttl_seconds or GLOBAL_LOCK_TTL
    if not _global_lock_owner:
        return
    while True:
        try:
            now = datetime.now(timezone.utc)
            exp = now + timedelta(seconds=ttl)
            locks_col.update_one(
                {"_id": "telegram_polling", "owner": _global_lock_owner},
                {"$set": {"expires_at": exp, "ts": now}}
            )
        except Exception as e:
            log.warning(f"Keepalive GLOBAL LOCK fall√≥: {e}")
        time.sleep(max(20, ttl // 2))


# ---------------- BOT + APP ----------------
bot = telebot.TeleBot(TOKEN) if TOKEN else None

app = Flask(__name__)
CORS(app, resources={r"/api/*": {"origins": "*"}}, allow_headers=["Content-Type", "x-api-key", "X-Api-Key"])

# ---------------- TELEGRAM WEBHOOK (opcional) ----------------
@app.route('/telegram_webhook/<secret>', methods=['POST'])
def telegram_webhook(secret):
    """Recibe updates de Telegram cuando USE_WEBHOOK=1.

    Seguridad: la ruta incluye un secret (TG_WEBHOOK_SECRET).
    Nota: usamos el cuerpo crudo (request.get_data) para m√°xima compatibilidad
    con PyTelegramBotAPI en modo webhook.
    """
    if not bot or not TOKEN:
        return 'bot not ready', 503
    if secret != TG_WEBHOOK_SECRET:
        return 'forbidden', 403

    try:
        raw = request.get_data(cache=False, as_text=True)
        if not raw:
            return 'bad request', 400

        from telebot import types
        update = types.Update.de_json(raw)

        # Logs √∫tiles para depuraci√≥n
        try:
            if update and getattr(update, 'message', None) and update.message and update.message.text:
                log.warning(f"[WEBHOOK] msg uid={update.message.from_user.id} chat={update.message.chat.id} text={update.message.text!r}")
            elif update and getattr(update, 'callback_query', None) and update.callback_query:
                log.warning(f"[WEBHOOK] cb uid={update.callback_query.from_user.id} data={update.callback_query.data!r}")
        except Exception:
            pass

        if update:
            # En algunos entornos (gunicorn + webhook) PyTelegramBotAPI puede
            # no disparar handlers con process_new_updates de forma consistente.
            # Para asegurar que /start y los botones siempre respondan,
            # hacemos un dispatch manual para los casos cr√≠ticos.
            handled = False
            try:
                from telebot import types as _t

                # ---- Mensajes de texto (botones / comandos) ----
                if getattr(update, 'message', None) and update.message and getattr(update.message, 'text', None):
                    txt = update.message.text or ""
                    n = _normalize_btn_text(txt)

                    # Comandos
                    if txt.strip().startswith('/start') and 'cmd_start' in globals():
                        globals()['cmd_start'](update.message)
                        handled = True

                    # Botones principales
                    elif n in (_normalize_btn_text('‚öΩ DEPORTES'), 'deportes') and 'deportes' in globals():
                        globals()['deportes'](update.message)
                        handled = True
                    elif n in (_normalize_btn_text('üé¨ PEL√çCULAS'), 'peliculas', 'pel√≠culas') and 'peliculas' in globals():
                        globals()['peliculas'](update.message)
                        handled = True
                    elif n in (_normalize_btn_text('üì∫ SERIES'), 'series') and 'series' in globals():
                        globals()['series'](update.message)
                        handled = True
                    elif n in (_normalize_btn_text('üèÜ WEBPREMIUM'), 'webpremium', 'web premium', 'webapp') and 'webpremium' in globals():
                        globals()['webpremium'](update.message)
                        handled = True
                    elif n in (_normalize_btn_text('üë§ MI ESTADO'), 'mi estado', 'estado') and 'mi_estado' in globals():
                        globals()['mi_estado'](update.message)
                        handled = True
                    elif n in (_normalize_btn_text('üìò MANUAL'), 'manual') and 'manual_usuario' in globals():
                        globals()['manual_usuario'](update.message)
                        handled = True
                    elif n in (_normalize_btn_text('üßæ PENDIENTES VIP'), 'pendientes vip') and 'pendientes_vip' in globals():
                        globals()['pendientes_vip'](update.message)
                        handled = True
                    elif n == _normalize_btn_text('üì¢ DIFUSI√ìN') and 'difusion_help' in globals():
                        globals()['difusion_help'](update.message)
                        handled = True

                # ---- Callback queries (botones inline) ----
                if (not handled) and getattr(update, 'callback_query', None) and update.callback_query and 'callbacks' in globals():
                    # Normalizamos a CallbackQuery si viniera como dict (defensivo)
                    c = update.callback_query
                    if isinstance(c, dict):
                        c = _t.CallbackQuery.de_json(c)
                    globals()['callbacks'](c)
                    handled = True

            except Exception as e:
                log.exception(f"Manual dispatch error: {e}")
                handled = True  # evitamos doble procesamiento

            # Fallback: deja que TeleBot procese lo dem√°s
            if not handled:
                bot.process_new_updates([update])
    except Exception as e:
        # Importante: devolvemos 200 para que Telegram no reintente infinitamente.
        log.exception(f"Webhook update error: {e}")
        return 'OK', 200

    return 'OK', 200

# ---------------- HELPERS ----------------
SESSION = requests.Session()

POSTER_CACHE = {}
POSTER_CACHE_TTL = int(os.getenv("POSTER_CACHE_TTL", "21600"))
POSTER_CACHE_MAX = int(os.getenv("POSTER_CACHE_MAX", "400"))
AI_FUZZY_CACHE = {}


def _ai_cache_get(key):
    return AI_FUZZY_CACHE.get(key)


def _ai_cache_set(key, value):
    if len(AI_FUZZY_CACHE) >= AI_FUZZY_CACHE_MAX:
        first_key = next(iter(AI_FUZZY_CACHE.keys()), None)
        if first_key:
            AI_FUZZY_CACHE.pop(first_key, None)
    AI_FUZZY_CACHE[key] = value


def _now_ts():
    return int(time.time())


def _poster_cache_get(key):
    entry = POSTER_CACHE.get(key)
    if not entry:
        return None
    ts, value = entry
    if _now_ts() - ts > POSTER_CACHE_TTL:
        POSTER_CACHE.pop(key, None)
        return None
    return value


def _poster_cache_set(key, value):
    if len(POSTER_CACHE) >= POSTER_CACHE_MAX:
        first_key = next(iter(POSTER_CACHE.keys()), None)
        if first_key:
            POSTER_CACHE.pop(first_key, None)
    POSTER_CACHE[key] = (_now_ts(), value)


def sha_id(*parts: str) -> str:
    raw = "|".join([p or "" for p in parts]).encode("utf-8", "ignore")
    return sha1(raw).hexdigest()[:12]


def stable_hash32(text: str) -> str:
    """Hash estable de 32 bits (8 hex).

    √ötil para deduplicaci√≥n o identificadores cortos.
    Para IDs de items ("_id"), el proyecto usa sha_id() (12 hex)
    para reducir colisiones.
    """
    raw = (text or "").encode("utf-8", "ignore")
    d = sha1(raw).digest()
    return f"{int.from_bytes(d[:4], 'big', signed=False):08x}"


def safe_lower(s: str) -> str:
    return (s or "").strip().lower()


# ---------------- SCOTLAND DETECTOR (GLOBAL, HIGH PRECISION) ----------------
# Motivo: en producci√≥n el header puede perderse y los partidos escoceses quedan bajo otra liga.
# Esta detecci√≥n NO usa fuzzy; requiere match exacto/alias completo y excluye falsos positivos (NBA/NHL/MLB/QPR).
_SCOTTISH_TEAM_ALIASES_GLOBAL = {
    "aberdeen","aberdeen fc","celtic","celtic fc","dundee","dundee fc","dundee united","dundee utd","dundee united fc",
    "heart of midlothian","hearts","hearts fc","hibernian","hibernian fc","hibs","kilmarnock","kilmarnock fc","killie",
    "motherwell","motherwell fc","rangers","rangers fc","ross county","ross county fc","ross co",
    "st johnstone","st johnstone fc","st mirren","st mirren fc",
    "st. johnstone","st. mirren","stjohnstone","stmirren",
    # m√°s comunes en feeds (championship/cups)
    "livingston","livi","greenock morton","morton","partick thistle","raith rovers","queens park","queen of the south",
    "ayr united","inverness","inverness ct","inverness caledonian thistle","dunfermline","arbroath","airdrieonians","falkirk",
    "cove rangers","hamilton","hamilton academical","accies","stirling albion","kelty hearts","alloa athletic",
}

_SCOTTISH_EXCLUDE_GLOBAL = {
    "boston celtics",  # NBA
    "new york rangers", "texas rangers",  # NHL/MLB
    "queens park rangers", "qpr",  # Inglaterra
}

def _norm_team_global(x: str) -> str:
    x = safe_lower(x or "")
    x = x.replace("&", " and ")
    x = x.replace("‚Äô", "").replace("'", "")
    x = x.replace(".", " ")
    x = re.sub(r"[\(\)\[\]\{\}]", " ", x)
    x = re.sub(r"\bfc\b", " ", x)
    x = re.sub(r"\s+", " ", x).strip()
    return x

# patrones por alias completo (word-boundary) para permitir nombres largos/variantes
_SCOT_PATTERNS_GLOBAL = [re.compile(r"\b" + re.escape(a) + r"\b", re.I) for a in sorted(_SCOTTISH_TEAM_ALIASES_GLOBAL, key=len, reverse=True)]

def is_scottish_team_global(name: str) -> bool:
    n = _norm_team_global(name)
    if not n:
        return False
    for bad in _SCOTTISH_EXCLUDE_GLOBAL:
        if re.search(r"\b" + re.escape(bad) + r"\b", n, re.I):
            return False
    if n in _SCOTTISH_TEAM_ALIASES_GLOBAL:
        return True
    return any(p.search(n) for p in _SCOT_PATTERNS_GLOBAL)

def is_scottish_match_global(a: str | None, b: str | None) -> bool:
    if not a or not b:
        return False
    return is_scottish_team_global(a) and is_scottish_team_global(b)



def ai_norm_text(s: str) -> str:
    if not s:
        return ""
    s = safe_lower(s)
    s = re.sub(r"[^\w\s\+]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def ai_similarity(a: str, b: str) -> float:
    if not a or not b:
        return 0.0
    return SequenceMatcher(None, a, b).ratio()


def _aliases_signature(aliases: dict) -> str:
    if not aliases:
        return "0"
    joined = "|".join(sorted(f"{k}:{v}" for k, v in aliases.items()))
    return sha1(joined.encode("utf-8", "ignore")).hexdigest()[:10]


def ai_best_alias(text: str, aliases: dict, min_score: float | None = None, cache_namespace: str | None = None) -> str | None:
    """
    Heur√≠stica tipo "IA": fuzzy matching y normalizaci√≥n robusta para alias.
    No reemplaza la l√≥gica existente: s√≥lo mejora cuando no hay match directo.
    """
    if not text or not aliases:
        return None
    threshold = AI_MATCH_THRESHOLD if min_score is None else min_score
    t = ai_norm_text(text)
    if not t:
        return None
    namespace = cache_namespace or str(id(aliases))
    alias_sig = _aliases_signature(aliases)
    cache_key = (namespace, t, len(aliases), alias_sig)
    cached = _ai_cache_get(cache_key)
    if cached is not None:
        return cached

    t_compact = t.replace(" ", "")
    best_score = 0.0
    best_canon = None
    for alias, canon in aliases.items():
        a = ai_norm_text(alias)
        if not a:
            continue
        if a in t:
            _ai_cache_set(cache_key, canon)
            return canon
        a_compact = a.replace(" ", "")
        score = max(ai_similarity(t, a), ai_similarity(t_compact, a_compact))
        if score > best_score:
            best_score = score
            best_canon = canon

    result = best_canon if best_score >= threshold else None
    _ai_cache_set(cache_key, result)
    return result


def ai_score_sports_item(
    meta_or_raw,
    competition: str | None = None,
    jornada: str | None = None,
    hora: str | None = None,
    canal: str | None = None,
    teams: tuple[str | None, str | None] | None = None,
) -> tuple[float, str]:
    """Heur√≠stica ligera para puntuar items deportivos (sin IA externa).

    Compatibilidad:
      - ai_score_sports_item(meta_dict)
      - ai_score_sports_item(raw, competition, jornada, hora, canal, teams)
    """
    if isinstance(meta_or_raw, dict):
        meta = meta_or_raw
        raw = str(meta.get("raw") or "")
        competition = meta.get("competition") if competition is None else competition
        jornada = meta.get("jornada") if jornada is None else jornada
        hora = meta.get("hora") if hora is None else hora
        canal = meta.get("canal") if canal is None else canal
        if teams is None:
            t = meta.get("teams")
            if isinstance(t, (list, tuple)) and len(t) == 2:
                teams = (t[0], t[1])
            else:
                teams = (None, None)
    else:
        raw = str(meta_or_raw or "")
        if teams is None:
            teams = (None, None)

    score = 0.0
    if competition:
        score += 0.2
    if jornada:
        score += 0.1
    if hora:
        score += 0.2
    if canal:
        score += 0.2
    if teams[0] and teams[1]:
        score += 0.3
    if any(k in safe_lower(raw) for k in ["final", "semifinal", "cuartos", "octavos", "fase de grupos", "ida", "vuelta"]):
        score += 0.05
    score = min(score, 1.0)
    if score >= 0.8:
        return score, "high"
    if score >= 0.5:
        return score, "medium"
    return score, "low"


def ai_score_media_item(meta: dict) -> tuple[float, str]:
    score = 0.0
    if meta.get("title_clean"):
        score += 0.35
    if meta.get("quality"):
        score += 0.2
    if meta.get("se"):
        score += 0.2
    if meta.get("flags"):
        score += 0.15
    if meta.get("section"):
        score += 0.1
    score = min(score, 1.0)
    if score >= 0.8:
        return score, "high"
    if score >= 0.5:
        return score, "medium"
    return score, "low"


# ---------------- MEMORY DEFAULTS ----------------
def _ensure_memory_defaults():
    defaults = {
        "_id": "memory",
        "bad_phrases": [
            "kaelus", "kaelus soporte", "el mejor soporte", "inicio", "herramientas",
            "nueva apk", "descargas", "codigos roku", "c√≥digos roku",
            "ir a inicio", "whatsapp", "telegram", "facebook",
            "aviso", "t√©rminos", "terminos", "privacidad",
            "wix", "cookies", "suscr√≠bete", "suscribete",
            "herramientas app", "nueva apk", "apk", "app", "soporte",
            "contacto", "chat", "ayuda", "home", "menu"
        ],
        "league_aliases": {
            "fa cup": "FA Cup",
            "ekstraklasa": "Ekstraklasa",
            "ligue 1": "Ligue 1",
            "bundesliga 2": "Bundesliga 2",
            "league one": "League One",
            "eredivisie": "Eredivisie",
            "rugby seis naciones": "Rugby Seis Naciones",
            "major arena soccer league": "Major Arena Soccer League",
            "pga tour golf": "PGA Tour Golf",
            "lpga tour": "LPGA Tour",
            "lpga": "LPGA Tour",
            "ncaa softball": "NCAA Softball",
            "liga mexicana de softbol": "Liga Mexicana de Softbol",
            "la liga femenil": "La Liga Femenil",
            "super lig": "Super Lig",
            "super liga argentina": "Super Liga Argentina",
            "saudi pro league": "Saudi Pro League",
            "serie a": "Serie A",
            "la liga": "LaLiga",
            "laliga": "LaLiga",
            "liga mx": "Liga MX",
            "ligamx": "Liga MX",
            "premier": "Premier League",
            "premier league": "Premier League",
            "champions league": "UEFA Champions League",
            "ucl": "UEFA Champions League",
        },
        "channel_aliases": {
            "espn2": "ESPN 2",
            "espn 2": "ESPN 2",
            "espn+": "ESPN+",
            "fox sports": "Fox Sports",
            "dazn": "DAZN",
            "movistar+": "Movistar+",
            "movistar la liga": "Movistar LaLiga",
            "win sports": "Win Sports",
            "tnt sports": "TNT Sports",
            "cbs sports": "CBS Sports",
            "vix+": "VIX+",
            "vix": "VIX",
            "azteca 7": "Azteca 7",
            "canal 5": "Canal 5",
            "amazon prime": "Amazon Prime",
            "gol tv": "Gol TV",
            "bein sports": "BeIN Sports",
            "tvcdportes": "TVC Deportes",
            "tvc deportes": "TVC Deportes",
            "fox one": "Fox One",
        }
    }
    mem_col.update_one({"_id": "memory"}, {"$setOnInsert": defaults}, upsert=True)

    # Merge incremental defaults into existing doc (no sobrescribe personalizaciones)
    try:
        cur = mem_col.find_one({"_id": "memory"}) or {}
        upd = {}

        # bad_phrases
        bp = cur.get("bad_phrases")
        if not isinstance(bp, list):
            upd["bad_phrases"] = defaults.get("bad_phrases", [])
        else:
            extra = [x for x in defaults.get("bad_phrases", []) if x not in bp]
            if extra:
                upd["bad_phrases"] = bp + extra

        # dict merges
        for key in ["league_aliases", "channel_aliases"]:
            base = cur.get(key)
            if not isinstance(base, dict):
                upd[key] = defaults.get(key, {})
            else:
                changed = False
                for k, v in (defaults.get(key, {}) or {}).items():
                    if k not in base:
                        base[k] = v
                        changed = True
                if changed:
                    upd[key] = base

        if upd:
            mem_col.update_one({"_id": "memory"}, {"$set": upd})
    except Exception as e:
        log.warning(f"[MEM] merge defaults skipped: {e}")
_ensure_memory_defaults()


def get_memory():
    """Lee memoria incremental desde Mongo.

    Importante: en entornos donde Mongo est√© temporalmente inaccesible (p.ej. pruebas locales),
    NO debe romper el parser. En ese caso degradamos a diccionarios vac√≠os.
    """
    try:
        d = mem_col.find_one({"_id": "memory"}) or {}
    except Exception:
        d = {}
    return {
        "bad_phrases": d.get("bad_phrases", []),
        "league_aliases": d.get("league_aliases", {}),
        "channel_aliases": d.get("channel_aliases", {}),
    }


def _normalize_btn_text(s: str) -> str:
    """Normaliza texto de botones.

    - Quita acentos y marcas (incluye variation selectors FE0F en emojis).
    - Min√∫sculas.
    - Colapsa espacios m√∫ltiples.
    """
    s = (s or "")
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

# ---------------- CLEANERS ----------------
def strip_weird_symbols_keep_plus(s: str) -> str:
    """
    - Conserva '+' porque puede ser parte del canal (Movistar+, ESPN+).
    - Elimina asteriscos y s√≠mbolos raros.
    """
    if not s:
        return ""
    # Normaliza signos que Wix suele usar en headings (evita perder 'Drama‚Ä¶' -> 'Drama')
    s = (s or "")
    s = s.replace("‚Ä¶", "...")
    s = s.replace("¬∑", ".")
    s = re.sub(r"[*‚òÖ‚òÜ‚úî‚úÖ‚ùå‚Ä¢‚ñ†‚óÜ‚ñ∂Ô∏è‚û°Ô∏è‚õîÔ∏èüî•üí•üí´]+", " ", s)
    s = re.sub(r"[^\w\s\+\-\.√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë:/]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def is_noise(text: str, bad_phrases: Optional[List[str]] = None) -> bool:
    t = safe_lower(text)
    t_compact = safe_lower(strip_weird_symbols_keep_plus(text)).strip()
    # UCLA es un equipo (Universidad de California de Los Angeles), no es UCL ni UEFA.
    # NO se considera ruido para que el parser lo clasifique correctamente (NCAA).
    if re.search(r"\bucla\b", t):
        return False
    if not t:
        return True

    # Compatibilidad: algunos puntos del c√≥digo llaman is_noise() con 1 solo argumento.
    # En ese caso, usamos la lista guardada en memoria (Mongo) o una lista vac√≠a.
    if bad_phrases is None:
        try:
            bad_phrases = (get_memory() or {}).get("bad_phrases", [])
        except Exception:
            bad_phrases = []

    # --- allow short section headers ---
    allow_short = {
        "cine", "series", "peliculas", "pel√≠culas", "ppv", "estrenos",
        "deportes", "menu", "home",
        # Deportes (encabezados cortos v√°lidos)
        "box", "boxeo",
        # VOD (g√©neros cortos que Wix puede dejar sin '...')
        "drama", "anime", "terror", "crimen", "retro", "latino",
        "accion", "acci√≥n",
    }
    if len(t_compact) < 6:
        if t_compact in allow_short or any(k in t_compact for k in ["eventos", "ppv", "pel", "seri"]):
            return False

        # "Copa" puede venir sola pero con bandera/emoji (ej. "Copaüá¶üá∑‚öΩÔ∏è").
        # En ese caso NO es ruido (es encabezado de competencia).
        if t_compact == "copa":
            if any(x in (text or "") for x in ["üá¶üá∑","üá©üá™","üáÆüáπ","üá™üá∏","üá≤üáΩ","üá∫üá∏","üáßüá∑","üè¥","‚öΩ","üèÜ"]):
                return False

        return True

    # Hard drops (Wix/menu/accesibilidad/imagenes)
    if t.startswith("inicio:") or t.startswith("image:"):
        return True
    if "use tab to navigate through the menu items" in t:
        return True
    if t.strip() == "visitas":
        return True
    if "ir a inicio" in t:
        return True

    # separadores
    if re.fullmatch(r"[\*\-\_\‚Äî\s]{10,}", t):
        return True

    # URLs
    if "http://" in t or "https://" in t or "www." in t or "bit.ly" in t:
        return True
    # Dominios sueltos (ej. 'kaelustvsoporte.com', 'wixsite.com')
    if re.search(r"\b[a-z0-9\-]{2,}\.(?:com|net|org|mx|tv|io|app|site)\b", t):
        return True

    # ------------------------------------------------------------
    # Deportes: NO tirar encabezados/partidos por 'bad_phrases'
    #
    # En Wix, algunos t√≠tulos de competiciones (p.ej. "Championshipüè¥‚öΩÔ∏è",
    # "Primera Aüá™üá®‚öΩÔ∏è", "A-Leagueüá¶üá∫‚öΩ") pueden coincidir accidentalmente con
    # frases marcadas como "basura" en memoria. Si esos headers se eliminan,
    # los partidos siguientes se pegan al bloque anterior (mezcla de ligas).
    #
    # Regla: si una l√≠nea parece CONTENIDO deportivo (header o evento),
    # ignoramos bad_phrases para esa l√≠nea.
    # ------------------------------------------------------------
    SPORT_EMOJIS = ["‚öΩ", "üèÄ", "üèà", "‚öæ", "ü•é", "‚õ≥", "üéæ", "ü•ä", "üèí", "ü§º", "‚õ∑", "üèÖ", "üèé", "üèê", "üèè", "üé≥", "üèä", "üö¥", "üèÉ"]

    def _has_flag(s: str) -> bool:
        if not s:
            return False
        if "üè¥" in s:
            return True
        for ch in s:
            o = ord(ch)
            if 0x1F1E6 <= o <= 0x1F1FF:  # regional indicator symbols
                return True
        return False

    def _looks_like_sports_line(raw: str) -> bool:
        if not raw:
            return False
        low_raw = safe_lower(raw)
        # Eventos: partido/combate + horario
        if re.search(r"(?i)\bvs\b", raw) or re.search(r"(?i)\b@\b", raw):
            return True
        if RX_TIME_AMPM.search(raw):
            return True
        if any(e in raw for e in SPORT_EMOJIS):
            return True
        if _has_flag(raw):
            return True
        if re.search(r"(?i)\b(jornada|ronda|fase|categoria|categor[i√≠]a)\b", low_raw):
            return True
        # Headers de competiciones (sin 'vs' ni 'por')
        norm = safe_lower(strip_weird_symbols_keep_plus(raw))
        if (" vs " not in f" {norm} ") and (" por " not in f" {norm} "):
            if any(k in norm for k in [
                "liga", "league", "copa", "cup", "nba", "ncaa", "wwe",
                "bundesliga", "serie", "ligue", "ekstraklasa", "championship",
                "a-league", "primera", "olimp"
            ]):
                if 5 <= len(norm) <= 60:
                    return True
        return False

    looks_sport = _looks_like_sports_line(text)

    # frases basura (menus/soporte/herramientas/etc)
    if (not looks_sport) and any(bp in t for bp in bad_phrases):
        return True

    # --- allow media headings with ellipsis ---
    # Wix usa "Genero..." (con tres puntos) para Pel√≠culas/Series. Estas l√≠neas
    # tienen alta proporci√≥n de puntuaci√≥n y el filtro de s√≠mbolos puede marcarlas
    # como ruido. Las conservamos expl√≠citamente.
    # IMPORTANTE: en Wix los "..." a veces vienen como un solo car√°cter Unicode "‚Ä¶".
    # Si evaluamos el texto sin normalizar, estos encabezados pueden caer en el filtro
    # de "demasiados s√≠mbolos" y desaparecer, provocando que TODO se vaya a 'Novedades'.
    raw_strip = (text or "").strip()
    raw_norm = strip_weird_symbols_keep_plus(raw_strip).strip()  # convierte '‚Ä¶' -> '...'

    # Encabezados tipo "Genero..." (acepta '...' o '‚Ä¶')
    if re.fullmatch(r"(?i)\s*(?:‚ú®\s*)?[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√± ]{3,40}\s*(?:\.\.\.|‚Ä¶|:)\s*", raw_strip):
        return False
    if re.fullmatch(r"(?i)\s*(?:‚ú®\s*)?[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√± ]{3,40}\s*(?:\.\.\.|:)\s*", raw_norm):
        return False

    # Super-secciones (NUEVAS PELICULAS / SERIES / NUEVOS EPISODIOS)
    if re.fullmatch(r"(?i)\s*(nuevas?\s+pel[i√≠]culas|nuevas?\s+series|nuevos?\s+(?:episodios?|cap[i√≠]tulos?))\s*", raw_norm):
        return False

    # "CONTENIDO NUEVO" nunca es ruido (puede traer fecha expl√≠cita)
    if "contenido nuevo" in safe_lower(raw_norm):
        return False

    # demasiados s√≠mbolos (ignora emojis/s√≠mbolos tipo 'So' para no eliminar encabezados con banderas/emoji)
    import unicodedata as _ud
    base_len = sum(1 for ch in t if _ud.category(ch) != 'So')
    nonalnum = sum(1 for ch in t if not (ch.isalnum() or ch.isspace() or _ud.category(ch) == 'So'))
    if (nonalnum / max(1, base_len)) > 0.25:
        return True

    return False

def normalize_competition(text: str, league_aliases: dict) -> str | None:
    """Normaliza el nombre de la competencia.

    Importante: NO mapear la palabra gen√©rica "champions" a UEFA,
    porque en la fuente aparece "CONCACAF Champions Cup".
    """
    t = safe_lower(text)

    # UCLA es un equipo (Universidad de California de Los Angeles).
    # Si aparece en la l√≠nea, NO intentamos inferir una competencia v√≠a alias/IA.
    # (Evita encabezados fantasma como UEFA Champions League por confusi√≥n con "UCL").
    if re.search(r"\bucla\b", t):
        return None

    def _is_explicit_uefa_ucl(src: str) -> bool:
        tl = safe_lower(src)
        if re.search(r"\bucla\b", tl):
            return False
        if ("uefa" in tl and "champions" in tl) or ("champions league" in tl):
            return True
        # Abreviatura UCL: solo si viene como palabra completa + emoji t√≠pico.
        if re.search(r"\bucl\b", tl) and any(e in src for e in ["üèÜ", "‚öΩ", "üá™üá∫"]):
            return True
        return False

    # Prioridad: CONCACAF Champions Cup (evita confundir con UEFA)
    if "concacaf" in t and ("champions" in t or "campeones" in t):
        return "Copa de Campeones CONCACAF"

    # Prioridad: AFC Champions League
    if "afc" in t and ("champions" in t or "campeones" in t or "liga de campeones" in t):
        return "Liga de Campeones de la AFC"

    candidates = []
    for k, canon in league_aliases.items():
        # Evita falsos positivos por subcadenas (ej. 'ucl' dentro de 'ucla')
        rx = r"(?<![A-Za-z0-9])" + re.escape(k) + r"(?![A-Za-z0-9])"
        if re.search(rx, t):
            # 'ucl' es especialmente sensible: solo aceptar si viene con emoji t√≠pico
            if safe_lower(k) == "ucl":
                if (not re.search(r"\bucl\b", t)) or re.search(r"\bucla\b", t) or (not any(e in text for e in ["üèÜ", "‚öΩ", "üá™üá∫"])):
                    continue
            # Evita encabezados fantasma de UEFA por alias demasiado gen√©ricos en memoria
            if canon == "UEFA Champions League":
                if not _is_explicit_uefa_ucl(text):
                    continue
            candidates.append(canon)

    if "laliga" in t or "la liga" in t:
        candidates.append("LaLiga")
    if "liga mx" in t or "ligamx" in t:
        if "femen" in t:
            candidates.append("Liga MX Femenil")
        else:
            candidates.append("Liga MX")
    if "premier" in t:
        candidates.append("Premier League")

    # UEFA Champions League (solo si viene expl√≠cito y con emoji t√≠pico).
    # Importante: NO usar "UCL" a pelo (evita falsos positivos y encabezados fantasma).
    if (("champions league" in t) or ("uefa" in t and "champions" in t)) and ("concacaf" not in t) and ("afc" not in t):
        if any(e in text for e in ["üèÜ", "‚öΩ", "üá™üá∫"]):
            candidates.append("UEFA Champions League")


    # CONCACAF Champions Cup / Champions Cup
    if "concacaf" in t and ("champions" in t or "campeones" in t):
        candidates.append("Copa de Campeones CONCACAF")

    if candidates:
        return candidates[0]

    # Fallback IA: blindaje contra "UEFA Champions League" fantasma.
    ai = ai_best_alias(text, league_aliases, cache_namespace="league")
    if ai == "UEFA Champions League" and (not _is_explicit_uefa_ucl(text)):
        return None
    return ai

def sanitize_channel_name(ch: str) -> str:
    ch = strip_weird_symbols_keep_plus(ch)
    ch = re.sub(r"\s+", " ", ch).strip()
    return ch


def detect_channel(text: str, channel_aliases: dict) -> str | None:
    # (se mantiene para compatibilidad)
    raw = text or ""
    t = safe_lower(raw)

    for alias, canon in channel_aliases.items():
        if alias in t:
            return sanitize_channel_name(canon)

    if "espn+" in t:
        return "ESPN+"
    if "espn" in t:
        m = re.search(r"\bespn\s*([0-9])\b", t)
        if m:
            return f"ESPN {m.group(1)}"
        return "ESPN"
    if "fox" in t and "sport" in t:
        return "Fox Sports"
    if "dazn" in t:
        return "DAZN"
    if "movistar" in t:
        return "Movistar+"
    if "tnt" in t and "sport" in t:
        return "TNT Sports"
    if "win" in t and "sport" in t:
        return "Win Sports"

    ai_match = ai_best_alias(text, channel_aliases, cache_namespace="channel")
    return sanitize_channel_name(ai_match) if ai_match else None



def _reorder_channels_for_display(channels: list[str]) -> list[str]:
    """Mantiene el orden, elimina duplicados y manda los tokens 'Categoria ...' al final."""
    if not channels:
        return []
    out: list[str] = []
    seen = set()
    for c in channels:
        c2 = sanitize_channel_name(str(c)) if c is not None else ""
        if not c2:
            continue
        k = safe_lower(c2)
        if k in seen:
            continue
        seen.add(k)
        out.append(c2)

    cats = [c for c in out if safe_lower(c).startswith("categoria")]
    rest = [c for c in out if not safe_lower(c).startswith("categoria")]
    return rest + cats


def detect_channels(text: str, channel_aliases: dict) -> list[str]:
    """Extrae m√∫ltiples canales posibles en una misma l√≠nea (incluye 'Categoria ...' como canal)."""
    raw = text or ""
    t = safe_lower(raw)
    found: list[str] = []

    def _add(x: str | None):
        if not x:
            return
        x2 = sanitize_channel_name(x)
        if x2 and x2 not in found and len(x2) <= 80:
            found.append(x2)

    # alias directos
    for alias, canon in (channel_aliases or {}).items():
        if alias and alias in t:
            _add(canon)

    # ESPN heur√≠stica
    if "espn+" in t:
        _add("ESPN+")
    for n in ("1", "2", "3", "4", "5"):
        if re.search(rf"\bespn\s*{n}\b", t):
            _add(f"ESPN {n}")
    if "espn" in t and not any(x.startswith("ESPN") for x in found):
        _add("ESPN")

    # comunes
    if "fox" in t and "sport" in t:
        _add("Fox Sports")
    if "dazn" in t:
        _add("DAZN")
    if "movistar" in t:
        _add("Movistar+")
    if "tnt" in t and "sport" in t:
        _add("TNT Sports")

    # tokens por coma (lista expl√≠cita del sitio)
    if "," in raw:
        parts = [p.strip() for p in raw.split(",") if p.strip()]
        for p in parts:
            pl = safe_lower(p)
            if pl.startswith("categoria "):
                _add(p)
                continue
            if any(k in pl for k in [
                "categoria", "espn", "fox", "movistar", "tnt", "dazn", "cbs",
                "vix", "azteca", "canal", "bein", "gol", "prime", "tvc",
                "sky", "star", "max", "hbo", "paramount", "amazon", "apple"
            ]):
                _add(p)

    return found[:60]

def clean_title_for_search(raw: str) -> str:
    s = (raw or "").strip()
    s = re.sub(r"\.(mp4|mkv|avi|ts)\b", "", s, flags=re.IGNORECASE)
    s = s.replace(".", " ")
    cut_tokens = ['2160p', '1080p', '720p', '4k', 'uhd', 'bluray', 'web-dl', 'webrip', 'hdrip', 'latino', 'castellano', 'sub', 'vose', 'dual', 'multi', 's0', 't0', 'cap', 'temporada', 'episodio', 'episodios', 'season', 'episode']
    low = s.lower()
    idx = len(s)
    for tok in cut_tokens:
        p = low.find(tok)
        if p != -1:
            idx = min(idx, p)
    s = s[:idx].strip()
    s = re.sub(r"\s+", " ", s).strip()
    return s[:80] if s else (raw or "")[:80]



def is_logo_url(u: str) -> bool:
    """Heur√≠stica: evita usar logos de canal como 'poster'."""
    if not u or not isinstance(u, str):
        return False
    s = u.strip().lower()
    # nombres t√≠picos
    bad = ["logo", "canal", "channel", "icon", "favicon", "sprite"]
    if any(b in s for b in bad):
        return True
    # archivos t√≠picos
    if any(s.endswith(x) for x in ["/logo.png", "/logo.jpg", "/logo.jpeg", "/logo.webp"]):
        return True
    return False




def _zuplo_origin() -> str:
    """Devuelve el origen (scheme://host) de POSTER_API_URL para normalizar rutas relativas."""
    try:
        if not POSTER_API_URL:
            return ""
        u = urlparse(POSTER_API_URL)
        if not u.scheme or not u.netloc:
            return ""
        return f"{u.scheme}://{u.netloc}"
    except Exception:
        return ""


def _poster_auth_headers() -> dict:
    """Headers est√°ndar para Zuplo/TMDB proxies. No expone key, solo la adjunta si existe.
    - Authorization: Bearer <key> (si ya viene con Bearer, no lo duplica)
    - x-api-key / apikey: <key> (crudo)
    """
    headers = {"accept": "image/*,application/json;q=0.9,*/*;q=0.8", "User-Agent": "KaelusBot/1.0"}
    if POSTER_API_KEY:
        k0 = str(POSTER_API_KEY).strip()
        raw = k0
        if k0.lower().startswith("bearer "):
            raw = k0.split(None, 1)[1].strip() if len(k0.split(None, 1)) > 1 else ""
        if raw:
            headers["Authorization"] = f"Bearer {raw}"
            headers["x-api-key"] = raw
            headers["apikey"] = raw
    return headers

def _zuplo_coerce_url(v):
    """Intenta extraer URL string desde distintos formatos.
    - Acepta http(s), //cdn..., data:image/*
    - Si Zuplo devuelve rutas relativas (/poster/..), las convierte usando el origen de POSTER_API_URL.
    """
    if not v:
        return None
    if isinstance(v, str):
        s = v.strip()
        if not s:
            return None
        if s.startswith("data:image/"):
            return s
        if s.startswith("//"):
            return "https:" + s
        if s.startswith("http://") or s.startswith("https://"):
            return s
        # rutas relativas (com√∫n en gateways/proxies)
        if s.startswith("/") and POSTER_API_URL:
            origin = _zuplo_origin()
            if origin:
                return origin + s
        return None
    if isinstance(v, dict):
        # keys comunes de gateways/proxies
        for k in ("url", "poster", "poster_url", "posterUrl", "img", "image", "image_url", "imageUrl", "secure_url", "src", "file", "file_url"):
            u = v.get(k)
            if isinstance(u, str) and u.strip().startswith("http"):
                return u.strip()
        # a veces vienen anidados
        for k in ("data", "result", "results", "images"):
            nested = v.get(k)
            if nested:
                u = _zuplo_coerce_url(nested)
                if u:
                    return u
    return None




def _zuplo_pick_first_url(v):
    # lista de strings/dicts
    if isinstance(v, list) and v:
        for it in v:
            u = _zuplo_coerce_url(it)
            if u:
                return u
    return _zuplo_coerce_url(v)


def get_media_art_zuplo(title_clean: str, content_type: str) -> dict:
    """Consulta Zuplo (POSTER_API_URL) y devuelve un dict con posibles artes:
      poster, backdrop, logo, thumb, url/img (alias)

    Se cachea en memoria para evitar sobrecarga.
    Soporta gateways que devuelven:
      - URL directa de imagen
      - JSON con {poster, img, url, ...}
      - JSON estilo TMDB con poster_path/backdrop_path (o results[0].poster_path)
    """
    if not POSTER_API_URL or not title_clean:
        return {}

    t = "tv" if content_type == "tv" else "movie"
    cache_key = f"art:{t}:{title_clean.lower()}"
    cached = _poster_cache_get(cache_key)
    if isinstance(cached, dict) and cached:
        return cached

    base = POSTER_API_URL.rstrip("/")
    q = f"title={quote_plus(title_clean)}&type={t}"

    
    # Probamos varias combinaciones comunes de endpoints y nombres de par√°metros, porque cada gateway puede variar.
    q_variants = [
        f"title={quote_plus(title_clean)}&type={t}",
        f"q={quote_plus(title_clean)}&type={t}",
        f"query={quote_plus(title_clean)}&type={t}",
        f"search={quote_plus(title_clean)}&type={t}",
        f"name={quote_plus(title_clean)}&type={t}",
    ]

    type_variants = [t] + (["tv", "series"] if t == "tv" else ["movie", "film"])
    q_variants += [f"title={quote_plus(title_clean)}&type={tv}" for tv in type_variants]
    q_variants += [f"q={quote_plus(title_clean)}&type={tv}" for tv in type_variants]

    path_variants = [
        "/poster_img", "/posterimg", "/poster-image",
        "/poster", "/posters", "/image", "/img", "/search", "/tmdb/poster", "/tmdb/search", "/v1/poster", "/api/poster"
    ]

    candidate_urls = []
    for pth in path_variants:
        for qv in q_variants:
            candidate_urls.append(f"{base}{pth}?{qv}")
    # tambi√©n probamos el root con query
    for qv in q_variants:
        candidate_urls.append(f"{base}?{qv}")


    headers = _poster_auth_headers()

    def _tmdb_img_from_path(p: str, kind: str = "poster") -> str:
        if not p or not isinstance(p, str):
            return ""
        p = p.strip()
        if not p.startswith("/"):
            return ""
        size = "w500" if kind == "poster" else "w780"
        return f"https://image.tmdb.org/t/p/{size}{p}"

    def _deep_find_first_path(obj):
        if isinstance(obj, dict):
            for k in ("poster_path", "posterPath"):
                v = obj.get(k)
                if isinstance(v, str) and v.strip().startswith("/"):
                    return ("poster", v.strip())
            for k in ("backdrop_path", "backdropPath"):
                v = obj.get(k)
                if isinstance(v, str) and v.strip().startswith("/"):
                    return ("backdrop", v.strip())
            for k in ("results", "data", "items"):
                found = _deep_find_first_path(obj.get(k))
                if found:
                    return found
            for v in obj.values():
                found = _deep_find_first_path(v)
                if found:
                    return found
        elif isinstance(obj, list):
            for it in obj:
                found = _deep_find_first_path(it)
                if found:
                    return found
        return None

    out = {}
    last_err = None

    for url in candidate_urls:
        try:
            r = SESSION.get(url, headers=headers, timeout=POSTER_TIMEOUT)
            if r.status_code >= 400:
                continue

            ct0 = (r.headers.get('content-type') or '').split(';')[0].strip().lower()
            # Si el gateway devuelve imagen directa, usamos esa URL como 'poster'
            if ct0.startswith('image/'):
                out = {'poster': url, 'img': url, 'url': url}
                _poster_cache_set(cache_key, out)
                return out

            data = {}
            if "json" in (r.headers.get("content-type") or "").lower():
                try:
                    data = r.json() or {}
                except Exception:
                    data = {}

            if isinstance(data, dict) and isinstance(data.get("data"), dict):
                data = data.get("data") or data

            poster = _zuplo_pick_first_url(data.get("poster")) or _zuplo_pick_first_url(data.get("img")) or _zuplo_pick_first_url(data.get("url"))
            backdrop = _zuplo_pick_first_url(data.get("backdrop")) or _zuplo_pick_first_url(data.get("fanart")) or _zuplo_pick_first_url(data.get("background"))
            logo = _zuplo_pick_first_url(data.get("logo"))
            thumb = _zuplo_pick_first_url(data.get("thumb")) or _zuplo_pick_first_url(data.get("thumbnail"))

            if not poster:
                poster = _zuplo_pick_first_url(data.get("posters"))
            if not backdrop:
                backdrop = _zuplo_pick_first_url(data.get("backdrops"))
            if not logo:
                logo = _zuplo_pick_first_url(data.get("logos"))
            if not thumb:
                thumb = _zuplo_pick_first_url(data.get("thumbs"))

            if not poster and not backdrop:
                found = _deep_find_first_path(data)
                if found:
                    kind, path = found
                    if kind == "poster":
                        poster = _tmdb_img_from_path(path, "poster")
                    else:
                        backdrop = _tmdb_img_from_path(path, "backdrop")
                        if not poster and backdrop:
                            poster = backdrop.replace("/w780", "/w500")

            if poster:
                out["poster"] = poster
            if backdrop:
                out["backdrop"] = backdrop
            if logo:
                out["logo"] = logo
            if thumb:
                out["thumb"] = thumb

            if poster:
                out.setdefault("img", poster)
                out.setdefault("url", poster)

            _poster_cache_set(cache_key, out)
            return out
        except Exception as e:
            last_err = e
            continue

    if last_err:
        log.warning(f"Zuplo art error: {last_err}")
    _poster_cache_set(cache_key, {})
    return {}


def get_poster_zuplo(title_clean: str, content_type: str) -> str:
    """Back-compat: devuelve solo un URL (poster preferido)."""
    art = get_media_art_zuplo(title_clean, content_type)
    poster = (art.get("poster") or art.get("img") or art.get("url") or art.get("backdrop") or art.get("thumb") or "")
    return poster or LOGO_URL


# ---------------- TIME HELPERS (NUEVO, NO BORRA NADA) ----------------
_TZ_ABBR_TO_IANA = {
    "UTC": "UTC",
    "GMT": "UTC",

    # Abreviaciones comunes
    "ET": "America/New_York",
    "CT": "America/Chicago",
    "CST": "America/Chicago",
    "CDT": "America/Chicago",
    "PT": "America/Los_Angeles",
    "PST": "America/Los_Angeles",
    "MST": "America/Denver",

    # Texto en espa√±ol (Kaelus)
    "ESTE": "America/New_York",
    "CENTRO": "America/Chicago",
    "PACIFICO": "America/Los_Angeles",

    # Texto en ingl√©s
    "EASTERN": "America/New_York",
    "CENTRAL": "America/Chicago",
    "PACIFIC": "America/Los_Angeles",

    # Europa (se mantiene)
    "CET": "Europe/Madrid",
    "BST": "Europe/London",
}

def _parse_time_hhmm(hhmm: str) -> tuple[int, int] | None:
    """Parsea horas tipo:
      - 14:45
      - 2:45 pm / 2 pm
      - 8 p.m. / 8 a.m.
    Retorna (HH24, MM).
    """
    if not hhmm:
        return None

    s = (hhmm or "").strip().lower()
    s = s.replace("a. m.", "am").replace("p. m.", "pm")
    s = s.replace("a.m.", "am").replace("p.m.", "pm")
    s = s.replace("a.m", "am").replace("p.m", "pm")
    s = s.replace(".", "")  # 8 p.m. -> 8 pm
    s = re.sub(r"\s+", " ", s).strip()

    # 24h HH:MM
    m24 = re.match(r"^([01]?\d|2[0-3])[:.]([0-5]\d)$", s)
    if m24:
        return int(m24.group(1)), int(m24.group(2))

    # 12h con am/pm (minutos opcionales)
    m12 = re.match(r"^(\d{1,2})(?::([0-5]\d))?\s*(am|pm)$", s)
    if not m12:
        return None

    h = int(m12.group(1))
    mm = int(m12.group(2) or "0")
    ampm = m12.group(3)

    if h < 1 or h > 12:
        return None

    if ampm == "am":
        if h == 12:
            h = 0
    else:  # pm
        if h != 12:
            h += 12
    return h, mm

def compute_start_utc(hora_original: str | None, timezone_txt: str | None, now_utc: datetime | None = None) -> datetime | None:
    """
    Convierte hora + tz abreviada a datetime UTC (fecha = hoy en esa zona, con heur√≠stica +1 d√≠a si ya pas√≥).
    """
    if not hora_original:
        return None
    tz_key = (timezone_txt or "UTC").upper()
    # Normaliza acentos (PAC√çFICO -> PACIFICO)
    try:
        tz_key = unicodedata.normalize("NFD", tz_key)
        tz_key = "".join(ch for ch in tz_key if unicodedata.category(ch) != "Mn")
    except Exception:
        pass
    iana = _TZ_ABBR_TO_IANA.get(tz_key, "UTC")
    try:
        z = ZoneInfo(iana)
    except Exception:
        z = timezone.utc

    hhmm = _parse_time_hhmm(hora_original)
    if not hhmm:
        return None

    now_utc = now_utc or datetime.now(timezone.utc)
    now_local = now_utc.astimezone(z)
    dt_local = datetime(now_local.year, now_local.month, now_local.day, hhmm[0], hhmm[1], tzinfo=z)

    # Heur√≠stica: si ya pas√≥ "demasiado" en esa zona, probablemente sea ma√±ana.
    # (mejor que marcar en vivo incorrecto).
    if (now_local - dt_local) > timedelta(hours=8):
        dt_local = dt_local + timedelta(days=1)

    return dt_local.astimezone(timezone.utc)


def format_time_12h(dt: datetime, tz_name: str, label_es: str) -> str:
    d = dt.astimezone(ZoneInfo(tz_name))
    hour = d.hour
    minute = d.minute
    ampm = "am" if hour < 12 else "pm"
    h12 = hour % 12
    if h12 == 0:
        h12 = 12
    return f"{h12}:{minute:02d} {ampm} {label_es}"


def format_three_timezones(start_utc: datetime) -> str:
    # Este / Centro / Pac√≠fico (como pidi√≥)
    parts = [
        format_time_12h(start_utc, "America/New_York", "Este"),
        format_time_12h(start_utc, "America/Chicago", "Centro"),
        format_time_12h(start_utc, "America/Los_Angeles", "Pac√≠fico"),
    ]
    return ", ".join(parts)


def is_live(now_utc: datetime, start_utc: datetime | None, duration_min: int) -> bool:
    if not start_utc:
        return False
    end_utc = start_utc + timedelta(minutes=duration_min)
    return start_utc <= now_utc <= end_utc


def sport_kind_from_text(low: str) -> str:
    low = low or ""
    if ("ü•é" in low) or any(k in low for k in ["softbol", "softball"]):
        return "softball"
    if any(k in low for k in ["ufc", "box", "boxeo", "bellator", "fight night", "wwe", "wrestling"]):
        return "combat"
    if any(k in low for k in ["wimbledon", "roland garros", "australian open", "us open", "atp", "wta"]):
        return "tennis"
    if any(k in low for k in ["pga", "liv golf", "the masters", "ryder cup", "dp world tour", "lpga"]):
        return "golf"
    if any(k in low for k in ["mlb", "beisbol", "b√©isbol", "serie del caribe", "liga del pacifico", "liga mexicana del pacifico"]):
        return "baseball"
    if any(k in low for k in ["nba", "baloncesto", "basket", "euroleague"]):
        return "basketball"
    if any(k in low for k in ["nfl", "ncaa football", "football americano", "thursday night football", "monday night football", "sunday night football", "super bowl"]):
        return "american_football"
    if any(k in low for k in ["nhl", "hockey", "hielo"]):
        return "hockey"
    if ("üèâ" in low) or any(k in low for k in ["rugby", "seis naciones", "6 naciones", "six nations"]):
        return "rugby"
    # Motorsports
    if ("üèé" in low) or any(k in low for k in ["nascar", "formula 1", "f1", "indycar", "motogp", "moto gp", "nascar craftsman", "truck series", "auto parts series", "xfinity", "fr8 racing", "echo park speedway"]):
        return "motorsport"
    return "soccer"


def sport_emoji(kind: str) -> str:
    if kind == "combat":
        return "ü•ä"
    if kind == "tennis":
        return "üéæ"
    if kind == "golf":
        return "‚õ≥"
    if kind == "baseball":
        return "‚öæ"
    if kind == "basketball":
        return "üèÄ"
    if kind == "american_football":
        return "üèà"
    if kind == "hockey":
        return "üèí"
    if kind == "motorsport":
        return "üèéÔ∏è"
    return "‚öΩ"


# ---------------- SCRAPING (WIX) ----------------
def fetch_html() -> str:
    """Descarga HTML de la p√°gina ra√≠z (Wix) con reintentos.

    En Wix es normal ver fallos intermitentes tipo:
      ConnectionResetError(104, 'Connection reset by peer')
    Si pasa, reintentamos con backoff y (si aplica) reiniciamos la sesi√≥n HTTP.
    """
    global SESSION
    import random

    headers = {
        # UA m√°s completo para evitar resets/bloqueos de algunos CDNs
        "User-Agent": os.getenv(
            "SCRAP_UA",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "es-ES,es;q=0.9,en;q=0.8",
        # Forzar conexi√≥n nueva reduce 'connection reset' por keep-alive roto
        "Connection": "close",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }

    retries = int(os.getenv("SCRAP_RETRIES", "4"))  # 5 intentos total por defecto
    connect_timeout = float(os.getenv("SCRAP_CONNECT_TIMEOUT", "8"))
    read_timeout = float(os.getenv("SCRAP_READ_TIMEOUT", "25"))

    last_err = None
    for attempt in range(retries + 1):
        try:
            r = SESSION.get(
                SCRAP_URL,
                timeout=(connect_timeout, read_timeout),
                headers=headers,
                allow_redirects=True,
            )
            r.raise_for_status()
            html = r.text or ""
            if len(html) < 200:
                raise ValueError("HTML vac√≠o o demasiado corto")
            return html

        except Exception as e:
            last_err = e

            # Si es un error de conexi√≥n, reinicia la sesi√≥n (conexi√≥n limpia)
            if isinstance(
                e,
                (
                    requests.exceptions.ConnectionError,
                    requests.exceptions.ChunkedEncodingError,
                    requests.exceptions.SSLError,
                ),
            ):
                try:
                    SESSION.close()
                except Exception:
                    pass
                SESSION = requests.Session()

            # Backoff exponencial + jitter (solo si quedan reintentos)
            if attempt < retries:
                sleep_s = min(1.5 * (2 ** attempt) + random.random(), 20)
                log.warning(
                    f"fetch_html retry {attempt + 1}/{retries + 1} error={e}"
                )
                time.sleep(sleep_s)
            else:
                log.error(f"fetch_html failed after {retries + 1} attempts: {e}")

    raise last_err



def _prune_menu_blocks(blocks: list[str]) -> list[str]:
    """Elimina el bloque de men√∫/navegaci√≥n t√≠pico de Wix que mete basura.
    Heur√≠stica: recorta desde 'VISITAS' hasta 'Use tab to navigate through the menu items.'
    """
    if not blocks:
        return []
    # drop explicit accessibility line anywhere
    blocks2 = [b for b in blocks if "use tab to navigate through the menu items" not in safe_lower(b)]
    low_blocks2 = [safe_lower(b) for b in blocks2]

    try:
        start = next(i for i, t in enumerate(low_blocks2) if t.strip() == "visitas" or t.startswith("visitas "))
    except StopIteration:
        return blocks2

    end = None
    for i in range(start, min(start + 80, len(low_blocks2))):
        if "use tab to navigate through the menu items" in low_blocks2[i]:
            end = i
            break
    if end is None:
        return blocks2

    return blocks2[:start] + blocks2[end + 1:]


def html_to_text_blocks(html: str) -> list[str]:
    soup = BeautifulSoup(html, "html.parser")

    # Eliminar elementos que casi siempre son ruido en Wix
    for t in soup.find_all(["script", "style", "noscript", "svg"]):
        try:
            t.decompose()
        except Exception:
            pass

    for t in soup.find_all(["header", "nav", "footer", "aside"]):
        try:
            t.decompose()
        except Exception:
            pass

    try:
        for t in soup.select('[role="navigation"],[role="banner"],[role="contentinfo"]'):
            try:
                t.decompose()
            except Exception:
                pass
    except Exception:
        pass

    def _strip_minimal_keep_emojis(s: str) -> str:
        """Limpieza m√≠nima SIN borrar banderas/emojis.
        (Necesario porque en la fuente los t√≠tulos traen üáÆüáπüá≤üáΩ etc.)
        """
        if not s:
            return ""
        s = s.replace("\u00a0", " ").replace("\xa0", " ")
        # quita solo s√≠mbolos t√≠picos de ruido
        s = re.sub(r"[*‚òÖ‚òÜ‚úî‚úÖ‚ùå‚Ä¢‚ñ†‚óÜ‚ñ∂Ô∏è‚û°Ô∏è‚õîÔ∏èüî•üí•üí´]+", " ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    blocks: list[str] = []
    # OJO: evitamos 'a' y 'span' porque el men√∫/links viven ah√≠
    for tag in soup.find_all(["h1", "h2", "h3", "h4", "h5", "p", "li"]):
        tx = tag.get_text("\n", strip=True)  # preserva <br> como saltos
        if not tx:
            continue
        for line in tx.splitlines():
            line = _strip_minimal_keep_emojis(line)
            if not line:
                continue
            blocks.append(line)

    # Heur√≠stica extra: quitar el bloque de men√∫ Wix si aparece
    blocks = _prune_menu_blocks(blocks)

    return blocks

def slice_after_marker(blocks: list[str], marker: str) -> int:
    """Retorna el √≠ndice del primer bloque que *parece* ser encabezado del marker.

    Importante: NO usar 'in' (subcadena), porque rompe con deportes que contienen palabras
    como 'Cup Series' (NASCAR) y los corta como si fueran 'SERIES'.
    """
    def _norm(s: str) -> str:
        s2 = strip_weird_symbols_keep_plus(s or "")
        s2 = re.sub(r"[^A-Za-z0-9√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√± ]+", " ", s2)
        s2 = re.sub(r"\s+", " ", s2).strip().lower()
        return s2

    m = _norm(marker)
    for i, b in enumerate(blocks):
        nb = _norm(b)
        if not nb:
            continue
        # match por prefijo o igualdad (encabezados reales)
        if nb == m or nb.startswith(m + " "):
            return i
    return -1


def build_sections_from_blocks(blocks: list[str]) -> dict:
    mem = get_memory()
    bad_phrases = mem["bad_phrases"]

    clean: list[str] = []
    prev_k: str | None = None
    for b in blocks:
        if is_noise(b, bad_phrases):
            continue
        k = safe_lower(b)
        # Solo quitamos duplicados consecutivos (no global),
        # porque en DEPORTES puede repetirse el mismo horario/canal en distintos partidos.
        if prev_k == k:
            continue
        prev_k = k
        clean.append(b)

    # ---- Inicio de deportes ----
    idx_ppv = slice_after_marker(clean, "EVENTOS PPV")

    # "Eventos Deportivos" aparece en men√∫ y en contenido: elegimos el mejor match
    ev_idxs = [i for i, b in enumerate(clean) if "eventos deportivos" in safe_lower(b)]
    idx_ev = -1
    if ev_idxs:
        dow_rx = re.compile(r"\b(lunes|martes|mi√©rcoles|miercoles|jueves|viernes|s√°bado|sabado|domingo)\b", re.IGNORECASE)
        with_dow = [i for i in ev_idxs if dow_rx.search(clean[i] or "")]
        if with_dow:
            idx_ev = with_dow[-1]
        else:
            idx_ev = ev_idxs[-1]

    # Si hay PPV, y el idx_ev cae antes (men√∫), empezamos en PPV
    start_deportes = 0
    if idx_ppv != -1:
        start_deportes = idx_ppv
        if idx_ev != -1 and idx_ev > idx_ppv:
            start_deportes = idx_ev
    else:
        start_deportes = idx_ev if idx_ev != -1 else 0

    # ---- Fin de deportes ----
    end_markers = [
        "PEL√çCULAS RECIENTES", "PELICULAS RECIENTES",
        "SERIES RECIENTES",
        "CONTENIDO NUEVO",
        "PEL√çCULAS", "PELICULAS", "CINE",
        "SERIES", "ESTRENOS",
        "NUEVOS EPISODIOS", "NUEVOS CAPITULOS", "NUEVAS PELICULAS", "NUEVAS SERIES"
    ]
    end_deportes = len(clean)
    for mk in end_markers:
        j = slice_after_marker(clean, mk)
        if j != -1 and j > start_deportes:
            end_deportes = min(end_deportes, j)

    deportes_blocks = clean[start_deportes:end_deportes]
    tail_blocks = clean[end_deportes:]

    return {
        "deportes_blocks": deportes_blocks,
        "tail_blocks": tail_blocks,
        "all_blocks": clean
    }


def _extract_match_teams(line: str) -> tuple[str | None, str | None]:
    """Extrae equipos/participantes de una l√≠nea.
    Soporta: 'A vs B' / 'A v B' / 'A @ B' / 'A - B'
    Evita arrastrar el nombre de la competencia y/o la sede.
    """
    if not line:
        return None, None

    # Normaliza s√≠mbolos/emoji que a veces rompen el separador (ej. "St Mirren‚öΩÔ∏èvsRangers").
    # Importante: NO usamos strip_weird_symbols_keep_plus() aqu√≠ porque elimina '@'.
    def _strip_symbols_for_match(s: str) -> str:
        if not s:
            return ""
        out = []
        for ch in s:
            # Mant√©n letras/n√∫meros (incluye Unicode), espacios y separadores comunes.
            if ch.isalnum() or ch.isspace() or ch in "+-@.:/‚Äô'":
                out.append(ch)
                continue
            import unicodedata as _ud
            cat = _ud.category(ch)
            if cat and (cat[0] in {"L", "N"}):  # letras/n√∫meros unicode
                out.append(ch)
            else:
                out.append(" ")
        s2 = "".join(out)
        s2 = re.sub(r"\s+", " ", s2).strip()
        return s2

    line_clean = _strip_symbols_for_match(line)

    # Quita prefijos tipo "GLOBAL 2-1 ..."
    line = re.sub(r"(?i)\bglobal\b\s*\d{1,2}\s*[-‚Äì]\s*\d{1,2}\b[^A-Za-z0-9√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]*", "", line).strip()

    COMP_PREFIXES = [
        "ncaa basketball", "ncaa womens basketball", "unrivaled basketball",
        "partidos mls", "mls", "pretemporada mlb", "torneo de las 6 naciones",
        "fa womens cup", "ufc fight night",
        "partidos nba", "nba", "premier league", "liga mx femenil", "liga mx",
        "serie b", "primera a", "copa libertadores", "copa del rey",
        "copa de alemania", "copa alemana", "copa de italia", "copa argentina",
        "torneo de reserva", "brasileriao", "brasileirao", "copa afc",
        "liga de campeones de la afc", "eliminatorias sub-17 concacaf",
        "primera division", "copa de campeones concacaf", "concacaf",
        "fa cup",
        "championship",
        "league one",
        "bundesliga 2",
        "bundesliga",
        "ligue 1",
        "segunda division",
        "segunda divisi√≥n",
        "ekstraklasa",
        "eredivisie",
        "campeonato paulista",
        "nascar cup series", "nascar craftsman truck series", "nascar auto parts series", "nascar craftsman", "nascar auto parts", "pelea estelar", "peleas preliminares", "peleas principales"]

    VENUE_KEYS = [
        "estadio","stadium","stadio","arena","coliseum","coliseo","center","centre",
        "park","parque","bridge","field","dome","ground","court","olympiastadion",
        "allianz", "villa park", "city ground", "stamford bridge"
    ]

    RX_TIME_ANY = re.compile(r"\b\d{1,2}(?::\d{2})?\s*(?:a\.?m\.?|p\.?m\.?|am|pm)\b", re.IGNORECASE)

    def _clean_name(s: str) -> str:
        s = strip_weird_symbols_keep_plus(s or "")
        s = re.sub(r"\s{2,}", " ", s).strip()
        low = safe_lower(s)

        # si arrastra nombre de competencia al inicio, qu√≠talo
        for p in COMP_PREFIXES:
            if low.startswith(p + " "):
                s = s[len(p):].strip()
                low = safe_lower(s)
                break

        # corta al encontrar 'por' (canales) o horario/zona
        s = re.split(r"\bpor\b", s, flags=re.IGNORECASE)[0]
        # corta desde la primera hora si aparece (por si el split fall√≥)
        tm = RX_TIME_ANY.search(s)
        if tm:
            s = s[:tm.start()].strip()

        # elimina conectores residuales (ej. "a las", "a la") que quedan antes de la hora
        s = re.sub(r"(?i)\b(?:a\s+las?|a\s+la)\b[^\w]*$", "", s).strip()
        s = re.sub(r"(?i)\b(?:a\s+partir\s+de)\s*$", "", s).strip()

        s = s.strip(" -‚Äì‚Äî:|\t")
        s = re.sub(r"\s{2,}", " ", s).strip()
        low = safe_lower(s)

        # Si arrastra sede/estadio/arena, intenta quedarte SOLO con el equipo (parte posterior).
        # Importante: busca palabras completas para evitar falsos positivos (ej. "Mansfield" contiene "field").
        best_pos = -1
        best_len = 0
        for k in VENUE_KEYS:
            for mm in re.finditer(r"\b" + re.escape(k) + r"\b", low):
                if mm.start() > best_pos:
                    best_pos = mm.start()
                    best_len = len(k)

        if best_pos != -1:
            after = s[best_pos + best_len:].strip(" -‚Äì‚Äî:|	")
            after = re.sub(r"\s{2,}", " ", after).strip()
            if len(after) >= 2:
                s = after

        return s.strip()

    # Separador principal (vs / v / @)
    # Nota: el feed a veces viene con espaciado roto o s√≠mbolos (ej. "vsRangers", "St Mirren‚öΩÔ∏èvsRangers").
    for _cand in (line, line_clean):
        m = re.search(r"(?i)^\s*(.+?)\s+(?:vs\.?|v\.?|va|@)\s+(.+)$", _cand)
        if not m:
            m = re.search(r"(?i)^\s*(.+?)\s+(?:vs\.?|v\.?|va|@)\s*(.+)$", _cand)
        if not m:
            m = re.search(r"(?i)^\s*(.+?)\s*(?:vs\.?|v\.?|va|@)\s+(.+)$", _cand)
        if not m:
            # √öltimo recurso: permite sin espacios, pero exige separador como palabra (@ o vs/v)
            m = re.search(r"(?i)^\s*(.+?)\s*(?:\bvs\.?\b|\bv\.?\b|@)\s*(.+)$", _cand)
        if m:
            left = m.group(1).strip()
            right = m.group(2).strip()
            # recorta right hasta antes de la primera hora (am/pm o 24h)
            tm = RX_TIME_ANY.search(right)
            if tm:
                right = right[:tm.start()].strip()
            else:
                tm24 = re.search(r"\b\d{1,2}:\d{2}\b", right)
                if tm24:
                    right = right[:tm24.start()].strip()
            # limpia conectores residuales
            right = re.sub(r"(?i)\b(?:a\s+las?|a\s+la)\s*$", "", right).strip()
            # tambi√©n recorta por 'por' si no hay hora a√∫n
            right = re.split(r"\bpor\b", right, flags=re.IGNORECASE)[0].strip()
            a = _clean_name(left)
            b = _clean_name(right)
            return a or None, b or None

    # Guion (A - B) SOLO si tiene espacios alrededor

    m2 = re.search(r"^\s*(.+?)\s+[-‚Äì‚Äî]\s+(.+?)\s*$", line)
    if m2:
        a = _clean_name(m2.group(1))
        b = _clean_name(m2.group(2))
        return a or None, b or None

    return None, None




def _extract_sede_inline(line: str, team_a: str | None = None) -> str | None:
    """Intenta extraer sede (Estadio/Arena/...) desde una l√≠nea de evento.
    Soporta sedes sin la palabra "Estadio" (ej: City Ground, Villa Park).
    """
    if not line:
        return None
    low = safe_lower(line)

    # Excepci√≥n: 'Major Arena Soccer League' es nombre de competencia, no sede
    if "major arena soccer league" in low:
        return None

    # si ya sabemos el team_a, y la l√≠nea parece "Sede ... TeamA vs", toma el prefijo como sede
    if team_a:
        pos = safe_lower(line).find(safe_lower(team_a))
        if pos > 0:
            prefix = line[:pos].strip(" -‚Äì‚Äî:|\t")
            if prefix and _is_probable_venue_line(prefix):
                prefix = strip_weird_symbols_keep_plus(prefix)
                if len(prefix) >= 4:
                    return prefix[:120]

    # fallback por keyword
    m = re.search(r"\b(estadio|arena|stadium|stadio|parque|park|recinto|ground|field|dome|centre|center|bridge|allianz|plaza|hall|apex)\b", low)
    if not m:
        return None
    start = m.start()
    end = len(line)
    if team_a:
        pos = safe_lower(line).find(safe_lower(team_a), m.end())
        if pos != -1 and pos > start:
            end = pos
    sede = line[start:end].strip(" -‚Äì‚Äî:|\t")
    sede = strip_weird_symbols_keep_plus(sede)
    if len(sede) < 4:
        return None
    return sede[:120]







def extract_box_match_teams_raw(line: str) -> tuple[str | None, str | None]:
    """Extrae participantes para BOX sin aplicar el limpiador de sedes de otros deportes.

    Objetivo: ser MUY tolerante con el formato real del feed.

    Soporta:
      - 'Tiago Muxanga vs Asinia Byfield a las 12 pm ...'
      - 'Brentwood, Essex Tiago Muxanga vs Asinia Byfield ...'
      - Separador con espacios rotos (ej. 'vsCeltic')

    Nota: NO intenta separar sede; eso lo hace extract_box_inline_venue_and_fighter().
    """
    if not line:
        return None, None

    # Normaliza s√≠mbolos/emoji que a veces rompen el separador.
    # Importante: NO usamos strip_weird_symbols_keep_plus() aqu√≠ porque elimina '@'.
    def _strip_symbols_for_match(s: str) -> str:
        if not s:
            return ""
        out = []
        for ch in s:
            # Mant√©n letras/n√∫meros (incluye Unicode), espacios y separadores comunes.
            if ch.isalnum() or ch.isspace() or ch in "+-@.:/‚Äô'":
                out.append(ch)
                continue
            import unicodedata as _ud
            cat = _ud.category(ch)
            if cat and (cat[0] in {"L", "N"}):
                out.append(ch)
            else:
                out.append(" ")
        s2 = "".join(out)
        s2 = re.sub(r"\s+", " ", s2).strip()
        return s2

    line_clean = _strip_symbols_for_match(line)

    # separador principal (vs / v / @). Intenta con varias tolerancias.
    for cand in (line, line_clean):
        m = re.search(r"(?i)^\s*(.+?)\s*(?:\bvs\.?\b|\bv\.?\b|\bva\b)\s*(.+)$", cand)
        if not m:
            m = re.search(r"(?i)^\s*(.+?)\s*@\s*(.+)$", cand)
        if not m:
            continue

        left = (m.group(1) or "").strip()
        right = (m.group(2) or "").strip()

        # corta right antes de canal o hora
        right = re.split(r"(?i)\bpor\b", right)[0].strip()
        tm = RX_TIME_ANY.search(right) or re.search(r"\b\d{1,2}:\d{2}\b", right)
        if tm:
            right = right[:tm.start()].strip()
        right = re.sub(r"(?i)\b(?:a\s+las?|a\s+la)\b[^\w]*$", "", right).strip()

        # limpia left (a veces arrastra 'a las' si el horario se peg√≥)
        left = re.split(r"(?i)\bpor\b", left)[0].strip()
        tmL = RX_TIME_ANY.search(left) or re.search(r"\b\d{1,2}:\d{2}\b", left)
        if tmL:
            left = left[:tmL.start()].strip()
        left = re.sub(r"(?i)\b(?:a\s+las?|a\s+la)\b[^\w]*$", "", left).strip()

        left = re.sub(r"\s{2,}", " ", left).strip(" -‚Äì‚Äî:|\t")
        right = re.sub(r"\s{2,}", " ", right).strip(" -‚Äì‚Äî:|\t")

        return (left or None), (right or None)

    return None, None


def extract_box_inline_venue_and_fighter(team_a: str, raw_line: str, context_comp: str | None = None) -> tuple[str, str | None]:
    """En BOX a veces la l√≠nea viene como: 'Plaza Juarez Yoali Mejia vs Miguel Nieblas'
    o 'Brentwood, Essex Tiago Muxanga vs Asinia Byfield'.

    Separa sede (venue) y peleador A para evitar confundir sedes con equipos.
    Devuelve (fighter_a, venue_or_None).

    Activaci√≥n:
      - Por default, solo si hay se√±ales fuertes de sede (coma o keyword t√≠pica de lugar).
      - En Evento BOX (Reino Unido) / üè¥ü•ä tambi√©n permite sedes sin coma (ej. 'Brentwood Essex').
    """
    if not team_a:
        return team_a, None

    a = str(team_a).strip()
    if not a:
        return a, None

    venue_kw = re.compile(r"(?i)\b(arena|apex|plaza|center|centre|coliseum|coliseo|stadium|stadio|park|parque|ground|dome|casino|hall|sportzentrum|zentrum|pavilion|pabellon|pabell√≥n)\b")

    # UK/üè¥: hay feeds donde la sede viene SIN coma ni keywords (ej. 'Brentwood Essex').
    ctx_low = safe_lower(context_comp or "")
    raw_low = safe_lower(raw_line or "")
    uk_hints = {
        "essex","kent","surrey","sussex","london","manchester","birmingham","liverpool","leeds",
        "glasgow","edinburgh","cardiff","wales","scotland","england","brentwood",
        # condados comunes (mejora recall en sedes sin coma)
        "yorkshire","lancashire","cheshire","cumbria","norfolk","suffolk","devon","cornwall","dorset",
        "hampshire","berkshire","wiltshire","gloucestershire","warwickshire","leicestershire","derbyshire",
        "nottinghamshire","oxfordshire","buckinghamshire","cambridgeshire","northamptonshire","lincolnshire",
        "hertfordshire","bedfordshire","staffordshire","worcestershire","herefordshire","shropshire",
    }
    prefer_uk_location_split = (
        ("reino unido" in ctx_low) or (" uk" in (" " + ctx_low + " ")) or (" gb" in (" " + ctx_low + " ")) or
        ("üè¥" in (raw_line or "")) or ("üè¥" in (context_comp or "")) or ("üá¨üáß" in (raw_line or "")) or
        any(h in raw_low for h in uk_hints) or re.search(r"\b[a-z]+shire\b", raw_low)
    )

    def _looks_like_uk_location(v: str) -> bool:
        vl = safe_lower(v or "")
        if not vl:
            return False
        if any(h in vl for h in uk_hints):
            return True
        if re.search(r"\b[a-z]+shire\b", vl):
            return True
        # 'North London', 'West Sussex', etc.
        if re.search(r"\b(?:north|south|east|west)\s+[a-z]{3,}\b", vl):
            return True
        return False

    # Sin se√±ales, no tocar (salvo UK sin coma).
    if ("," not in a) and (not venue_kw.search(a)) and (not prefer_uk_location_split):
        return a, None

    tokens = a.split()
    if len(tokens) < 4:
        return a, None

    # Probamos tomar √∫ltimos 2-4 tokens como nombre del peleador.
    for n in (2, 3, 4):
        if len(tokens) <= n:
            continue
        fighter = " ".join(tokens[-n:]).strip()
        venue = " ".join(tokens[:-n]).strip()
        if not venue:
            continue

        # Venue debe tener se√±ales fuertes... salvo UK donde permitimos sedes sin coma.
        if ("," not in venue) and (not venue_kw.search(venue)):
            if not (prefer_uk_location_split and _looks_like_uk_location(venue)):
                continue

        # Fighter debe parecer nombre (2+ palabras con inicial may√∫scula)
        f_tokens = fighter.split()
        if any(',' in t for t in f_tokens):
            continue
        caps = sum(1 for t in f_tokens if t and t[0].isalpha() and t[0].isupper())
        if caps < 2:
            continue

        # Evita que 'venue' contenga 'vs'
        if re.search(r"(?i)\bvs\b", venue):
            continue

        return fighter, venue

    return a, None

    venue_kw = re.compile(r"(?i)\b(arena|apex|plaza|center|centre|coliseum|coliseo|stadium|stadio|park|parque|ground|dome|casino|hall|sportzentrum|zentrum|pavilion|pabellon|pabell√≥n)\b")

    # Sin se√±ales, no tocar.
    if ("," not in a) and (not venue_kw.search(a)):
        return a, None

    tokens = a.split()
    if len(tokens) < 4:
        return a, None

    # Probamos tomar √∫ltimos 2-4 tokens como nombre del peleador.
    for n in (2, 3, 4):
        if len(tokens) <= n:
            continue
        fighter = " ".join(tokens[-n:]).strip()
        venue = " ".join(tokens[:-n]).strip()
        if not venue:
            continue

        # Venue debe tener se√±ales fuertes
        if ("," not in venue) and (not venue_kw.search(venue)):
            continue

        # Fighter debe parecer nombre (2+ palabras con inicial may√∫scula)
        f_tokens = fighter.split()
        if any(',' in t for t in f_tokens):
            continue
        caps = sum(1 for t in f_tokens if t and t[0].isalpha() and t[0].isupper())
        if caps < 2:
            continue

        # Evita que 'venue' contenga 'vs'
        if re.search(r"(?i)\bvs\b", venue):
            continue

        return fighter, venue

    return a, None

def parse_sports_items(deportes_blocks: list[str]) -> list[dict]:
    """
    Parser robusto (stateful) para DEPORTES:
      - Detecta correctamente t√≠tulos de competencias/eventos (NBA, NCAA, WWE, Olimpiadas, etc.)
      - Jornada/Fase/Ronda NO se "riegan": se aplican solo dentro de la competencia actual.
      - Soporta info multi-bloque: match -> hora -> canales -> sede.
      - Soporta marcador GLOBAL (IDA/VUELTA) y lo asocia al evento correcto.
      - Evita crear competencias ficticias como "Otros".
    """
    mem = get_memory()
    league_aliases = mem["league_aliases"]
    channel_aliases = mem["channel_aliases"]

    # Se√±ales del d√≠a (NO inferir competencias que no aparezcan en Kaelus)
    _src_blob = "\n".join([str(b) for b in (deportes_blocks or []) if b])
    _src_low = safe_lower(_src_blob)

    SOURCE_HAS_FA_CUP = bool(re.search(r"(?i)\bfa\s*cup\b", _src_blob)) or ("copa de inglaterra" in _src_low)
    # BOX/Boxeo puede venir como "Evento BOX", "BOXü•ä", etc.
    SOURCE_HAS_BOX = ("ü•ä" in _src_blob) or bool(re.search(r"(?i)\bbox(?:eo)?\b", _src_blob)) or bool(re.search(r"(?i)\bevento\s*box", _src_blob))


    THIN_NOISE = {"deportes", "deport√©s", "otros", "ppv", "eventos ppv"}
    STOP_PHRASES = [
        "use tab to navigate", "privacy", "cookies", "suscr√≠bete", "suscribete",
        "kaelus soporte", "herramientas", "contacto"
    ]

    # ------------------------------------------------------------
    # Rescate por equipos (cuando Wix pierde el encabezado de la liga)
    #
    # Caso real reportado:
    #   - "Championshipüè¥‚öΩÔ∏è" y "Primera Aüá™üá®‚öΩÔ∏è" a veces NO llegan como header
    #     separado; el partido se queda pegado a la liga anterior (Ekstraklasa,
    #     Primera A Colombia, etc.) y por eso "desaparece".
    #
    # Estrategia: si detectamos un partido (A vs B) y ambos equipos pertenecen
    # a un set muy espec√≠fico de la liga, reasignamos la competencia SOLO para
    # ese evento (sin re-etiquetar todo el segmento).
    # ------------------------------------------------------------
    def _fold_key(s: str) -> str:
        s2 = safe_lower(strip_weird_symbols_keep_plus(s or ""))
        try:
            s2 = unicodedata.normalize("NFD", s2)
            s2 = "".join(ch for ch in s2 if unicodedata.category(ch) != "Mn")
        except Exception:
            pass
        s2 = re.sub(r"[^a-z0-9 ]+", " ", s2)
        s2 = re.sub(r"\s+", " ", s2).strip()
        return s2

    # EFL Championship (Inglaterra) ‚Äî m√≠nimo para evitar falsos positivos
    EFL_CHAMP_TEAMS = {
        # EFL Championship / Inglaterra (lista ampliada para rescates cuando se pierde el encabezado)
        "blackburn rovers", "preston north end", "preston",
        "coventry city", "west bromwich", "west bromwich albion",
        "stoke city", "leicester city", "southampton", "charlton athletic",
        "hull city", "cardiff city", "swansea city", "bristol city", "millwall",
        "queens park rangers", "qpr",
        "sheffield united", "sheffield wednesday",
        "middlesbrough", "norwich city", "watford", "derby county",
        "sunderland", "ipswich town",
    }

    # Liga Pro / Primera A Ecuador
    ECUADOR_LIGA_TEAMS = {
        "orense", "ldu quito", "liga de quito", "liga quito",
        "ldu",
        "barcelona", "barcelona sc", "emelec",
        "independiente del valle", "el nacional", "delfin", "delfin sc",
        "mushuc runa", "aucas", "universidad catolica", "u catolica",
        "tecnico universitario", "cumbaya",
    }

    # A-League (Australia) ‚Äî para rescate si el header se pierde
    ALEAGUE_TEAMS = {
        "adelaide united", "perth glory", "wellington phoenix",
        "auckland", "sydney fc", "brisbane roar",
        "central coast", "central coast mariners", "melbourne city",
        "newcastle jets",
    }

    # A-League Women (Australia) ‚Äî pistas de equipos/nombres que suelen aparecer SIN "Wanderers/Mariners".
    # Se usa solo como heur√≠stica cuando Wix pierde el header.
    ALEAGUE_WOMEN_HINTS = {
        "canberra united",
        "western sydney",
        "central coast",
    }

    def _infer_comp_from_match_teams(a: str | None, b: str | None) -> str | None:
        if not a or not b:
            return None
        ka, kb = _fold_key(a), _fold_key(b)
        # requiere ambos equipos dentro del set para evitar falsos positivos
        if ka in EFL_CHAMP_TEAMS and kb in EFL_CHAMP_TEAMS:
            return "Championship"
        if ka in ECUADOR_LIGA_TEAMS and kb in ECUADOR_LIGA_TEAMS:
            return "Primera A Ecuador"
        if ka in ALEAGUE_TEAMS and kb in ALEAGUE_TEAMS:
            if (ka in ALEAGUE_WOMEN_HINTS) or (kb in ALEAGUE_WOMEN_HINTS):
                return "A-League Women"
            return "A-League"
        return None

    
    def _is_probable_venue_line(s: str) -> bool:
        low = safe_lower(s)

        # No confundir l√≠neas de horario (\"Este/Centro/Pac√≠fico\") con sedes.
        if _looks_like_time_line(s):
            return False

        # Excepci√≥n: 'Major Arena Soccer League' es competencia, no sede
        if "major arena soccer league" in low:
            return False

        # Heur√≠stica por coma: ubicaciones tipo "Brentwood, Essex" / "Las Vegas, NV"
        # (sin se√±ales de competencia/partido/canal/horario)
        if "," in low:
            if not re.search(r"(?i)\b(vs\.?|@|jornada|ronda|fase|semifinal|cuartos|octavos|final|liga|league|copa|cup|por|canal|tv|ppv)\b", low):
                if not RX_TIME_ANY.search(s) and not re.search(r"\b\d{1,2}:\d{2}\b", s):
                    parts = [p.strip(" -‚Äì‚Äî:|\t") for p in s.split(",") if p.strip()]
                    # 2-4 partes suele ser "Ciudad, Regi√≥n/Estado, Pa√≠s"
                    if 2 <= len(parts) <= 4 and len(s) <= 70:
                        # Debe haber letras en cada parte y casi nada de d√≠gitos
                        if all(re.search(r"[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]", p) for p in parts) and (sum(len(re.findall(r"\d", p)) for p in parts) <= 2):
                            return True

        # Palabras clave t√≠picas de sedes / recintos
        if bool(re.search(
            r"\b(sede|estadio|stadium|stadio|arena|coliseum|coliseo|recinto|parque|park|ground|court|center|centre|centro|field|dome|"
            r"olympiastadion|stade|plaza|auditorio|gimnasio|sportzentrum|zentrum|pavilion|pabellon|pabell√≥n|"
            r"bridge|allianz|hall|apex|madison|mauroy|jean\s+bouin|bernabeu|pizjuan|pizju√°n|sanchez|s√°nchez|velodrome)\b",
            low
        )):
            return True

        # Venue sin keyword (casos cl√°sicos): City Ground, Villa Park, etc.
        return any(k in low for k in [
            "universitario", "maradona", "hotspur", "santiago bernabeu",
            "ramon s√°nchez pizju√°n", "ram√≥n s√°nchez pizju√°n", "villa park", "city ground",
            # sedes reportadas (sin keyword tipo Stadium/Arena)
            "via del mare", "v√≠a del mare", "unipol domus", "parc des princes",
            "el sadar", "riyadh air metropolitano",
            # otras sedes frecuentes sin keyword
            "alten forsterei",
        ])

    def _extract_global(s: str) -> str | None:
        if "global" not in safe_lower(s):
            return None
        # GLOBAL 4-1 San Diego ...
        m = re.search(r"(?i)\bGLOBAL\b\s*([0-9]{1,2}\s*[-‚Äì]\s*[0-9]{1,2})(?:\s+([^,|]+))?", s)
        if not m:
            return "GLOBAL"
        score = re.sub(r"\s+", "", m.group(1)).replace("‚Äì", "-")
        tail = (m.group(2) or "").strip()
        # corta antes de horarios
        tail = re.split(r"(?i)\b(?:hora|a\s+las)\b", tail)[0].strip()
        tail = re.split(r"\b\d{1,2}(?::\d{2})?\s*(?:a\.?m\.?|p\.?m\.?|am|pm)\b", tail, flags=re.IGNORECASE)[0].strip()
        return f"GLOBAL {score} {tail}".strip()

    def _extract_stage_text(s: str) -> str | None:
        low = safe_lower(s)

        def _ordinal_es(n: int) -> str:
            # Abreviaturas comunes en espa√±ol: 1ra, 2da, 3ra, 4ta...
            if n == 1:
                return "1ra"
            if n == 2:
                return "2da"
            if n == 3:
                return "3ra"
            if n == 7:
                return "7ma"
            if n == 8:
                return "8va"
            if n == 9:
                return "9na"
            if n == 10:
                return "10ma"
            return f"{n}ta"

        def _round_label(n: int) -> str:
            parts = [f"{_ordinal_es(n)} Ronda"]
            if re.search(r"\bida\b", low):
                parts.append("IDA")
            elif re.search(r"\bvuelta\b", low):
                parts.append("Vuelta")
            if "clasific" in low:
                parts.append("Clasificaci√≥n")
            return " ".join(parts)


        # Jornada
        mj = RX_JORNADA.search(s)
        if mj:
            return f"Jornada#{mj.group(1)}"

        # Ronda (num√©rica) - acepta '4taRonda' sin espacios
        mr = re.search(r"\b(\d{1,2})\s*(?:ta|¬™|a)?\s*ronda\b", low, flags=re.IGNORECASE)
        if mr:
            n = mr.group(1)
            return _round_label(int(n))

        # Ronda (con palabra) - acepta 'CuartaRonda'
        word_map = {
            "primera": "1",
            "segunda": "2",
            "tercera": "3",
            "cuarta": "4",
            "quinta": "5",
            "sexta": "6",
            "s√©ptima": "7",
            "septima": "7",
            "octava": "8",
            "novena": "9",
            "d√©cima": "10",
            "decima": "10",
        }
        mw = re.search(r"\b(" + "|".join(word_map.keys()) + r")\s*ronda\b", low, flags=re.IGNORECASE)
        if mw:
            n = word_map.get(safe_lower(mw.group(1)), "")
            if n:
                return _round_label(int(n))

        # Fases/Rondas t√≠picas (incluye ida/vuelta)
        patterns = [
            r"fase\s+de\s+grupos",
            r"cuartos\s+de\s+final(?:\s+(?:de\s+)?(ida|vuelta))?",
            r"semifinal(?:es)?(?:\s+(?:de\s+)?(ida|vuelta))?",
            r"octavos\s+de\s+final(?:\s+(?:de\s+)?(ida|vuelta))?",
            r"dieciseisavos\s+de\s+final(?:\s+(?:de\s+)?(ida|vuelta))?",
            r"final(?:es)?(?:\s+(?:de\s+)?(ida|vuelta))?\b",
            r"primer[as]?\s*ronda\s+(?:de\s+)?(ida|vuelta)",
            r"primers?\s*ronda\s+(?:de\s+)?(ida|vuelta)",
            r"fase\s+previa",
            r"play\s*offs?",
        ]
        for p in patterns:
            m = re.search(p, low, flags=re.IGNORECASE)
            if m:
                txt = m.group(0)
                txt = re.sub(r"\s+", " ", txt).strip()
                words = [w.capitalize() for w in txt.split()]
                for wi in range(1, len(words)):
                    if words[wi] in {"De","Del","La","Las","Los","Y","A","Al","En"}:
                        words[wi] = words[wi].lower()
                for wi in range(len(words)):
                    if words[wi].lower() == "ida":
                        words[wi] = "IDA"
                    elif words[wi].lower() == "vuelta":
                        words[wi] = "Vuelta"
                return " ".join(words)

        # WWE: promos (si la competencia actual es WWE)
        if low.strip() in {"nxt", "raw", "smackdown", "dynamite"}:
            return low.strip().upper()

        return None

    def _normalize_competition_extended(s: str) -> str | None:
        t = safe_lower(s)
        # versi√≥n sin acentos para matching robusto (Ol√≠mpicos vs Olimpicos)
        t_fold = t
        try:
            t_fold = unicodedata.normalize("NFD", t_fold)
            t_fold = "".join(ch for ch in t_fold if unicodedata.category(ch) != "Mn")
        except Exception:
            pass
        cleaned = strip_weird_symbols_keep_plus(s)
        t_clean = safe_lower(cleaned)

        # ignora gen√©ricos
        if t_clean.strip() in THIN_NOISE:
            return None
        if "eventos deportivos" in t:
            return None


        # Canal/broadcaster: "Liga 1 Max" (Per√∫). No es competencia.
        if "liga 1 max" in t_clean:
            return None

        # Championship (EFL - Inglaterra): a veces el HTML aplanado mezcla l√≠neas
        # y este token queda embebido. Si aparece expl√≠citamente y viene con se√±ales
        # de f√∫tbol/UK (‚öΩ/üè¥ o un 'vs'), lo respetamos.
        if re.search(r"(?i)\bchampionship\b", t_clean):
            if ("üè¥" in (s or "")) or ("‚öΩ" in (s or "")) or (" vs " in f" {t_clean} "):
                return "Championship"

        # League One (EFL - Inglaterra, 3ra divisi√≥n): misma l√≥gica que Championship.
        if re.search(r"(?i)\bleague\s*one\b", t_clean):
            if ("üè¥" in (s or "")) or ("‚öΩ" in (s or "")) or (" vs " in f" {t_clean} ") or ("england" in t) or ("inglaterra" in t):
                return "League One"

        # --- reglas expl√≠citas por texto/emoji (evita confusiones) ---
        # Copas gen√©ricas por bandera
        if t_clean.strip() == "copa" and "üá©üá™" in s:
            return "Copa de Alemania"
        if t_clean.strip() == "copa" and "üáÆüáπ" in s:
            return "Copa de Italia"
        if t_clean.strip() == "copa" and "üá¶üá∑" in s:
            return "Copa Argentina"

        # Torneos espec√≠ficos (preferir esto antes de alias/AI)
        if "copa del rey" in t:
            return "Copa del Rey"
        if "copa" in t and "üá©üá™" in s:
            return "Copa de Alemania"
        if "copa" in t and "üá¶üá∑" in s and "reserva" not in t:
            return "Copa Argentina"
        if ("torneo" in t and "reserva" in t) and "üá¶üá∑" in s:
            return "Torneo de Reserva Argentina"
        if "brasiler" in t or "brasileirao" in t:
            return "Brasileirao"
        if "campeonato paulista" in t_clean and ("üáßüá∑" in s or "brasil" in t):
            return "Campeonato Paulista"
        if ("serie b" in t_clean) and ("serie a" not in t_clean):
            # Usualmente viene como "Serie BüáÆüáπ‚öΩ"; si no trae bandera, igual la respetamos.
            if ("üáÆüáπ" in s) or ("italia" in t) or (t_clean.strip().startswith("serie b")):
                return "Serie B"
        if "uefa" in t and "champions league" in t and ("femen" in t or "women" in t or "womens" in t):
            return "UEFA Champions League Femenil"
        if "uefa womens europa" in t or "uefa women's europa" in t or ("uefa" in t and "womens europa" in t):
            return "UEFA Womens Europa Cup"
        if "copa afc" in t:
            return "Copa AFC"
        if "primera division" in t and ("üá≠üá≥" in s or "honduras" in t):
            return "Primera Division Honduras"
        # Honduras: en Kaelus suele venir como "Liga Nacionalüá≠üá≥‚öΩÔ∏è".
        # Si no se normaliza, el bot pierde el contexto pa√≠s y puede mostrar el t√≠tulo sin bandera.
        if "liga nacional" in t_clean and ("üá≠üá≥" in s or "honduras" in t or "hn" in t_clean):
            return "Liga Nacional Honduras"
        # Australia soccer (A-League / A-League Women)
        # Nota: en algunos d√≠as el alias/memory puede mapear 'A-League' y comerse el sufijo 'Women'.
        # Por eso detectamos primero variantes expl√≠citas antes de caer a alias.
        if re.search(r"(?i)\ba[\-\s]?league\b", t_clean) or ("a-league" in t_clean) or ("a league" in t_clean):
            # Women / Women's / variantes
            if re.search(r"(?i)\bwomen(?:'s)?\b", t_clean) or re.search(r"(?i)\bfemen", t_clean):
                return "A-League Women"
            # Fallback: buscar en el texto original (por si el cleaning separ√≥ tokens)
            if re.search(r"(?i)\bwomen(?:'s)?\b", t) or re.search(r"(?i)\bfemen", t):
                return "A-League Women"
            return "A-League"

        # Portugal leagues
        if "primeira liga" in t_clean and ("üáµüáπ" in s or "portugal" in t):
            return "Primeira Liga"
        if "segunda liga" in t_clean and ("üáµüáπ" in s or "portugal" in t):
            return "Segunda Liga"

        # Argentina soccer
        if "liga profesional" in t_clean and ("üá¶üá∑" in s or "argentina" in t):
            return "Liga Profesional"

        # Golf tours
        if "dp world tour" in t_clean:
            return "DP World Tour"
        if "pga tour" in t_clean:
            return "PGA Tour"

        # MMA / UFC
        if ("ufc" in t_clean) and ("fight night" in t_clean):
            return "UFC Fight Night"

        # MLS (Estados Unidos / Canad√°)
        if "partidos mls" in t_clean or (t_clean.strip() == "mls") or (("mls" in t_clean) and ("partidos" in t_clean)):
            return "Partidos MLS"

        # MLB Spring Training
        if "pretemporada mlb" in t_clean:
            return "Pretemporada MLB"

        # FA Women's Cup (Inglaterra)
        if ("fa womens cup" in t_clean) or ("fa women's cup" in t_clean):
            return "FA Womens Cup"

        # Torneo de las 6 Naciones (Rugby). A veces NO trae la palabra 'rugby', solo el emoji üèâ.
        if ("seis naciones" in t_clean) or ("6 naciones" in t_clean) or ("six nations" in t_clean):
            if ("üèâ" in (s or "")) or ("torneo" in t_clean) or ("rugby" in t_clean):
                return "Torneo de las 6 Naciones"


        if "unrivaled" in t and "basket" in t:
            return "Unrivaled Basketball"

        # ---- NUEVAS LIGAS / EVENTOS (2026-02) ----
        # BOX: soporta m√∫ltiples variantes (con/sin "Evento", con/sin bandera, con "BOXus/BOXusa", etc.)
        # Ejemplos: "Evento BOXüá©üá™ü•ä", "Evento BOXüá≤üáΩü•ä", "BOXü•ä", "BOX", "Evento BOX üè¥ü•ä", "Evento BOXüá¨üáßü•ä"
        is_box_header = False
        if re.search(r"(?i)\bevento\s*box(?:eo)?\b", t_clean):
            is_box_header = True
        elif re.search(r"(?i)\bbox(?:eo)?\b", t_clean) and ("ü•ä" in s):
            is_box_header = True
        elif t_clean.strip() in {"box", "boxeo"}:
            is_box_header = True

        if is_box_header:
            country = None
            if ("üá©üá™" in s) or ("üá©üá™" in cleaned) or re.search(r"(?i)\bbox\s*(?:de|ger|germany)\b", t_clean) or ("boxde" in t_clean) or ("alemania" in t):
                country = "Alemania"
            elif ("üá≤üáΩ" in s) or re.search(r"(?i)\bbox\s*mx\b", t_clean) or ("boxmx" in t_clean) or ("mexico" in t) or ("m√©xico" in t):
                country = "M√©xico"
            elif ("üá∫üá∏" in s) or re.search(r"(?i)\bbox\s*(?:us|usa)\b", t_clean) or ("boxus" in t_clean) or ("boxusa" in t_clean) or ("estados unidos" in t) or ("united states" in t):
                country = "Estados Unidos"
            elif ("üè¥" in s) or ("üá¨üáß" in s) or re.search(r"(?i)\bbox\s*gb\b", t_clean) or ("boxgb" in t_clean) or ("uk" in t) or ("reino unido" in t) or ("inglaterra" in t) or ("brit" in t) or ("london" in t):
                country = "Reino Unido"
            return f"Evento BOX ({country})" if country else "Evento BOX"

        # FA Cup (Copa de Inglaterra) y ligas inglesas
        # Nota: a veces viene como "FACup" o "FAcup" sin espacio.
        if re.search(r"(?i)fa\s*cup\b", t_clean) or re.search(r"(?i)\bf\.?a\.?\s*cup\b", t_clean) or re.search(r"(?i)\bfacup\b", t_clean) or ("copa de inglaterra" in t_clean):
            return "FA Cup"
        # Si por alguna raz√≥n viene raro, pero trae bandera + "Cuarta Ronda" y contexto, es FA Cup.
        if ("üè¥" in s) and ("ronda" in t_clean) and ("cuarta" in t_clean or "4ta" in t_clean or "fourth" in t_clean) and ("cup" in t_clean or "fa" in t_clean):
            return "FA Cup"
        if t_clean.strip() == "championship" and "vs" not in t_clean:
            return "Championship"
        if t_clean.startswith("championship") and "vs" in t_clean:
            return "Championship"
        if "league one" in t_clean:
            return "League One"

        # Liga Femenil de Inglaterra
        if "womens super league" in t_clean or "women's super league" in t_clean:
            return "Womens Super League"

        # Ligas europeas espec√≠ficas
        if "ekstraklasa" in t_clean:
            return "Ekstraklasa"
        if "serie a" in t_clean and ("üáÆüáπ" in s or "italia" in t):
            return "Serie A"
        if "ligue 1" in t_clean and ("üá´üá∑" in s or "francia" in t):
            return "Ligue 1"
        if re.search(r"\bligue\s*1\b", t_clean):
            return "Ligue 1"
        if "bundesliga 2" in t_clean and ("üá©üá™" in s or "alemania" in t):
            return "Bundesliga 2"
        if ("segunda division" in t_clean or "segunda divisi√≥n" in t_clean) and ("üá™üá∏" in s or "espa√±a" in t):
            return "Segunda Divisi√≥n"
        if "eredivisie" in t_clean or "erredivis" in t_clean:
            return "Eredivisie"

        # Turqu√≠a / Argentina / Arabia Saudita
        if "super lig" in t_clean and ("üáπüá∑" in s or "turquia" in t or "turqu√≠a" in t):
            return "Super Lig"
        if "super liga" in t_clean and ("üá¶üá∑" in s or "argentina" in t):
            return "Super Liga Argentina"
        if "pro league" in t_clean and ("üá∏üá¶" in s or "saudi" in t or "arabia" in t):
            return "Saudi Pro League"

        # F√∫tbol femenil
        if "la liga femenil" in t_clean and ("üá™üá∏" in s or "espa√±a" in t):
            return "La Liga Femenil"

        # Otros deportes / ligas
        if "ncaa" in t_clean and "softball" in t_clean:
            return "NCAA Softball"
        if "liga mexicana" in t_clean and ("softbol" in t_clean or "softball" in t_clean):
            return "Liga Mexicana de Softbol"
        if "major arena soccer league" in t_clean:
            return "Major Arena Soccer League"
        if ("seis naciones" in t_clean or "6 naciones" in t_clean or "six nations" in t_clean) and (("üèâ" in (s or "")) or ("rugby" in t_clean) or ("torneo" in t_clean)):
            return "Torneo de las 6 Naciones"
        if "all elite wrestling" in t_clean:
            return "All Elite Wrestling"
        if "lpga" in t_clean and "tour" in t_clean:
            return "LPGA Tour"

        if "pga tour" in t_clean and "golf" in t_clean:
            return "PGA Tour Golf"

        # Primera Divisi√≥n (por bandera)
        if ("primera division" in t_clean or "primera divisi√≥n" in t_clean) and "üáµüá™" in s:
            return "Primera Divisi√≥n Per√∫"
        if ("primera division" in t_clean or "primera divisi√≥n" in t_clean) and "üá®üá∑" in s:
            return "Primera Divisi√≥n Costa Rica"
        if ("primera division" in t_clean or "primera divisi√≥n" in t_clean) and "üá®üá±" in s:
            return "Primera Divisi√≥n Chile"
        # --- Disambiguaciones cr√≠ticas (ANTES de league_aliases) ---
        # Liga MX Femenil NO debe caer en 'Liga MX'
        if ("liga mx" in t_clean or "ligamx" in t_clean) and ("femen" in t_clean):
            return "Liga MX Femenil"
        # Serie A / Serie B: no depender de bandera (a veces el emoji llega roto)
        if re.search(r"\bserie\s*a\b", t_clean):
            return "Serie A"
        if re.search(r"\bserie\s*b\b", t_clean):
            return "Serie B"
        # Bundesliga vs Bundesliga 2
        if re.search(r"\bbundesliga\s*2\b", t_clean):
            return "Bundesliga 2"
        if ("bundesliga" in t_clean) and ("2" not in t_clean):
            return "Bundesliga"
        # NASCAR
        if "nascar" in t_clean and "cup series" in t_clean:
            return "NASCAR Cup Series"

        if "nascar" in t_clean and ("craftsman" in t_clean or "truck series" in t_clean):
            return "NASCAR Craftsman Truck Series"
        if "nascar" in t_clean and ("auto parts" in t_clean or "echo park" in t_clean):
            return "NASCAR Auto Parts Series"

        # alias conocidos del memory (luego de reglas expl√≠citas)
        comp = normalize_competition(s, league_aliases)
        if comp:
            return comp

        # heur√≠sticas ampliadas
        if "copa alemana" in t or "dfb pokal" in t:
            return "Copa Alemana"
        if "copait" in t or "copa it" in t or "copa italiana" in t or "coppa italia" in t:
            return "Copa de Italia"
        if "copa" in t and ("üáÆüáπ" in s or "italia" in t):
            return "Copa de Italia"
        if "libertadores" in t:
            return "Copa Libertadores"
        if "concacaf" in t and ("campeones" in t or "champions" in t):
            return "Copa de Campeones CONCACAF"
        if "liga mx" in t and "femen" in t:
            return "Liga MX Femenil"
        if ("liga expansion" in t or "expansion mx" in t or ("expansi" in t and "mx" in t)):
            return "Liga Expansi√≥n MX"
        if "segunda liga" in t and ("portugal" in t or "pt" in t or "üáµüáπ" in s):
            return "Liga de Segunda Division de Portugal"
        if "liga de campeones afc" in t or ("afc" in t and "campeones" in t):
            return "Liga de Campeones de la AFC"
        if "eliminatorias" in t and "sub-17" in t and "concacaf" in t:
            return "Eliminatorias Sub-17 CONCACAF"
        if "partidos nba" in t or (t_clean.strip() == "nba") or (" nba" in t and "ncaa" not in t):
            return "Partidos NBA"
        if "ncaa" in t and "gymnast" in t:
            return "NCAA Womens Gymnastics"
        
        if "ncaa" in t and ("women" in t or "womens" in t or "femen" in t):
            return "NCAA Womens Basketball"
        if "ncaa" in t and "basketball" in t:
            return "NCAA Basketball"
        if "primera a" in t and ("col" in t or "colombia" in t or "üá®üá¥" in s):
            return "Primera A Colombia"
        if "primera a" in t and ("ecu" in t or "ecuador" in t or "üá™üá®" in s):
            return "Primera A Ecuador"
        if "wwe" in t or ("wrestling" in t and "wwe" in t):
            return "WWE Wrestling"
        if ("juegos olimp" in t_fold or "olimpic" in t_fold) and "invierno" in t_fold:
            # por tu regla: InviernoIT = Italia 2026
            if "üáÆüáπ" in s or "it" in t or "italia" in t or "2026" in t:
                return "Juegos Ol√≠mpicos de Invierno (Italia 2026)"
            return "Juegos Ol√≠mpicos de Invierno"

        # Premier League (a veces s√≥lo viene como encabezado con emoji)
        if "premier league" in t:
            return "Premier League"

        # fallback: solo si realmente parece un encabezado de competencia (no partido/horario)
        if (
            any(k in t for k in ["copa", "liga", "eliminatoria", "olimp", "wwe", "ncaa", "nba", "premier", "serie", "brasiler", "unrivaled"])
            and (" vs " not in f" {t} ")
            and ("por" not in t)
            and ("global" not in t)
            and (not _looks_like_time_line(s))
        ):
            cleaned2 = cleaned
            cleaned2 = re.sub(r"(?i)\b(?:jornada|fecha)\s*#?\s*\d+\b", "", cleaned2).strip()
            cleaned2 = re.sub(r"(?i)\b(?:fase\s+de\s+grupos|cuartos\s+de\s+final(?:\s+(?:ida|vuelta))?|semifinal(?:es)?(?:\s+(?:ida|vuelta))?|octavos\s+de\s+final(?:\s+(?:ida|vuelta))?|dieciseisavos\s+de\s+final(?:\s+(?:ida|vuelta))?|final(?:es)?(?:\s+(?:ida|vuelta))?|primera\s+ronda(?:\s+(?:ida|vuelta))?)\b", "", cleaned2).strip()
            cleaned2 = re.sub(r"\s{2,}", " ", cleaned2).strip(" -‚Äì‚Äî:|")
            if 3 <= len(cleaned2) <= 60:
                return cleaned2 or None

        return None

    def _strip_comp_prefix(title: str, comp: str) -> str:
        if not title or not comp:
            return title
        t = title.strip()
        c = comp
        c_base = re.sub(r"\s*\(.*?\)\s*", "", c).strip()
        if safe_lower(t).startswith(safe_lower(c_base)):
            t = t[len(c_base):].strip(" -‚Äì‚Äî:|")

        # BOX: encabezado abreviado tipo 'BOXü•ä' / 'BOXEOü•ä' (sin 'Evento')
        if safe_lower(comp).startswith("evento box"):
            t_clean = safe_lower(strip_weird_symbols_keep_plus(t))
            if t_clean.startswith("box"):
                t2 = strip_weird_symbols_keep_plus(t)
                t2 = re.sub(r"(?i)^\s*box(?:eo)?\b", "", t2).strip(" -‚Äì‚Äî:|")
                return t2

        return t

    def _looks_like_time_line(s: str) -> bool:
        # Para DEPORTES exigimos am/pm, as√≠ evitamos falsos positivos como "Jornada#26".
        return bool(RX_TIME_AMPM.search(s or ""))


    # ------------------------------------------------------------------
    # Motor inteligente (reglas + scoring) para detectar Liga Escocesa.
    # En la fuente a veces viene mal etiquetada como "Premier League".
    # Regla ra√≠z: si aparecen equipos de la Scottish Premiership => Liga Escocesa.
    # ------------------------------------------------------------------

    # ------------------------------------------------------------------
    # Motor "IA" (scoring + fuzzy) para identificar Scottish Premiership.
    # En la fuente a veces viene mal etiquetada como "Premier League".
    # Regla ra√≠z (la tuya):
    #   - si aparece CUALQUIER equipo de esta lista dentro del bloque => "Liga Escocesa"
    #   - si NO aparece ninguno => se deja como "Premier League" (Inglaterra)
    # ------------------------------------------------------------------

    SCOTTISH_TEAM_ALIASES = {
        # Scottish Premiership (base + alias comunes)
        "aberdeen",
        "aberdeen fc",
        "celtic",
        "celtic fc",
        "dundee",
        "dundee fc",
        "dundee united",
        "dundee utd",
        "dundee united fc",
        "heart of midlothian",
        "hearts",
        "hearts fc",
        "hibernian",
        "hibernian fc",
        "hibs",
        "kilmarnock",
        "kilmarnock fc",
        "killie",
        "motherwell",
        "motherwell fc",
        "rangers",
        "rangers fc",
        "ross county",
        "ross county fc",
        "ross co",
        "st johnstone",
        "st johnstone fc",
        "st mirren",
        "st mirren fc",
        # variantes frecuentes
        "st. johnstone",
        "st. mirren",
        "stjohnstone",
        "stmirren",
        # Scottish Championship / Cup (comunes en feeds)
        "livingston",
        "livi",
        "greenock morton",
        "morton",
        "partick thistle",
        "raith rovers",
        "queens park",
        "queen of the south",
        "ayr united",
        "inverness",
        "inverness ct",
        "inverness caledonian thistle",
        "dunfermline",
        "arbroath",
        "airdrieonians",
        "falkirk",
        "cove rangers",
        "hamilton",
        "hamilton academical",
        "accies",
        "stirling albion",
        "kelty hearts",
        "alloa athletic",
    }

    def _norm_team_name(x: str) -> str:
        x = safe_lower(x or "")
        x = x.replace("&", " and ")
        x = re.sub(r"[\(\)\[\]\{\}]", " ", x)
        x = x.replace(".", " ")
        x = x.replace("‚Äô", "").replace("\'", "")
        x = re.sub(r"\bfc\b", " ", x)
        x = re.sub(r"\s+", " ", x).strip()
        return x

    # Precompilamos regex para b√∫squeda r√°pida en texto grande
    _SCOT_PATTERNS = [re.compile(r"\b" + re.escape(a) + r"\b", re.I) for a in sorted(SCOTTISH_TEAM_ALIASES, key=len, reverse=True)]

    # Exclusiones para evitar falsos positivos en otros deportes / otras ligas.
    # Ejemplos: Boston Celtics (NBA), New York Rangers (NHL), Texas Rangers (MLB),
    # Queens Park Rangers (Inglaterra), etc.
    _SCOT_EXCLUDE_ALIASES = {
        "boston celtics",
        "new york rangers",
        "texas rangers",
        "queens park rangers",
        "qpr",
    }


    def _is_scottish_team_like(name: str) -> bool:
        """Identifica equipos de ligas escocesas con alta precisi√≥n.

        Nota: se elimin√≥ el fuzzy por token-overlap porque generaba falsos positivos
        (p.ej. Athletic Bilbao -> 'athletic', Manchester United -> 'united',
        Boston Celtics -> 'celtics', NY/Texas Rangers, Queens Park Rangers, etc.).
        """
        n = _norm_team_name(name)
        if not n:
            return False

        # Exclusiones expl√≠citas (otros deportes / clubes no escoceses)
        for bad in _SCOT_EXCLUDE_ALIASES:
            if re.search(r"\b" + re.escape(bad) + r"\b", n, re.I):
                return False

        # Match directo por alias completo (alta precisi√≥n)
        return any(pat.search(n) for pat in _SCOT_PATTERNS)

    def _is_scottish_match(a: str | None, b: str | None) -> bool:
        if not a or not b:
            return False
        return is_scottish_team_global(a) and is_scottish_team_global(b)

    def _is_scottish_block(blob_text: str) -> bool:
        t = _norm_team_name(blob_text)
        # Regla dr√°stica: 1 sola detecci√≥n es suficiente para clasificar como Liga Escocesa.
        return any(p.search(t) for p in _SCOT_PATTERNS)

    # ------------------------------------------------------------------
    # Heur√≠sticas BOX fuera de su encabezado (cuando la fuente mezcla combates
    # dentro de ligas de f√∫tbol; p.ej. "Liga MX" + Jornada + sede + combates).
    # ------------------------------------------------------------------

    _SOCCER_COMP_HINTS = {
        "liga", "league", "premier", "laliga", "bundesliga", "serie a", "serie b", "ligue",
        "copa", "cup", "fa cup", "concacaf", "championship",
    }
    _CLUB_WORD_HINTS = {
        "fc", "cf", "sc", "ac", "cd", "ud", "afc", "utd", "united", "city",
        "club", "sporting", "athletic", "atletico", "atl√©tico", "real", "deportivo",
        "racing", "uni√≥n", "union",
        "central", "talleres", "juniors", "boys", "phoenix", "jets", "glory",
        # sufijos t√≠picos de clubes (para no confundir con nombres de persona)
        "town", "county", "rovers", "rangers", "wanderers", "thistle", "albion",
    }
    _BOXING_WORDS = {
        "box", "boxeo", "boxing", "ko", "tko", "wbc", "wba", "wbo", "ibf",
        "peso", "welter", "lightweight", "heavyweight", "middleweight", "super",
        "asaltos", "rounds", "cintur", "title", "campeon", "campe√≥n",
    }

    def _is_soccer_comp_name(comp: str) -> bool:
        cl = safe_lower(comp or "")
        if "‚öΩ" in (comp or ""):
            return True
        return any(h in cl for h in _SOCCER_COMP_HINTS)

    def _is_person_name_like(name: str) -> bool:
        if not name:
            return False
        if "," in name:
            return False
        n = strip_weird_symbols_keep_plus(name)
        n = re.sub(r"[^A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±' -]", " ", n)
        n = re.sub(r"\s{2,}", " ", n).strip()
        toks = [t for t in n.split() if t]

        # Evita falsos positivos: muchos clubes √°rabes empiezan con 'Al ...' y se parecen a nombres.
        if re.match(r"(?i)^\s*al[-\s]+[a-z√°√©√≠√≥√∫√º√±]", n):
            return False
        if not (2 <= len(toks) <= 4):
            return False
        # Evita equipos por palabras t√≠picas de clubes
        low = safe_lower(n)
        if any(re.search(rf"\b{re.escape(w)}\b", low) for w in _CLUB_WORD_HINTS):
            return False
        # Debe parecer nombre propio: al menos 2 tokens con inicial may√∫scula
        cap = sum(1 for t in toks if t and t[0].isupper())
        if cap < 2:
            return False
        if any(re.search(r"\d", t) for t in toks):
            return False
        return True

    def _infer_box_country_from_venue(venue: str) -> str | None:
        v = safe_lower(venue or "")
        if not v:
            return None
        # UK / Reino Unido
        if any(k in v for k in ["essex", "london", "manchester", "birmingham", "liverpool", "leeds", "glasgow", "edinburgh", "uk", "united kingdom", "reino unido", "england", "scotland", "wales", "cardiff"]):
            return "Reino Unido"
        # M√©xico
        if any(k in v for k in ["m√©xico", "mexico", "cdmx", "guadalajara", "monterrey", "tijuana", "puebla", "queretaro", "leon", "le√≥n", "cancun", "canc√∫n", "baja california", "sonora", "jalisco", "nuevo leon", "nuevo le√≥n"]):
            return "M√©xico"
        # Estados Unidos
        if any(k in v for k in ["usa", "united states", "estados unidos", "las vegas", "new york", "texas", "california", "florida", "miami", "chicago", "houston", "phoenix", "atlanta"]):
            return "Estados Unidos"
        if re.search(r"\b(AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|IA|ID|IL|IN|KS|KY|LA|MA|MD|ME|MI|MN|MO|MS|MT|NC|ND|NE|NH|NJ|NM|NV|NY|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VA|VT|WA|WI|WV|DC)\b", venue):
            return "Estados Unidos"
        # Alemania
        if any(k in v for k in ["alemania", "germany", "berlin", "munich", "m√ºnchen", "hamburg", "frankfurt", "d√ºsseldorf", "duesseldorf", "cologne", "k√∂ln"]):
            return "Alemania"
        return None

    def _infer_box_comp_from_venue(venue: str | None) -> str:
        # Evita inferir pa√≠s a partir de l√≠neas que NO son sede (p.ej. canales '... USA')
        v = (venue or "").strip()
        v_low = safe_lower(v)
        if (not v) or (" vs " in f" {v_low} ") or (" por " in f" {v_low} ") or _looks_like_time_line(v) or RX_TIME_ANY.search(v):
            v = ""
        c = _infer_box_country_from_venue(v)
        return f"Evento BOX ({c})" if c else "Evento BOX"

    def _looks_like_box_rescue(a: str, b: str, raw: str, seg_comp: str, venue: str | None, jornada: str | None, ronda: str | None) -> bool:
        # Se√±ales: nombres propios vs nombres propios (BOX/MMA) y no nombres de clubes.
        if not (_is_person_name_like(a) and _is_person_name_like(b)):
            return False
        low_raw = safe_lower(raw or "")

        # Contexto: viene bajo competencia de f√∫tbol o trae Jornada/Ronda heredada
        soccer_ctx = _is_soccer_comp_name(seg_comp) or bool(jornada) or bool(ronda)

        # Evita falsos positivos: tenis tambi√©n es "Persona vs Persona"
        tennis_ctx = ("üéæ" in (raw or "")) or any(w in low_raw for w in [
            "tenis", "tennis", "atp", "wta", "grand slam", "wimbledon", "roland garros",
            "us open", "australian open", "masters", "challenger", "itf"
        ])

        # No te metas si el texto ya parece claramente f√∫tbol (‚öΩ).
        if "‚öΩ" in (raw or ""):
            return False

        # Regla principal:
        # - Si estamos dentro de contexto "f√∫tbol + Jornada/Ronda" y aparece Persona vs Persona,
        #   lo rescatamos como BOX incluso si NO hay sede ni palabra "box" (caso cr√≠tico).
        # Regla:
        # 1) Caso cr√≠tico hist√≥rico: Persona vs Persona dentro de contexto f√∫tbol/Jornada/Ronda.
        # 2) Caso nuevo: Persona vs Persona pegado bajo OTRA competencia (ej. Olimpiadas) pero con se√±ales fuertes de BOX.
        if tennis_ctx:
            return False

        # Se√±ales fuertes de BOX aunque no venga bajo f√∫tbol
        box_hints = False
        if (raw or ""):
            if "ü•ä" in raw:
                box_hints = True
        if not box_hints:
            # palabras clave (evita falsos positivos con l√≠mites de palabra cuando aplica)
            if any(w in low_raw for w in [" box", "box ", "boxeo", "dazn", "ppv", "pay per view"]):
                box_hints = True
            if re.search(r"(?i)\b(ko|tko)\b", low_raw):
                box_hints = True
            if re.search(r"(?i)\b(wbc|wba|ibf|wbo)\b", low_raw):
                box_hints = True
            if any(w in low_raw for w in ["cintur", "campeon", "campe√≥n", "asalt", "round"]):
                box_hints = True

        # Nunca rescates si el texto parece claramente f√∫tbol (‚öΩ).
        if "‚öΩ" in (raw or ""):
            return False

        # Blindaje: NO rescatar BOX si no hay se√±ales fuertes (BOX/PPV/DAZN/KO/ü•ä, etc.)
        # y tampoco hay una sede previa. Esto evita falsos positivos como clubes √°rabes (Al Hilal vs Al Wahda).
        if (not box_hints) and (not venue):
            return False

        return True

    # Detecta header "Eventos Deportivos ..." (lo ignoramos)
    header = None
    for b in deportes_blocks:
        if "eventos deportivos" in safe_lower(b):
            header = b
            break

    items: list[dict] = []
    seen_ids: set[str] = set()

    current_comp: str | None = None
    current_jornada: str | None = None
    current_ronda: str | None = None

    # "pendings" con alcance (evita fugas entre competencias)
    pending_location: str | None = None
    pending_location_comp: str | None = None
    pending_global: str | None = None
    pending_global_comp: str | None = None

    pending_event: dict | None = None
    now_utc = datetime.now(timezone.utc)

    def _finalize_pending_event():
        nonlocal pending_event
        if not pending_event:
            return
        title = pending_event.get("t") or ""
        meta = pending_event.get("meta") or {}
        comp = meta.get("competition") or None
        # Heur√≠stica (blindaje): si el evento ES un partido escoc√©s, forzamos "Liga Escocesa"
        # independientemente del titular que venga encima (a veces la fuente lo mezcla o lo pierde).
        # Esto NO usa fuzzy: requiere match directo de ambos equipos en el diccionario escoc√©s.
        ta, tb = _extract_match_teams(title)
        if ta and tb and is_scottish_match_global(ta, tb):
            comp = "Liga Escocesa"
            meta["competition"] = comp
        jornada = meta.get("jornada") or None
        ronda = meta.get("ronda") or None

        if not title or not comp:
            pending_event = None
            return

        # id estable + anti-dup
        item_id = sha_id("SPORT", title, comp, str(jornada or ""), str(ronda or ""), str(meta.get("sede") or ""), str(meta.get("start_utc") or ""))
        if item_id in seen_ids:
            pending_event = None
            return
        seen_ids.add(item_id)

        kind = meta.get("sport_kind") or sport_kind_from_text(safe_lower(title))
        cat = "‚öΩ DEPORTES"
        if kind == "combat":
            cat = "ü•ä COMBATE"
        if kind == "tennis":
            cat = "üéæ TENIS"
        if kind == "golf":
            cat = "‚õ≥ GOLF"
        if "nba" in safe_lower(comp):
            cat = "üèÄ NBA"
        if "ncaa" in safe_lower(comp):
            cat = "üèÄ NCAA"
        if "wwe" in safe_lower(comp):
            cat = "ü§º WWE"

        u = f"{WEBAPP_URL}#item={item_id}"
        img = LOGO_URL

        items.append({"_id": item_id, "t": title, "u": u, "cat": cat, "img": img, "meta": meta})
        pending_event = None

    def _extract_time_text(raw: str) -> str | None:
        """Extrae el texto completo de horarios (Este/Centro/Pac√≠fico) desde la primera hora
        hasta antes de 'por' (canales)."""
        if not raw:
            return None
        s = raw.replace("\xa0", " ").replace("\u00a0", " ")
        m = RX_TIME_AMPM.search(s)
        if not m:
            return None
        tail = s[m.start():]
        # corta antes de canales
        tail = re.split(r"(?i)\bpor\b", tail)[0]
        tail = tail.strip(" ,.;")
        return re.sub(r"\s+", " ", tail).strip() or None

    def _extract_categoria(raw: str) -> str | None:
        """Extrae el texto de categor√≠a (despu√©s de 'Categoria/Categor√≠a')."""
        if not raw:
            return None
        m = re.search(r"(?i)\bcat(?:egor[i√≠]a)?\b\s*(.+)$", raw)
        if not m:
            return None
        cat = m.group(1)
        # limpia prefijos y puntuaci√≥n
        cat = re.sub(r"^[\s:;\-]+", "", cat).strip()
        cat = cat.strip(" .,:;")
        cat = re.sub(r"\s+", " ", cat).strip()
        if not cat:
            return None
        # elimina emojis/s√≠mbolos sueltos al final
        cat = re.sub(r"[\u2600-\u27BF\U0001F300-\U0001FAFF]+$", "", cat).strip()
        if not cat:
            return None
        return cat[:80]

    def _extract_channels_por(raw: str) -> list[str]:
        """Extrae canales despu√©s de 'por'.

        Caso est√°ndar:
          '... por ESPN y FOX'

        Caso importante (BOX/PPV):
          '... por Categor√≠a PPV (DAZN)'
        Aqu√≠ el canal viene dentro de par√©ntesis y NO debe perderse.
        """
        if not raw:
            return []
        m = re.search(r"(?i)\bpor\b(.+)$", raw)
        if not m:
            return []

        tail_raw = (m.group(1) or "").strip()

        # Si comienza con 'Categor√≠a', intenta rescatar el/los canales dentro de par√©ntesis.
        # (La categor√≠a se extrae aparte con _extract_categoria)
        tail_no_cat = re.split(r"(?i)\bcat(?:egor[i√≠]a)?\b", tail_raw)[0]
        tail_no_cat = tail_no_cat.strip(" .,:;")
        tail_no_cat = re.sub(r"\s+", " ", tail_no_cat).strip()

        candidates = tail_no_cat
        if not candidates:
            # Busca canales dentro de '(...)'
            par = " ".join(re.findall(r"\(([^)]+)\)", tail_raw))
            candidates = par.strip() if par else tail_raw

        candidates = candidates.strip(" .,:;")
        candidates = re.sub(r"\s+", " ", candidates).strip()
        if not candidates:
            return []

        parts = re.split(r",|\s+y\s+", candidates)
        out: list[str] = []
        for p in parts:
            p = sanitize_channel_name(p)
            if not p:
                continue
            lowp = safe_lower(p)
            if lowp in {"ppv", "pay per view", "payperview", "categoria"}:
                continue
            out.append(p)
        return list(dict.fromkeys(out))

    def _extract_first_time_and_tz(raw: str) -> tuple[str | None, str | None]:
        """Obtiene la primera hora (con am/pm) y la zona (ESTE/CENTRO/PACIFICO)."""
        if not raw:
            return None, None
        tm = RX_TIME_AMPM.search(raw)
        if not tm:
            return None, None
        hora = re.sub(r"\s+", " ", tm.group(0)).strip()
        # Busca zona inmediatamente despu√©s
        rest = raw[tm.end(): tm.end() + 25]
        z = RX_TZ.search(rest) or RX_TZ.search(raw)
        tz = z.group(1).upper() if z else None
        return hora, tz

    def _attach_time_and_channels(meta: dict, raw: str):
        low = safe_lower(raw)

        # hora
        if not meta.get("hora_text"):
            ht = _extract_time_text(raw)
            if ht:
                meta["hora_text"] = ht

        if (not meta.get("hora_original")) and _looks_like_time_line(raw):
            hora_original, timezone_txt = _extract_first_time_and_tz(raw)
            if hora_original:
                meta["hora_original"] = hora_original
                meta["timezone"] = timezone_txt
                start_utc = compute_start_utc(hora_original, timezone_txt, now_utc=now_utc)
                meta["start_utc"] = start_utc.isoformat() if start_utc else None
                dur = int(meta.get("duration_min") or LIVE_DURATION_NORMAL_MIN)
                meta["is_live_now"] = bool(is_live(now_utc, start_utc, dur)) if start_utc else False

        # canales (preferimos lo que viene despu√©s de 'por ...')
        canales_por = _extract_channels_por(raw)
        if canales_por:
            canales_por = _reorder_channels_for_display(canales_por)
            meta["canales"] = list(dict.fromkeys(canales_por))
            if not meta.get("canal") and meta["canales"]:
                meta["canal"] = meta["canales"][0]
        else:
            canales = detect_channels(raw, channel_aliases)
            canal_one = detect_channel(raw, channel_aliases)
            canal_one = sanitize_channel_name(canal_one) if canal_one else None
            if canales:
                canales = [sanitize_channel_name(c) for c in canales if c]
            if canal_one and canal_one not in (canales or []):
                canales = [canal_one] + (canales or [])
            canales = _reorder_channels_for_display([c for c in (canales or []) if c])
            if canales:
                meta["canales"] = list(dict.fromkeys(canales))  # unique preserving order
                if not meta.get("canal"):
                    meta["canal"] = meta["canales"][0]


        # categor√≠a (mantener 'Categor√≠a ...' para el output solicitado)
        cat = _extract_categoria(raw)
        if cat and not meta.get("categoria"):
            meta["categoria"] = cat

        # GLOBAL
        g = _extract_global(raw)
        if g and not meta.get("global"):
            meta["global"] = g

        # sede (solo si es una l√≠nea de sede real y no trae match)
        if _is_probable_venue_line(raw) and not meta.get("sede") and ("vs" not in low and " @ " not in low):
            meta["sede"] = raw

    # ------------------------------------------------------------------
    # Parser por segmentos (competici√≥n -> l√≠neas). Esto evita que se
    # mezclen sedes/jornadas entre competiciones cuando el HTML se aplana.
    # ------------------------------------------------------------------

    def _emit_item(title: str, competition: str, meta: dict, raw_for_id: str) -> None:

        """Emite un item deportivo con el contrato oficial de /api/links.


        Debe incluir: _id, t, u, cat, img, meta

        """

        title = (title or '').strip()

        competition = (competition or '').strip()

        if not title or not competition:

            return


        meta = dict(meta or {})

        meta.setdefault('type', 'sport')

        meta['competition'] = competition

        meta['raw'] = raw_for_id


        # scoring IA (para ordenar en UI)

        ai_score, ai_confidence = ai_score_sports_item(meta)

        meta['ai_score'] = ai_score

        meta['ai_confidence'] = ai_confidence


        # kind/cat (para subfiltros en UI)

        kind = meta.get('sport_kind') or sport_kind_from_text(safe_lower(f"{competition} {title} {raw_for_id}"))

        meta['sport_kind'] = kind


        cat = '‚öΩ DEPORTES'

        if kind == 'combat':

            cat = 'ü•ä COMBATE'

        elif kind == 'tennis':

            cat = 'üéæ TENIS'

        elif kind == 'golf':

            cat = '‚õ≥ GOLF'

        if 'nba' in safe_lower(competition):

            cat = 'üèÄ NBA'

        if 'ncaa' in safe_lower(competition):

            cat = 'üèÄ NCAA'

        if 'wwe' in safe_lower(competition):

            cat = 'ü§º WWE'


        # ID estable (12 hex)

        item_id = sha_id(

            'SPORT',

            competition,

            title,

            str(meta.get('start_utc') or meta.get('hora_original') or ''),

            str(meta.get('sede') or ''),

            str(meta.get('jornada') or ''),

            str(meta.get('ronda') or ''),

            str(meta.get('canal') or ''),

        )

        if item_id in seen_ids:

            return

        seen_ids.add(item_id)


        u = f"{WEBAPP_URL}#item={item_id}"

        items.append({

            '_id': item_id,

            't': title,

            'u': u,

            'cat': cat,

            'img': LOGO_URL,

            'meta': meta,

        })

    def _clean_line(raw0: str) -> str | None:
        raw = (raw0 or "").replace("¬†", " ").replace("¬†", " ").strip()
        raw = re.sub(r"[*‚òÖ‚òÜ‚úî‚úÖ‚ùå]+", " ", raw)
        raw = re.sub(r"\s+", " ", raw).strip()
        if not raw:
            return None

        low = safe_lower(raw)
        if header and raw == header:
            return None
        if any(p in low for p in STOP_PHRASES):
            return None
        if is_noise(raw):
            return None

        return raw

    def _has_channel(raw: str) -> bool:
        return bool(detect_channel(raw, channel_aliases) or detect_channels(raw, channel_aliases))



    def _explode_inline_headers(raw: str) -> list[str]:
        """Divide l√≠neas cuando Wix "pega" encabezados de competencia dentro de la misma l√≠nea.

        Soporta:
          - Encabezado pegado con espacios: '... por ESPN Championshipüè¥‚öΩÔ∏è Coventry vs ...'
          - Encabezado pegado SIN espacio: '... por Espn3MxChampionshipüè¥‚öΩÔ∏è ...' (raro, pero pasa por HTML aplanado)

        Devuelve una lista de piezas en orden, SIN perder contenido (horarios/canales completos).
        """
        if not raw:
            return []

        # Solo intentamos si la l√≠nea ya trae se√±ales de "contenido" (hora/canales),
        # porque ah√≠ es donde normalmente se pegan encabezados por el HTML aplanado.
        if not (_looks_like_time_line(raw) or _has_channel(raw)):
            return [raw]

        comp0 = _normalize_competition_extended(raw)

        def _norm_start(s: str) -> str:
            x = safe_lower(strip_weird_symbols_keep_plus(s))
            x = re.sub(r"^[^a-z0-9√°√©√≠√≥√∫√º√±]+", "", x).strip()
            return x

        def _starts_with_comp(suffix: str, comp: str) -> bool:
            return bool(_norm_start(suffix).startswith(_norm_start(comp or "")))

        def _prefix_has_signal(prefix: str) -> bool:
            a_p, b_p = _extract_match_teams(prefix)
            return bool(_looks_like_time_line(prefix) or _has_channel(prefix) or _extract_global(prefix) or (a_p and b_p))

        boundaries: list[int] = []

        # 1) Candidatos por separador de espacio (normal)
        for j in range(len(raw) - 1):
            if raw[j] != " ":
                continue
            k = j + 1
            while k < len(raw) and raw[k] == " ":
                k += 1
            if k >= len(raw):
                break
            # Ahorra trabajo: encabezados suelen empezar con may√∫scula/d√≠gito
            if not (raw[k].isupper() or raw[k].isdigit()):
                continue
            prefix = raw[:j].strip()
            suffix = raw[k:].strip()
            if len(prefix) < 8 or len(suffix) < 6:
                continue
            if not _prefix_has_signal(prefix):
                continue
            comp2 = _normalize_competition_extended(suffix)
            if not comp2:
                continue
            if comp0 and (safe_lower(comp2) == safe_lower(comp0)):
                continue
            if _is_probable_venue_line(suffix):
                continue
            if not _starts_with_comp(suffix, comp2):
                continue
            boundaries.append(k)

        # 2) Fallback: encabezado pegado SIN espacio (camel-case / concatenaci√≥n)
        # Ej: '...MxChampionshipüè¥‚öΩÔ∏è...'
        if not boundaries:
            for j in range(1, len(raw) - 1):
                if raw[j].isupper() and raw[j-1].isalnum() and raw[j-1] != " ":
                    prefix = raw[:j].strip()
                    suffix = raw[j:].strip()
                    if len(prefix) < 8 or len(suffix) < 6:
                        continue
                    if not _prefix_has_signal(prefix):
                        continue
                    comp2 = _normalize_competition_extended(suffix)
                    if not comp2:
                        continue
                    if comp0 and (safe_lower(comp2) == safe_lower(comp0)):
                        continue
                    if _is_probable_venue_line(suffix):
                        continue
                    if not _starts_with_comp(suffix, comp2):
                        continue
                    boundaries.append(j)

        if not boundaries:
            return [raw]

        boundaries = sorted(set(boundaries))
        pieces: list[str] = []
        start_i = 0
        for b in boundaries:
            piece = raw[start_i:b].strip()
            if piece:
                pieces.append(piece)
            start_i = b
        last = raw[start_i:].strip()
        if last:
            pieces.append(last)

        return pieces

        toks = raw.split()
        if len(toks) < 6:
            return [raw]

        # Buscamos un sufijo corto que sea SOLO encabezado (sin vs/hora/canales/global/sede).
        for i in range(2, len(toks)):
            suffix = " ".join(toks[i:]).strip()
            if len(suffix) < 6 or len(suffix) > 80:
                continue

            a2, b2 = _extract_match_teams(suffix)
            if a2 and b2:
                continue
            if _looks_like_time_line(suffix):
                continue
            if _has_channel(suffix):
                continue
            if _extract_global(suffix):
                continue
            if _is_probable_venue_line(suffix):
                continue

            comp2 = _normalize_competition_extended(suffix)
            if not comp2:
                continue

            # Prefijo debe quedarse con algo √∫til.
            prefix = " ".join(toks[:i]).strip()
            if len(prefix) < 8:
                continue

            return [prefix, suffix]

        return [raw]
    def _fix_team_name(name: str, competition: str) -> str:
        n = (name or "").strip()
        if not n:
            return n
        # Ajustes puntuales para nombres completos (seg√∫n tus observaciones)
        if safe_lower(competition) == "copa del rey" and safe_lower(n) == "athletic":
            return "Athletic de Bilbao"
        return n

    def _fix_venue_name(venue: str) -> str:
        v = re.sub(r"\s+", " ", (venue or "").replace("¬†", " ")).strip()
        # Quita emojis/puntuaci√≥n al inicio (p.ej. 'üá≤üáΩü•ä Plaza Juarez')
        v = re.sub(r"^[^A-Za-z0-9√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]+", "", v).strip()
        if safe_lower(v).startswith("estadio renato dall"):
            return "Estadio Renato Dall'Ara"
        return v

    # 1) Limpieza + conservaci√≥n del orden
    lines: list[str] = []
    for raw0 in deportes_blocks:
        raw = _clean_line(raw0)
        if not raw:
            continue
        for piece in _explode_inline_headers(raw):
            piece = (piece or "").strip()
            if piece:
                lines.append(piece)

    # 2) Segmentaci√≥n por encabezados de competici√≥n
    segments: list[tuple[str, list[str]]] = []
    current_comp_seg: str | None = None
    current_lines_seg: list[str] = []
    orphan_venue: str | None = None  # sede/ubicaci√≥n antes de que exista un encabezado (rescate BOX)

    for raw in lines:
        comp = _normalize_competition_extended(raw)

        # Fallback fuerte para BOX: algunos feeds traen solo 'BOX' / 'BOXü•ä' sin el prefijo 'Evento'
        if not comp:
            raw_clean = safe_lower(strip_weird_symbols_keep_plus(raw))
            if raw_clean in {"box", "boxeo"} or (("box" in raw_clean) and ("ü•ä" in raw)):
                comp = "Evento BOX"

        a, b = _extract_match_teams(raw)
        has_match = bool(a and b)
        has_time = _looks_like_time_line(raw)
        has_ch = _has_channel(raw)
        has_global = bool(_extract_global(raw))
        is_venue = _is_probable_venue_line(raw)

        # ------------------------------------------------------------
        # Rescate cuando el HTML pierde el encabezado:
        # - Captura sedes "hu√©rfanas" (sin competencia activa) para BOX.
        # - Si aparece un combate Persona vs Persona sin header, crea un segmento BOX gen√©rico.
        # - Si aparece un partido escoc√©s sin header, crea segmento Liga Escocesa.
        # ------------------------------------------------------------
        if current_comp_seg is None and is_venue and (not comp) and (not has_match) and (not has_time) and (not has_ch) and (not has_global):
            orphan_venue = raw
            continue

        if current_comp_seg is None and has_match and (not comp):
            # BOX sin titular (persona vs persona) ‚Äî evita tenis
            if a and b and _is_person_name_like(a) and _is_person_name_like(b):
                low_raw = safe_lower(raw or "")
                tennis_ctx = ("üéæ" in (raw or "")) or any(w in low_raw for w in [
                    "tenis","tennis","atp","wta","grand slam","wimbledon","roland garros",
                    "us open","australian open","masters","challenger","itf"
                ])
                box_hints = ("ü•ä" in (raw or "")) or any(w in low_raw for w in ["box","boxeo","ppv","dazn","ko","tko","wbc","wba","ibf","wbo"])
                venue_hint = bool(orphan_venue) and bool(re.search(r"(?i)\b(club|arena|apex|coliseo|plaza|casino|hall|center|centre|stadium|estadio)\b", orphan_venue))
                if (not tennis_ctx) and ("‚öΩ" not in (raw or "")) and (box_hints or venue_hint):
                    comp_guess = _infer_box_comp_from_venue(orphan_venue)
                    current_comp_seg = comp_guess
                    current_lines_seg = []
                    if orphan_venue:
                        current_lines_seg.append(orphan_venue)
                    orphan_venue = None
                    current_lines_seg.append(raw)
                    continue

            # Escocia sin titular (raro pero posible)
            if a and b and is_scottish_match_global(a, b):
                current_comp_seg = "Liga Escocesa"
                current_lines_seg = []
                if orphan_venue:
                    current_lines_seg.append(orphan_venue)
                orphan_venue = None
                current_lines_seg.append(raw)
                continue

        stage_in_line = _extract_stage_text(raw)
        is_box = bool(comp) and safe_lower(comp).startswith('evento box')
        # Header: por default evitamos sedes, pero permitimos BOX aunque traiga 'Plaza/Arena'
        allow_venue_header = is_box
        raw_norm = safe_lower(strip_weird_symbols_keep_plus(raw))
        comp_norm = safe_lower(strip_weird_symbols_keep_plus(comp or ""))
        starts_with_comp = bool(comp) and bool(comp_norm) and raw_norm.startswith(comp_norm)

        is_header = bool(comp) and (not has_match) and (not has_time) and (not has_ch) and (not has_global) and ((not is_venue) or allow_venue_header)

        # Header relajado: si la l√≠nea EMPIEZA con el nombre de la competencia,
        # la tratamos como encabezado aunque traiga hora/canales (caso Olimpiadas / NFL / etc. con info inline).
        if (not is_header) and starts_with_comp and (not has_match) and (not has_global) and ((not is_venue) or allow_venue_header):
            # Si cambia la competencia, cortamos el segmento anterior aqu√≠.
            is_header = True

        if is_header:
            if current_comp_seg is not None:
                segments.append((current_comp_seg, current_lines_seg))
            current_comp_seg = comp
            orphan_venue = None
            # Si el encabezado trae instancia (p.ej. 'FA Cup ... Cuarta Ronda') o viene con sede en BOX, no lo pierdas.
            # En BOX dejamos pasar el encabezado para poder capturar sedes inline como 'Brentwood, Essex' o 'Meta Apex, Las Vegas'.
            keep_header_line = bool(stage_in_line or is_box or has_time or has_ch or has_match or has_global)
            if not keep_header_line:
                # Si el encabezado trae contenido inline (evento/horario/canales), no lo tires (Softbol/Olimpiadas/etc.).
                rest_inline = _strip_comp_prefix(raw, comp) if comp else raw
                if rest_inline and re.search(r"[A-Za-z0-9√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]", rest_inline):
                    keep_header_line = True
            current_lines_seg = [raw] if keep_header_line else []
            continue

        # Cambio fuerte de competencia inline (p.ej. 'FA Cupüè¥‚öΩÔ∏è Burton vs West Ham' en una sola l√≠nea)
        if comp and current_comp_seg is not None and comp != current_comp_seg and (has_match or stage_in_line):
            segments.append((current_comp_seg, current_lines_seg))
            current_comp_seg = comp
            current_lines_seg = [raw]
            continue

        # Si llega una l√≠nea con info y trae competici√≥n inline, √∫sala para abrir segmento.
        if comp and current_comp_seg is None:
            current_comp_seg = comp
            orphan_venue = None
            current_lines_seg = [raw]
            continue

        if current_comp_seg is not None:
            current_lines_seg.append(raw)

    if current_comp_seg is not None:
        segments.append((current_comp_seg, current_lines_seg))

    # 3) Parse por segmento
    # Motor de clasificaci√≥n (puntajes): corrige segmentos mal etiquetados por la fuente.
    # Nota: aqu√≠ NO usamos APIs externas; es un clasificador local por se√±ales (equipo/instancia/emoji).
    def _reclassify_segment_comp(seg_comp: str, seg_lines: list[str]) -> str:
        base = safe_lower(seg_comp or "")
        blob = safe_lower((seg_comp or "") + " " + " ".join(seg_lines or []))
        blob_raw = " ".join(seg_lines or [])

        def _has_any(text: str, hints: set[str]) -> bool:
            return any(h in text for h in hints)

        # Equipos/se√±ales t√≠picas de FA Cup (para evitar que se "olvide" cuando el encabezado se pierde)
        fa_hints = {
            "west ham", "manchester city", "salford", "aston villa", "newcastle",
            "liverpool", "brighton", "burton",
            "arsenal", "chelsea", "manchester united", "tottenham", "everton",
            "fulham", "brentford", "wolves", "bournemouth", "crystal palace",
            "nottingham forest", "leicester", "southampton", "ipswich",
        }

        # 1) Liga Escocesa: a veces viene como "Premier League" pero los equipos son de Escocia.
        #    Clasificador por "score" (con tu lista): si detectamos equipos escoceses, re-etiquetamos el bloque.
        #    Regla pr√°ctica: si hay 2+ hits de equipos escoceses en partidos del segmento (normalmente Escocia vs Escocia),
        #    se reclasifica como Liga Escocesa aunque el header diga Premier League.
        if ("premier league" in base) and _is_scottish_block(blob):
            match_pairs = []
            for _ln in seg_lines:
                _a, _b = _extract_match_teams(_ln)
                if _a and _b:
                    match_pairs.append((_a, _b))
            if match_pairs:
                scot_team_hits = 0
                for _a, _b in match_pairs:
                    if is_scottish_team_global(_a):
                        scot_team_hits += 1
                    if is_scottish_team_global(_b):
                        scot_team_hits += 1
                if scot_team_hits >= 2:
                    return "Liga Escocesa"
            else:
                # No hay partidos detectables (pero s√≠ se√±ales fuertes de Escocia).
                return "Liga Escocesa"

        # 2) FA Cup: SOLO si aparece en la fuente del d√≠a (Kaelus).
        #    Si el encabezado se pierde por el aplanado del HTML, podemos rescatar usando se√±ales,
        #    pero NUNCA debemos inventarlo.
        if SOURCE_HAS_FA_CUP:
            # palabra clave directa
            if SOURCE_HAS_FA_CUP and (re.search(r"(?i)\bfa\s*cup\b", blob) or re.search(r"(?i)copa\s+de\s+inglaterra", blob)):
                return "FA Cup"
            # se√±ales (equipos) solo si sabemos que FA Cup existe hoy
            if _has_any(blob, {"burton", "salford"}):
                return "FA Cup"
            if ("ronda" in blob) and _has_any(blob, fa_hints):
                return "FA Cup"

        # 3) BOX: solo si aparece BOX/ü•ä en la fuente del d√≠a (Kaelus).
        #    Si aparece 'box' y el emoji ü•ä en el bloque, nunca debe caer en otra competencia (p.ej. Liga MX/Ju√°rez).
        if SOURCE_HAS_BOX and ("box" in blob) and ("ü•ä" in blob_raw or "evento" in blob):
            if base.startswith('evento box'):
                return seg_comp
            return "Evento BOX"

        return seg_comp

    for seg_comp, seg_lines in segments:
        seg_comp = _reclassify_segment_comp(seg_comp, seg_lines)
        active_comp = seg_comp
        seg_is_box = safe_lower(active_comp).startswith("evento box")
        current_jornada_seg: str | None = None
        current_ronda_seg: str | None = None
        pending_venue_seg: str | None = None
        pending_global_seg: str | None = None

        open_event: dict | None = None  # {'t':..., 'comp':..., 'meta':..., 'raw':...}

        def _flush_open_event():
            nonlocal open_event
            if not open_event:
                return
            _emit_item(open_event.get('t', ''), open_event.get('comp', ''), open_event.get('meta', {}), open_event.get('raw', ''))
            open_event = None

        for raw in seg_lines:
            stage = _extract_stage_text(raw)
            a, b = _extract_match_teams(raw)
            # BOX: usa extractor especial (no elimina sedes como 'Plaza/Arena') y separa sede inline
            if seg_is_box:
                a_box, b_box = extract_box_match_teams_raw(raw)
                if a_box and b_box:
                    a, b = a_box, b_box
            if seg_is_box and a and b:
                a2, v2 = extract_box_inline_venue_and_fighter(a, raw, active_comp)
                if v2:
                    pending_venue_seg = v2
                    a = a2

            # Rescate radical: combates Persona vs Persona mezclados dentro de ligas de f√∫tbol
            # (ej. "Liga MX" ‚â† BOX aunque venga "Brentwood, Essex").
            box_rescue = False
            if SOURCE_HAS_BOX and (not seg_is_box) and a and b:
                if _looks_like_box_rescue(a, b, raw, active_comp, pending_venue_seg, current_jornada_seg, current_ronda_seg):
                    box_rescue = True
                    a_box, b_box = extract_box_match_teams_raw(raw)
                    if a_box and b_box:
                        a, b = a_box, b_box
                    # Si viene sede pegada al primer peleador, sep√°rala
                    if a and b:
                        a2, v2 = extract_box_inline_venue_and_fighter(a, raw, active_comp)
                        if v2:
                            pending_venue_seg = v2
                            a = a2

            has_match = bool(a and b)
            has_time = _looks_like_time_line(raw)
            has_ch = _has_channel(raw)
            global_txt = _extract_global(raw)
            has_global = bool(global_txt)

            comp_inline_v = _normalize_competition_extended(raw)

            # Rescate por equipos cuando el encabezado de la liga NO viene (o se perdi√≥).
            # Solo aplica si la l√≠nea no trae competencia detectable, pero s√≠ un partido.
            if has_match and (not seg_is_box) and (not comp_inline_v):
                comp_guess = _infer_comp_from_match_teams(a, b)
                # Blindaje: NO cambiar entre A-League y A-League Women solo por inferencia de equipos.
                # Muchos clubes existen en ambas y el header ya define el contexto.
                if (active_comp in {"A-League", "A-League Women"}) and (comp_guess in {"A-League", "A-League Women"}):
                    comp_guess = None
                if comp_guess and (comp_guess != active_comp):
                    _flush_open_event()
                    active_comp = comp_guess
                    seg_is_box = safe_lower(active_comp).startswith("evento box")
                    pending_venue_seg = None
                    pending_global_seg = None
                    current_jornada_seg = None
                    current_ronda_seg = None

            # Hard split: Wix a veces "pega" encabezados con contenido (match/hora/canales)
            # o mete el encabezado de otra competencia dentro del mismo segmento.
            # Si la l√≠nea *empieza* con otra competencia, cambiamos contexto AUNQUE tenga match/hora/canales.
            if comp_inline_v and (comp_inline_v != active_comp) and (not _is_probable_venue_line(raw)):
                raw_norm_inline = safe_lower(strip_weird_symbols_keep_plus(raw))
                raw_norm_inline = re.sub(r"^[^a-z0-9√°√©√≠√≥√∫√º√±]+", "", raw_norm_inline).strip()
                comp_norm_inline = safe_lower(strip_weird_symbols_keep_plus(comp_inline_v or ""))
                comp_norm_inline = re.sub(r"^[^a-z0-9√°√©√≠√≥√∫√º√±]+", "", comp_norm_inline).strip()
                starts_with_comp_inline = bool(comp_norm_inline) and raw_norm_inline.startswith(comp_norm_inline)

                # 1) Encabezado + contenido inline (ej. 'Championshipüè¥‚öΩÔ∏è Coventry vs ...')
                # 2) Encabezado puro embebido (sin match/hora/canales)
                if starts_with_comp_inline or ((not has_match) and (not has_time) and (not has_ch) and (not has_global)):
                    _flush_open_event()
                    active_comp = comp_inline_v
                    seg_is_box = safe_lower(active_comp).startswith("evento box")
                    pending_venue_seg = None
                    pending_global_seg = None
                    current_jornada_seg = None
                    current_ronda_seg = None

                    # Si era solo encabezado (sin instancia), brinca la l√≠nea.
                    if (not has_match) and (not has_time) and (not has_ch) and (not has_global) and (not stage):
                        continue
            # BOX: si la l√≠nea es solo el encabezado (sin sede/instancia), la ignoramos para no crear eventos vac√≠os.
            if comp_inline_v and safe_lower(comp_inline_v).startswith("evento box"):
                v_only_hdr = _strip_comp_prefix(raw, comp_inline_v)
                if (not v_only_hdr) and (not has_match) and (not has_time) and (not has_ch) and (not has_global) and (not stage):
                    continue


            # UFC Fight Night: se subdivide por "Peleas Preliminares", "Peleas Principales" y "Pelea Estelar".
            # No inventar rivales para Preliminares/Principales (son segmentos de la cartelera).
            if active_comp == "UFC Fight Night":
                # Captura sede (una sola vez por cartelera)
                if _is_probable_venue_line(raw) and (not has_match) and (not has_time) and (not has_ch) and (not has_global):
                    pending_venue_seg = raw
                    continue

                m_pre = re.match(r"(?i)^\s*peleas\s+preliminares\b", raw)
                m_main = re.match(r"(?i)^\s*peleas\s+principales\b", raw)
                m_star = re.match(r"(?i)^\s*pelea\s+estelar\b", raw)

                if m_pre or m_main:
                    _flush_open_event()
                    title_fixed = "Peleas Preliminares" if m_pre else "Peleas Principales"
                    meta = {"ufc_group": title_fixed}
                    if pending_venue_seg:
                        meta["sede"] = pending_venue_seg
                    _attach_time_and_channels(meta, raw)
                    cat = _extract_categoria(raw)
                    if cat:
                        meta["categoria"] = cat
                    open_event = {"t": title_fixed, "comp": active_comp, "meta": meta, "raw": raw}
                    continue

                if m_star:
                    # Pelea Estelar: s√≠ tiene rivales (vs)
                    raw2 = re.sub(r"(?i)^\s*pelea\s+estelar\s*", "", raw).strip()
                    a2, b2 = _extract_match_teams(raw2)
                    _flush_open_event()
                    meta = {"ufc_group": "Pelea Estelar"}
                    if pending_venue_seg:
                        meta["sede"] = pending_venue_seg
                    if a2 and b2:
                        open_event = {"t": f"{a2} vs {b2}", "comp": active_comp, "meta": meta, "raw": raw2}
                    else:
                        open_event = {"t": raw2 or "Pelea Estelar", "comp": active_comp, "meta": meta, "raw": raw2}
                    continue

            # BOX: encabezado puede venir con sede pegada en la misma l√≠nea aunque no tenga palabras tipo 'Arena/Plaza'.
            box_inline_venue = False
            if comp_inline_v and safe_lower(comp_inline_v).startswith("evento box"):
                v_only = _strip_comp_prefix(raw, comp_inline_v)
                if v_only and (not has_match) and (not has_time) and (not has_ch) and (not has_global) and (not stage):
                    box_inline_venue = True

            looks_like_box_location = False
            if seg_is_box and (len(raw) <= 70) and (not re.search(r"(?i)\b(jornada|ronda|fase|semifinal|cuartos|octavos|final)\b", raw)):
                # casos t√≠picos: "Brentwood, Essex" / "Meta Apex, Las Vegas" / "CDMX/M√©xico"
                if ("," in raw) or ("/" in raw):
                    looks_like_box_location = True
                else:
                    # UK / sedes sin coma (ej. "Brentwood Essex")
                    toks_loc = [t for t in (raw or "").split() if t]
                    if 2 <= len(toks_loc) <= 4 and (not re.search(r"\d", raw)):
                        # TitleCase o ALLCAPS cortos
                        if all((t[0].isupper() or (len(t) <= 3 and t.isupper())) for t in toks_loc if t and t[0].isalpha()):
                            low_loc = safe_lower(raw)
                            comp_low_loc = safe_lower(active_comp or "")
                            uk_hints = [
                                "essex","kent","surrey","sussex","london","manchester","birmingham","liverpool","leeds",
                                "glasgow","edinburgh","cardiff","wales","scotland","england","brentwood",
                            ]
                            if ("reino unido" in comp_low_loc) or ("üè¥" in (active_comp or "")) or ("üè¥" in raw) or any(h in low_loc for h in uk_hints) or re.search(r"\b[a-z]+shire\b", low_loc):
                                looks_like_box_location = True
            venue_only = (((_is_probable_venue_line(raw) and (not has_match) and (not has_time) and (not has_ch) and (not has_global) and (not stage)) or box_inline_venue or looks_like_box_location))
            
            # BOX: si la sede llega DESPU√âS del combate, adj√∫ntala al evento abierto (no la trates como inicio de otro).
            if open_event and venue_only and seg_is_box and (not open_event.get("meta", {}).get("sede")) and (not has_match) and (not has_time) and (not has_ch) and (not has_global) and (not stage):
                meta = open_event.get("meta", {})
                meta["sede"] = _fix_venue_name(_strip_comp_prefix(raw, open_event.get("comp","") or active_comp) or raw)
                open_event["meta"] = meta
                pending_venue_seg = None
                continue

            stage_only = bool(stage) and (not has_match) and (not has_time) and (not has_ch) and (not has_global) and (not venue_only)
            global_only = bool(global_txt) and (not has_match) and (not has_time) and (not has_ch)

            # Si ya hay un evento completo abierto y aparece un nuevo bloque (sede/instancia/partido), ci√©rralo.
            if open_event and (has_match or stage_only or venue_only) and (open_event.get('meta', {}).get('hora_text') or open_event.get('meta', {}).get('canales') or open_event.get('meta', {}).get('canal')):
                _flush_open_event()

            if stage_only:
                # BOX no lleva Jornada/Ronda; evita contaminar combates.
                if seg_is_box:
                    continue
                st_low = safe_lower(stage)
                comp_low = safe_lower(active_comp)

                # FA Cup: SOLO usa Ronda. Si se cuela una Jornada por aplanado del HTML, se ignora.
                # Adem√°s, cuando llega una ronda v√°lida, limpiamos la jornada para que nunca gane prioridad.
                if comp_low == "fa cup":
                    if st_low.startswith("jornada"):
                        continue
                    current_ronda_seg = stage
                    current_jornada_seg = None
                    continue

                if st_low.startswith('jornada'):
                    current_jornada_seg = stage
                else:
                    current_ronda_seg = stage
                continue

            if venue_only:
                # Caso especial: encabezados BOX a veces vienen como 'Evento BOX... Plaza Juarez'
                # Guardamos solo la sede para no confundir con 'Juarez' (Liga MX) ni contaminar el texto.
                comp_inline_v = _normalize_competition_extended(raw)
                if comp_inline_v and safe_lower(comp_inline_v).startswith("evento box"):
                    v_only = _strip_comp_prefix(raw, comp_inline_v)
                    # Si el "resto" es solo emoji (p.ej. "üè¥ü•ä"), NO lo tomes como sede.
                    if v_only and (not re.search(r"[A-Za-z0-9√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]", v_only)):
                        pass
                    else:
                        pending_venue_seg = v_only or raw
                else:
                    if pending_venue_seg != raw:
                        pending_venue_seg = raw
                continue

            if global_only:
                pending_global_seg = global_txt
                continue

            # Adjuntar detalles a evento abierto (si corresponde)
            if open_event and (not has_match):
                meta = open_event.get('meta', {})
                if not meta.get('sede') and _is_probable_venue_line(raw) and (not has_time) and (not has_ch) and (not has_global):
                    meta['sede'] = _fix_venue_name(raw)
                    open_event['meta'] = meta
                    continue
                if global_txt and not meta.get('global'):
                    meta['global'] = global_txt
                _attach_time_and_channels(meta, raw)
                open_event['meta'] = meta
                continue

            # Continuaci√≥n suelta de canales (por aplanado de Wix):
            # Si la l√≠nea solo trae canales/TV y NO hay un evento abierto, la ignoramos
            # para evitar crear "partidos" fantasma con nombres de canales.
            if (not open_event) and (not has_match) and (not has_time) and has_ch and (not stage) and (not venue_only) and (not has_global):
                # Ej: "Sky Sport Calcio, One Soccer, ..." (continuaci√≥n de canales)
                continue

            # Inicio de nuevo evento
            comp_inline = _normalize_competition_extended(raw)

            # Si la l√≠nea es SOLO un encabezado de competencia (sin contenido), no crear evento.
            if comp_inline and (not has_match) and (not has_time) and (not has_ch) and (not has_global) and (not stage) and (not venue_only):
                rest = _strip_comp_prefix(raw, comp_inline)
                if not rest:
                    continue

            competition = comp_inline or active_comp

            # Rescate BOX: combates Persona vs Persona mezclados dentro de ligas de f√∫tbol.
            if box_rescue and (not safe_lower(competition).startswith("evento box")):
                competition = _infer_box_comp_from_venue(pending_venue_seg or raw)

            # Guard meticuloso: 'UCLA' es un equipo, nunca 'UCL' (UEFA).
            # Si aparece UCLA en cualquier parte del texto, forzamos NCAA y elegimos el deporte por contexto/emoji.
            _low_raw = safe_lower(raw)
            if (not safe_lower(competition).startswith("evento box")) and re.search(r"\bucla\b", _low_raw):
                if ('softball' in _low_raw) or ('ü•é' in raw) or ('softball' in safe_lower(competition)):
                    competition = 'NCAA Softball'
                else:
                    competition = 'NCAA Basketball'


            meta: dict = {"type": "sport"}
            if current_jornada_seg and (not safe_lower(competition).startswith("evento box")):
                meta['jornada'] = current_jornada_seg
            if current_ronda_seg and (not safe_lower(competition).startswith("evento box")):
                meta['ronda'] = current_ronda_seg
            if pending_venue_seg:
                meta['sede'] = _fix_venue_name(pending_venue_seg)
            if pending_global_seg:
                meta['global'] = pending_global_seg

            categoria = _extract_categoria(raw)
            if categoria:
                meta['categoria'] = categoria

            _attach_time_and_channels(meta, raw)

            # T√≠tulo
            if has_match:
                team_a = _fix_team_name(a, competition)
                team_b = _fix_team_name(b, competition)
                # Premier League üè¥‚öΩÔ∏è puede ser Escocia: decide por equipos
                if is_scottish_match_global(team_a, team_b):
                    competition = 'Liga Escocesa'
                title = f"{team_a} vs {team_b}"
            else:
                title = _strip_comp_prefix(raw, competition)
                # Eventos sin 'vs': corta tiempo/canales del t√≠tulo para no duplicar.
                if title:
                    # quita 'por ...'
                    title = re.sub(r"(?i)\s+\bpor\b\s+.+$", "", title).strip()
                    tm2 = RX_TIME_AMPM.search(title)
                    if tm2:
                        tcut = title[:tm2.start()].strip()
                        tcut = re.sub(r"(?i)\b(a\s+las?|a\s+la)\s*$", "", tcut).strip()
                        tcut = re.sub(r"(?i)\b(a\s*partir|apartir)\s+de\s+las?\s*$", "", tcut).strip()
                        tcut = re.sub(r"(?i)\b(a\s*partir|apartir)\s+de\s*$", "", tcut).strip()
                        if len(tcut) >= 3:
                            title = tcut
                # WWE: extrae promoci√≥n y evita repetirla como t√≠tulo largo
                if 'wwe' in safe_lower(competition):
                    m = re.search(r"(?i)\b(nxt|raw|smackdown|dynamite)\b", title)
                    if m:
                        promo = m.group(1)
                        meta['ronda'] = promo.upper()
                        title = promo.title()

            # limpia pendientes por partido/evento
            # BOX: la sede suele aplicar a TODOS los combates del evento.
            # NO la borres al iniciar cada combate; solo se reemplaza si llega una nueva sede.
            if (not seg_is_box) and (not box_rescue) and (not safe_lower(competition).startswith("evento box")) and ("ufc" not in safe_lower(competition)):
                pending_venue_seg = None
            pending_global_seg = None

            open_event = {"t": title, "comp": competition, "meta": meta, "raw": raw}

        _flush_open_event()

    _finalize_pending_event()


    # ------------------------------------------------------------------
    # Post-procesador anti-revoltura (solucion "drastica" pero segura):
    # Re-clasifica competencias cuando la fuente pierde encabezados o etiqueta mal.
    #
    # Objetivos:
    #   1) FA Cup siempre debe quedar como "FA Cup" (no se mezcla con Liga MX / Premier).
    #   2) "Premier League" + equipos escoceses => "Liga Escocesa".
    #   3) BOX nunca lleva Jornada/Ronda; ademas propagamos sede si falta en algun combate.
    # ------------------------------------------------------------------

    FA_CUP_TEAM_HINTS = {
        # Los que reportaste + varios comunes para evitar que se "olvide"
        "west ham", "manchester city", "salford", "aston villa", "newcastle",
        "liverpool", "brighton", "burton",
        # otros ingleses frecuentes (ayuda en futuras capturas)
        "arsenal", "chelsea", "manchester united", "tottenham", "everton",
        "fulham", "brentford", "wolves", "bournemouth", "crystal palace",
        "nottingham forest", "leicester", "southampton", "ipswich", "sunderland",
        "sheffield", "middlesbrough", "leeds", "norwich", "watford", "derby",
        "preston", "blackburn", "coventry", "cardiff", "swansea",
    }

    def _has_any_hint(text: str, hints: set[str]) -> bool:
        tl = safe_lower(text or "")
        return any(h in tl for h in hints)

    def _force_comp(it: dict, new_comp: str) -> None:
        meta = it.get("meta") or {}
        meta["competition"] = new_comp
        it["meta"] = meta

    def _extract_ordinal_round(text: str) -> str | None:
        """Extrae una 'Xta Ronda' (p.ej. 4ta Ronda / Cuarta Ronda) desde texto."""
        low = safe_lower(text or "")

        def _ordinal_es(n: int) -> str:
            if n == 1:
                return "1ra"
            if n == 2:
                return "2da"
            if n == 3:
                return "3ra"
            if n == 7:
                return "7ma"
            if n == 8:
                return "8va"
            if n == 9:
                return "9na"
            if n == 10:
                return "10ma"
            return f"{n}ta"

        def _round_only(n: int) -> str:
            return f"{_ordinal_es(n)} Ronda"


        # 1) formato num√©rico
        m = re.search(r"\b(\d{1,2})\s*(?:ta|¬™|a)?\s*ronda\b", low)
        if m:
            n = m.group(1)
            return _round_only(int(n))

        # 2) formato con palabra
        word_map = {
            "primera": "1",
            "segunda": "2",
            "tercera": "3",
            "cuarta": "4",
            "quinta": "5",
            "sexta": "6",
            "s√©ptima": "7",
            "septima": "7",
            "octava": "8",
            "novena": "9",
            "d√©cima": "10",
            "decima": "10",
        }
        for w, n in word_map.items():
            if re.search(r"\b" + re.escape(w) + r"\s*ronda\b", low):
                return _round_only(int(n))
        return None

    def _force_fa_cup(it: dict, blob_text: str) -> None:
        """Forza FA Cup y blinda la instancia.

        Error recurrente: al estar el HTML aplanado, se "pega" una Jornada de Liga MX.
        Aqu√≠ se elimina la Jornada SIEMPRE y se intenta rescatar 'Xta Ronda'.
        """
        _force_comp(it, "FA Cup")
        meta = it.get("meta") or {}
        meta.pop("jornada", None)

        # Mant√©n una ronda existente si ya est√° bien, si no, extrae del texto disponible.
        ronda = str(meta.get("ronda") or "").strip()
        if not ronda or ("ronda" not in safe_lower(ronda)):
            rr = _extract_ordinal_round(blob_text)
            if not rr:
                rr = _extract_ordinal_round(str(meta.get("raw") or ""))
            if not rr:
                rr = _extract_ordinal_round(str(it.get("t") or ""))
            if rr:
                meta["ronda"] = rr

        it["meta"] = meta

    # 1) Reclasificacion por item (mas robusto que solo por segmento)
    for it in items:
        meta = it.get("meta") or {}
        comp = str(meta.get("competition") or "").strip()
        raw = str(meta.get("raw") or "")
        title = str(it.get("t") or "")
        blob = f"{title} {raw}"

        # A0) Rescate por equipos (cuando el header se perdi√≥ y el partido qued√≥ pegado a otra liga)
        #     - Championship / League One (Inglaterra, EFL)
        #     - Primera A Ecuador (Liga de Ecuador)
        #     - A-League (Australia)
        try:
            ta3, tb3 = _extract_match_teams(title)  # preferimos el t√≠tulo ya limpio
            if not (ta3 and tb3):
                ta3, tb3 = _extract_match_teams(raw)
        except Exception:
            ta3, tb3 = (None, None)

        if ta3 and tb3:
            ka3, kb3 = _fold_key(ta3), _fold_key(tb3)

            # Championship (EFL - Inglaterra)
            comp_low = safe_lower(comp or "")
            _protect_low = {
                "premier league","fa cup","fa womens cup","womens super league",
                "serie a","serie b","bundesliga","bundesliga 2","ligue 1","laliga","la liga femenil",
                "primeira liga","segunda liga","eredivisie","partidos mls","pretemporada mlb",
                "primera divisi√≥n per√∫","primera division per√∫","primera divisi√≥n chile","primera division chile",
                "primera divisi√≥n costa rica","primera division costa rica",
            }

            # Championship (EFL - Inglaterra): solo si AMBOS equipos corresponden (evita mezclar con Premier League)
            if (ka3 in EFL_CHAMP_TEAMS and kb3 in EFL_CHAMP_TEAMS) and (comp_low not in _protect_low) and ("championship" not in comp_low) and ("league one" not in comp_low):
                _force_comp(it, "Championship")
                comp = "Championship"

            # League One (EFL - Inglaterra) ‚Äî futuro
            if (("league one" in safe_lower(title)) or ("league one" in safe_lower(raw))) and ("league one" not in safe_lower(comp or "")):
                _force_comp(it, "League One")
                comp = "League One"

            # Primera A Ecuador (Liga de Ecuador)
                        # Primera A Ecuador (Liga de Ecuador): solo si ambos equipos corresponden.
            if (ka3 in ECUADOR_LIGA_TEAMS and kb3 in ECUADOR_LIGA_TEAMS) and ("ecuador" not in comp_low) and (comp_low not in _protect_low):
                _force_comp(it, "Primera A Ecuador")
                comp = "Primera A Ecuador"

            # A-League (Australia) / A-League Women
                        # A-League (Australia) / A-League Women: solo si ambos equipos corresponden.
            if (ka3 in ALEAGUE_TEAMS and kb3 in ALEAGUE_TEAMS) and ("a-league" not in comp_low) and (comp_low not in _protect_low):
                forced = "A-League Women" if ((ka3 in ALEAGUE_WOMEN_HINTS) or (kb3 in ALEAGUE_WOMEN_HINTS)) else "A-League"
                _force_comp(it, forced)
                comp = forced

        # A) Liga Escocesa: la fuente suele confundirla y ponerla bajo "Premier League".
        #    Para evitar mezclar otras ligas (La Liga, Ligue 1, etc.) u otros deportes (NBA/NHL/MLB),
        #    SOLO corregimos cuando el contexto original es Premier League / Scottish / Premiership.
        try:
            a2, b2 = _extract_match_teams(title)
        except Exception:
            a2, b2 = (None, None)

        comp_low = safe_lower(comp or "")
        is_scot_source_ctx = (
            ("premier league" in comp_low) or
            ("scottish" in comp_low) or
            ("premiership" in comp_low) or
            (comp_low.strip() == "liga escocesa")
        )
        is_europe_ctx = any(k in comp_low for k in ["uefa", "champions", "europa", "conference"])
        is_us_sport_ctx = any(k in comp_low for k in ["nba", "wnba", "nfl", "mlb", "nhl", "ncaa"])

        if a2 and b2 and is_scot_source_ctx and (not is_europe_ctx) and (not is_us_sport_ctx):
            if is_scottish_match_global(a2, b2):
                _force_comp(it, "Liga Escocesa")
                comp = "Liga Escocesa"

        # A3) Scottish Premiership expl√≠cita (algunas fuentes la ponen as√≠)
        if comp and ("scottish" in safe_lower(comp) or "premiership" in safe_lower(comp)):
            _force_comp(it, "Liga Escocesa")
            comp = "Liga Escocesa"

        # B) BOX: no permite Jornada/Ronda nunca (evita 'Jornada#6' en Plaza Juarez, etc.)
        if safe_lower(comp).startswith("evento box"):
            meta.pop("jornada", None)
            meta.pop("ronda", None)
            it["meta"] = meta

        # B1) Si el texto es claramente BOX pero el encabezado qued√≥ como "BOX/BOXEO" (sin 'Evento'),
        # normaliza a "Evento BOX" para que SIEMPRE exista titular.
        if safe_lower(comp) in {"box", "boxeo"}:
            _force_comp(it, "Evento BOX")
            meta.pop("jornada", None)
            meta.pop("ronda", None)
            meta["sport_kind"] = "combat"
            it["meta"] = meta
            comp = "Evento BOX"


        # B2) Rescate final BOX si el combate qued√≥ pegado bajo otro titular (ej. Olimpiadas).
        # Se√±ales: Persona vs Persona + (DAZN/PPV/ü•ä/BOX) en cualquier parte del blob.
        if (not safe_lower(comp).startswith("evento box")):
            try:
                ba, bb = _extract_match_teams(title)
            except Exception:
                ba, bb = (None, None)
            if SOURCE_HAS_BOX and ba and bb and _is_person_name_like(ba) and _is_person_name_like(bb):
                blob_low = safe_lower(blob)
                if ("ü•ä" in blob) or ("dazn" in blob_low) or ("ppv" in blob_low) or ("box" in blob_low) or ("boxeo" in blob_low) or re.search(r"\b(ko|tko)\b", blob_low) or re.search(r"\b(wbc|wba|ibf|wbo)\b", blob_low):
                    sede = str(meta.get("sede") or "")
                    new_comp = _infer_box_comp_from_venue(sede)
                    _force_comp(it, new_comp)
                    meta.pop("jornada", None)
                    meta.pop("ronda", None)
                    it["meta"] = meta
                    comp = new_comp
        # B2) Rescate final ultra-defensivo:
        # Si es Persona vs Persona (y NO tenis), forzamos Evento BOX aunque el bloque venga como f√∫tbol u otro.
        # Esto garantiza que el BOX gen√©rico siempre tenga titular.
        try:
            a3, b3 = _extract_match_teams(title)
        except Exception:
            a3, b3 = (None, None)
        if SOURCE_HAS_BOX and a3 and b3 and _is_person_name_like(a3) and _is_person_name_like(b3):
            low_blob = safe_lower(blob)
            tennis_ctx = ("üéæ" in blob) or any(w in low_blob for w in ["tenis", "tennis", "atp", "wta", "wimbledon", "roland garros", "us open", "australian open", "masters", "challenger", "itf"])
            if (not tennis_ctx) and ("‚öΩ" not in blob):
                if (not comp) or _is_soccer_comp_name(comp) or bool(meta.get("jornada")) or bool(meta.get("ronda")):
                    new_comp = _infer_box_comp_from_venue(str(meta.get("sede") or "") + " " + blob)
                    _force_comp(it, new_comp)
                    meta.pop("jornada", None)
                    meta.pop("ronda", None)
                    meta["sport_kind"] = "combat"
                    it["meta"] = meta
                    comp = new_comp

        # B2) FA Cup ya clasificada: blindaje de instancia (quita Jornada SIEMPRE)
        if SOURCE_HAS_FA_CUP and safe_lower(comp) == "fa cup":
            _force_fa_cup(it, blob)
            comp = "FA Cup"

        # B3) Liga MX / Liga MX Femenil: NO debe traer Ronda de copas.
        if safe_lower(comp) in {"liga mx", "liga mx femenil"}:
            meta.pop("ronda", None)
            it["meta"] = meta

        # C) FA Cup: deteccion explicita (palabra clave) o implicita (ronda + equipos)
        # C2) FA Cup por 'hints fuertes' aunque no venga 'Ronda' (Burton/Salford suelen ser copa).
        if SOURCE_HAS_FA_CUP and safe_lower(comp) in {"liga mx", "premier league", "championship", "league one", "segunda division", "serie a", "serie b", "bundesliga", "bundesliga 2"} and _has_any_hint(blob, {"burton", "salford"}):
            _force_fa_cup(it, blob)
            continue

        if SOURCE_HAS_FA_CUP and (re.search(r"(?i)\bfa\s*cup\b", blob) or re.search(r"(?i)copa\s+de\s+inglaterra", blob)):
            _force_fa_cup(it, blob)
            continue

        ronda = str(meta.get("ronda") or "")
        jornada = str(meta.get("jornada") or "")

        # Si trae "Ronda" pero NO "Jornada", y ademas hay equipos ingleses/hints, forzamos FA Cup
        if SOURCE_HAS_FA_CUP and (not jornada) and ronda and ("ronda" in safe_lower(ronda)):
            if _has_any_hint(blob, FA_CUP_TEAM_HINTS):
                _force_fa_cup(it, blob)
                continue

        # Si por error quedo como Liga MX/Premier League con "Ronda", es casi seguro copa (aqui: FA Cup)
        if SOURCE_HAS_FA_CUP and safe_lower(comp) in {"liga mx", "premier league"} and ronda and ("ronda" in safe_lower(ronda)):
            if _has_any_hint(blob, FA_CUP_TEAM_HINTS):
                _force_fa_cup(it, blob)
                continue

    # 2) Propagar sede dentro de cada evento BOX (si falta en algun combate)
    from collections import Counter
    box_groups: dict[str, list[dict]] = {}
    for it in items:
        meta = it.get("meta") or {}
        comp = str(meta.get("competition") or "")
        if safe_lower(comp).startswith("evento box"):
            box_groups.setdefault(comp, []).append(it)

    for comp, group in box_groups.items():
        venues = []
        for it in group:
            v = (it.get("meta") or {}).get("sede")
            if v and str(v).strip():
                venues.append(str(v).strip())
        if not venues:
            continue
        common = Counter(venues).most_common(1)[0][0]
        for it in group:
            meta = it.get("meta") or {}

            if not str(meta.get("sede") or "").strip():
                meta["sede"] = common
                it["meta"] = meta

    # Blindaje final: no mostrar "UEFA Champions League" si no viene expl√≠cito en la fuente.
    def _is_explicit_uefa_item(it: dict) -> bool:
        meta = it.get("meta") or {}
        raw = str(meta.get("raw") or "")
        tl = safe_lower(raw)
        if re.search(r"\bucla\b", tl):
            return False
        if ("champions league" in tl) or ("uefa" in tl and "champions" in tl):
            return True
        if re.search(r"\bucl\b", tl) and any(e in raw for e in ["üèÜ", "‚öΩ", "üá™üá∫"]):
            return True
        return False

    filtered: list[dict] = []
    for it in items:
        meta = it.get("meta") or {}
        comp = str(meta.get("competition") or "")
        title = str(it.get("t") or "")

        # ------------------------------------------------------------
        # FIX cr√≠tico: LPGA Tour no debe mezclarse dentro de PGA Tour.
        # - A veces Wix aplana el HTML y el item queda con competition='PGA Tour'
        #   aunque el texto pertenece a LPGA. Reasignamos.
        # - Tambi√©n eliminamos el 'header fantasma' "LPGA Tour" que se cuela
        #   como si fuera un evento.
        # ------------------------------------------------------------
        blob_lpga = f"{title} {meta.get('raw') or ''}"
        if safe_lower(comp) == "pga tour" and re.search(r"(?i)\blpga\b", blob_lpga):
            meta["competition"] = "LPGA Tour"
            it["meta"] = meta
            comp = "LPGA Tour"

        # Header fantasma: si el t√≠tulo es 'LPGA Tour' pero la competencia NO es LPGA Tour, descarta.
        if re.search(r"(?i)\blpga\s*tour\b", title) and safe_lower(comp) != "lpga tour":
            continue

        # Si el t√≠tulo es (solo) el encabezado de LPGA Tour (a veces viene con emojis/banderas),
        # NO debe imprimirse como "evento". En su lugar:
        #   - intentamos derivar el t√≠tulo real desde el raw (p.ej. "Honda LPGA Thailand ...")
        #   - si no se puede derivar, lo descartamos como header fantasma
        if _fold_key(title) == "lpga tour":
            raw_line = str(meta.get("raw") or "")
            derived = _strip_comp_prefix(raw_line, "LPGA Tour")
            if derived:
                # quita 'por ...'
                derived = re.sub(r"(?i)\s+\bpor\b\s+.+$", "", derived).strip()
                # corta antes de la primera hora
                tm2 = RX_TIME_AMPM.search(derived)
                if tm2:
                    tcut = derived[:tm2.start()].strip()
                    tcut = re.sub(r"(?i)\b(a\s+las?|a\s+la)\s*$", "", tcut).strip()
                    tcut = re.sub(r"(?i)\b(a\s*partir|apartir)\s+de\s+las?\s*$", "", tcut).strip()
                    tcut = re.sub(r"(?i)\b(a\s*partir|apartir)\s+de\s*$", "", tcut).strip()
                    if len(tcut) >= 3:
                        derived = tcut
                # quita emojis/puntuaci√≥n al inicio
                derived = re.sub(r"^[^A-Za-z0-9√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]+", "", derived).strip()

            if derived and (_fold_key(derived) != "lpga tour"):
                it["t"] = derived
                title = derived
            else:
                # Si NO logramos derivar un t√≠tulo real, y tampoco hay datos √∫tiles, descartarlo.
                has_event_meta = bool(
                    str(meta.get("hora_original") or "").strip() or
                    str(meta.get("start_utc") or "").strip() or
                    (meta.get("canales") or []) or
                    str(meta.get("sede") or "").strip() or
                    str(meta.get("global") or "").strip() or
                    str(meta.get("jornada") or "").strip() or
                    str(meta.get("ronda") or "").strip() or
                    RX_TIME_AMPM.search(raw_line)
                )
                if not has_event_meta:
                    continue

        if comp == "UEFA Champions League" and (not _is_explicit_uefa_item(it)):
            continue
        filtered.append(it)

    return filtered[:600]


def classify_movie_or_series(raw: str) -> tuple[str, dict]:
    low = safe_lower(raw)

    is_series = bool(RX_SERIE.search(raw))
    has_year = bool(RX_YEAR.search(raw))
    has_quality = bool(RX_QUALITY.search(raw))

    kind = "series" if is_series else "movie"
    if not is_series and (has_year or has_quality):
        kind = "movie"

    flags = []
    if any(x in low for x in ["lat", "latino", "esp lat", "esplat"]):
        flags.append("üá≤üáΩ Latino")
    if any(x in low for x in ["cast", "castellano", "esp ", "espa√±a"]):
        flags.append("üá™üá∏ Castellano")
    if any(x in low for x in ["sub", "subt", "vose"]):
        flags.append("‚å®Ô∏è Subtitulado")
    if any(x in low for x in ["dual", "multi"]):
        flags.append("üîä Dual/Multi")

    q = RX_QUALITY.search(raw)
    quality = q.group(0).upper() if q else None

    se = None
    m = RX_SERIE.search(raw)
    if m:
        se = m.group(0).upper().replace(" ", "")

    meta = {
        "raw": raw,
        "type": kind,
        "flags": flags,
        "quality": quality,
        "se": se,
        "is_vip": True
    }
    return kind, meta


def normalize_date_label_es(raw: str) -> str | None:
    """
    Normaliza etiquetas de fecha a:
      'DD DE MES DE YYYY'

    Acepta variantes comunes del feed (tolerante a emojis/puntuaci√≥n):
      - 'mi√©rcoles 21 de enero de 2026'
      - '01 DE FEBRERO DE 2026'
      - 'MIERCOLES 18 DE FEBRERO DEL 2026'
      - 'Fecha: Miercoles 18 de Febrero del 2026'
      - '19 de Febrero' (sin a√±o) -> asume a√±o actual (seg√∫n USER_TZ)
      - 'Contenido Nuevo'  -> fecha del d√≠a (seg√∫n USER_TZ)
    """
    if not raw:
        return None

    # Limpieza b√°sica
    raw2 = strip_weird_symbols_keep_plus(raw).replace("\u00a0", " ").replace("\xa0", " ").strip()
    low2 = safe_lower(raw2).strip()

    # "Contenido Nuevo" puede venir SOLO (placeholder) o venir con fecha expl√≠cita:
    #   "CONTENIDO NUEVO JUEVES 19 DE FEBRERO DEL 2026"
    # En ese caso, extraemos la fecha real (no la fecha del servidor).
    if "contenido nuevo" in low2:
        # intenta parsear lo que viene despu√©s de "Contenido Nuevo"
        tail = re.sub(r"(?i)\bcontenido\s+nuevo\b", " ", raw2).strip()
        if tail:
            cand = normalize_date_label_es(tail)
            if cand:
                return cand

        # si no pudimos extraer fecha real, tratamos como placeholder
        try:
            tz = ZoneInfo(USER_TZ)
        except Exception:
            tz = timezone.utc
        today = datetime.now(tz).date()
        months = ["ENERO","FEBRERO","MARZO","ABRIL","MAYO","JUNIO","JULIO","AGOSTO","SEPTIEMBRE","OCTUBRE","NOVIEMBRE","DICIEMBRE"]
        return f"{today.day:02d} DE {months[today.month-1]} DE {today.year}"

    # Quita prefijo FECHA:
    s = re.sub(r"(?i)^\s*fecha\s*[:\-]?\s*", "", raw2).strip()
    s = re.sub(r"\s{2,}", " ", s)

    # Normalizaci√≥n para matching
    up = s.upper().replace("SETIEMBRE", "SEPTIEMBRE")
    up = re.sub(r"^(LUNES|MARTES|MIERCOLES|MI√âRCOLES|JUEVES|VIERNES|SABADO|S√ÅBADO|DOMINGO)\s+", "", up).strip()
    up = re.sub(r"\s+DEL\s+", " DE ", up)
    up = re.sub(r"\s{2,}", " ", up).strip()

    # Quita basura al final (puntos, comas, etc.)
    up = re.sub(r"[^0-9A-Z√Å√â√ç√ì√ö√ë\s]+$", "", up).strip()
    up = re.sub(r"\s{2,}", " ", up).strip()

    months = ["ENERO","FEBRERO","MARZO","ABRIL","MAYO","JUNIO","JULIO","AGOSTO","SEPTIEMBRE","OCTUBRE","NOVIEMBRE","DICIEMBRE"]

    def _month_ok(mes: str) -> bool:
        return (mes or "").upper().replace("SETIEMBRE", "SEPTIEMBRE") in months

    # 1) Con a√±o expl√≠cito (varias variantes)
    m0 = re.match(r"^(\d{1,2})\s+(?:DE\s+)?([A-Z√Å√â√ç√ì√ö√ë]+)\s+(?:DE\s+)?(\d{4})$", up)
    if m0 and _month_ok(m0.group(2)):
        dd = m0.group(1).zfill(2)
        mes = m0.group(2).upper().replace("SETIEMBRE", "SEPTIEMBRE")
        yyyy = m0.group(3)
        return f"{dd} DE {mes} DE {yyyy}"

    # 2) Sin a√±o (asumimos a√±o actual, seg√∫n TZ)
    m1 = re.match(r"^(\d{1,2})\s+(?:DE\s+)?([A-Z√Å√â√ç√ì√ö√ë]+)$", up)
    if m1 and _month_ok(m1.group(2)):
        try:
            tz = ZoneInfo(USER_TZ)
        except Exception:
            tz = timezone.utc
        yyyy = datetime.now(tz).year
        dd = m1.group(1).zfill(2)
        mes = m1.group(2).upper().replace("SETIEMBRE", "SEPTIEMBRE")
        return f"{dd} DE {mes} DE {yyyy}"

    # 3) Formatos est√°ndar (min√∫sculas / con "de")
    m = RX_DATE_ES.search(s)
    if m:
        dd = m.group(1).zfill(2)
        mes = m.group(2).upper().replace("SETIEMBRE", "SEPTIEMBRE")
        yyyy = m.group(3)
        return f"{dd} DE {mes} DE {yyyy}"

    m2 = RX_DATE_CAPS.search(s.upper())
    if m2:
        dd = m2.group(1).zfill(2)
        mes = m2.group(2).upper().replace("SETIEMBRE", "SEPTIEMBRE")
        yyyy = m2.group(3)
        return f"{dd} DE {mes} DE {yyyy}"

    # 4) '19 de febrero' sin a√±o (texto en min√∫sculas)
    s_low = safe_lower(s).replace("setiembre", "septiembre").strip()
    m3 = re.match(r"^(\d{1,2})\s+de\s+(enero|febrero|marzo|abril|mayo|junio|julio|agosto|septiembre|octubre|noviembre|diciembre)\s*$", s_low, re.I)
    if m3:
        try:
            tz = ZoneInfo(USER_TZ)
        except Exception:
            tz = timezone.utc
        yyyy = datetime.now(tz).year
        dd = m3.group(1).zfill(2)
        mes = m3.group(2).upper()
        return f"{dd} DE {mes} DE {yyyy}"

    return None


# ---------------- MEDIA (pel√≠culas / series) ----------------
MIN_MEDIA_YEAR = int(os.getenv("MIN_MEDIA_YEAR", "2024"))
MAX_MEDIA_DATES = int(os.getenv("MAX_MEDIA_DATES", "2"))
RX_LANG_TAG = re.compile(r"\b(?:lat(?:ino)?|sub(?:t)?|dual|multi|cast(?:ellano)?|eng|vose)\b", re.I)
RX_URLISH = re.compile(r"(https?://|www\.)|\b[\w-]+\.(?:com|net|org|app|io|gg|me|tv|site|xyz)\b", re.I)

def _extract_any_year(raw: str) -> int | None:
    years = [int(y) for y in RX_YEAR.findall(raw or "")]
    return max(years) if years else None

def _is_support_garbage_media(raw: str) -> bool:
    low = safe_lower(raw or "").strip()
    if not low:
        return True
    # Links / dominios / urls
    if RX_URLISH.search(raw or ""):
        return True
    # Solo n√∫meros (c√≥digos)
    if re.fullmatch(r"\d{4,}", low):
        return True
    # Patrones t√≠picos: FIREDL / Downloader + c√≥digo
    if re.search(r"(?i)\b(firedl|downloader)\b\s*\d{3,}", raw or ""):
        return True
    if re.search(r"(?i)\bpin\b.*\d{3,}", raw or ""):
        return True
    # Keywords muy t√≠picas de soporte/apps
    bad_words = [
        "pin", "adult", "firedl", "downloader", "mediafire", "iptv", "smarters", "samrters", "gse",
        "vpn", "telmex", "firestick", "fire tv", "firetv", "android", "googletv", "google tv",
        "windows", "mac", "webplayer", "url", "link", "c√≥digo", "codigo", "banners",
        "antibloqueo", "aplicacion", "aplicaci√≥n", "version", "versi√≥n", "galeria", "galer√≠a",
        "preguntas frecuentes", "descontinuad", "equipos", "vigentes", "descontinuados",
        "dealer", "distribuidor", "plataformas", "buscador", "lupa", "solo telmex", "no telmex",
        "cualquier internet", "solo equipos", "ultima generacion", "√∫ltima generaci√≥n",
        "one plus", "xciptv", "ibo", "smartes"
    ]
    for w in bad_words:
        if w in low:
            return True
    # Demasiados d√≠gitos (c√≥digos) en una sola l√≠nea
    # Nota: en PEL√çCULAS/SERIES es normal ver "A√±o + resoluci√≥n" (ej. 1996 + 480p),
    # lo cual dispara muchos d√≠gitos. Solo marcamos como basura si NO parece un t√≠tulo real.
    digits = sum(ch.isdigit() for ch in (raw or ""))
    if digits >= 6 and digits >= (len(raw) * 0.25):
        # Si parece un √≠tem VOD (tiene a√±o + calidad/idioma) NO lo descartes.
        has_year = bool(RX_YEAR.search(raw or ""))
        has_quality = bool(RX_QUALITY.search(raw or ""))
        has_lang = bool(RX_LANG_TAG.search(raw or ""))
        letters = sum(ch.isalpha() for ch in (raw or ""))
        words = [w for w in re.split(r"\s+", (raw or "").strip()) if w]
        looks_like_vod = (has_year and (has_quality or has_lang) and letters >= 4 and len(words) >= 2)
        if not looks_like_vod:
            return True
    return False

def _looks_like_movie_title(raw: str) -> bool:
    if _is_support_garbage_media(raw):
        return False
    # Si parece serie, no la cuentes como pel√≠cula
    if RX_SERIE.search(raw or ""):
        return False
    has_quality = bool(RX_QUALITY.search(raw or ""))
    has_lang = bool(RX_LANG_TAG.search(raw or ""))
    y = _extract_any_year(raw)
    has_year = y is not None
    low = safe_lower(raw or "").strip()
    # Ej: 'NUEVA 2024' / 'NUEVA VERSION 2024'
    if re.fullmatch(r"(nueva|nuevo|nuevas|nuevos)\s*(?:version|versi√≥n)?\s*\d{4}", low):
        return False
    words = [w for w in re.split(r"\s+", (raw or "").strip()) if w]
    letters = sum(ch.isalpha() for ch in (raw or ""))
    return (has_year or has_quality or has_lang) and len(words) >= 2 and letters >= 4

def _looks_like_series_title(raw: str) -> bool:
    if _is_support_garbage_media(raw):
        return False
    if not RX_SERIE.search(raw or ""):
        return False
    words = [w for w in re.split(r"\s+", (raw or "").strip()) if w]
    letters = sum(ch.isalpha() for ch in (raw or ""))
    return len(words) >= 2 and letters >= 4


def parse_movies_series_items(tail_blocks: list[str]) -> list[dict]:
    """
    Extrae Pel√≠culas y Series del feed con:
      - Filtro anti-basura (Soporte/Apps) ya aplicado.
      - Clasificaci√≥n por:
          * date_label (solo >= MIN_MEDIA_YEAR)
          * super_section (Nuevas Peliculas / Nuevas Series / Nuevos Episodios)
          * section (g√©nero / Estrenos / etc.)
      - Preserva el ORDEN del feed y permite repetir secciones (ej. "Novedades" dos veces)
        usando meta.section_group.
    """
    items: list[dict] = []
    seen_ids: set[str] = set()

    current_bucket: str | None = None  # "Peliculas" | "Series"
    current_super: str | None = None   # "Nuevas Peliculas" | "Nuevas Series" | "Nuevos Episodios"
    current_date_label: str | None = None

    current_section: str = "Novedades"
    section_block_idx: int = 0
    current_section_group: str | None = None
    section_has_items: bool = False


    order_counter: int = 0

    mem = get_memory()
    bad_phrases = (mem or {}).get("bad_phrases", [])

    # G√©neros conocidos (se usa para mejorar split de headings pegados y reconocer encabezados)
    genres = [
        "novedades", "estrenos",
        "drama", "terror", "western", "comedia", "anime", "crimen", "latino",
        "documental", "documentales", "telenovelas",
        "accion", "acci√≥n", "accion y aventura", "acci√≥n y aventura",
        "ciencia ficcion", "ciencia ficci√≥n", "cienciaficcion", "ciencia-ficcion", "scifi", "sci fi", "sci-fi",
        "infantiles", "retro", "clasicos", "cl√°sicos",
        "suspenso", "thriller", "misterio",
        "fantasia", "fantas√≠a", "romance", "aventura", "familia",
        "animacion", "animaci√≥n",
        "musical", "historia"
    ]
    known_heading = set(genres + ["nuevas peliculas", "nuevas pel√≠culas", "nuevas series", "nuevos episodios", "nuevos capitulos", "nuevos cap√≠tulos"])

    # Fecha por defecto para evitar √≠tems sin fecha (la WebApp oculta date_label vac√≠o).
    # Importante: "Contenido Nuevo" en el Wix suele significar "lo m√°s reciente".
    # Si el sitio NO se actualiza hoy, mapearlo al d√≠a del servidor rompe (y/o colapsa fechas).
    # Estrategia:
    #   - Si existe "Contenido Nuevo" y despu√©s aparece una fecha expl√≠cita, asumimos que esa fecha
    #     es el "d√≠a anterior" y que "Contenido Nuevo" corresponde a +1 d√≠a.
    #   - Si no podemos inferir, usamos la fecha del servidor (USER_TZ).
    MONTHS = ["ENERO","FEBRERO","MARZO","ABRIL","MAYO","JUNIO","JULIO","AGOSTO","SEPTIEMBRE","OCTUBRE","NOVIEMBRE","DICIEMBRE"]

    def _lbl_to_date(lbl: str):
        up = (lbl or "").strip().upper()
        up = re.sub(r"^\s*FECHA\s*:\s*", "", up).strip()
        up = re.sub(r"^(LUNES|MARTES|MIERCOLES|MI√âRCOLES|JUEVES|VIERNES|SABADO|S√ÅBADO|DOMINGO)\s+", "", up).strip()
        up = re.sub(r"\s{2,}", " ", up)
        m = re.match(r"^(\d{1,2})\s+DE\s+([A-Z√Å√â√ç√ì√ö√ë]+)\s+(?:DE|DEL)\s+(\d{4})$", up)
        if not m:
            return None
        dd = int(m.group(1))
        mes = (m.group(2) or "").upper().replace("SETIEMBRE", "SEPTIEMBRE")
        yyyy = int(m.group(3))
        mm = (MONTHS.index(mes) + 1) if mes in MONTHS else 0
        if mm <= 0:
            return None
        return date(yyyy, mm, dd)

    def _date_to_lbl(d: date) -> str:
        return f"{d.day:02d} DE {MONTHS[d.month-1]} DE {d.year}"

    def _dmy_key(lbl: str):
        d = _lbl_to_date(lbl)
        if not d:
            return (9999, 99, 99, (lbl or "").upper())
        return (-d.year, -d.month, -d.day, (lbl or "").upper())

    try:
        tz = ZoneInfo(USER_TZ)
    except Exception:
        tz = timezone.utc
    _server_today = datetime.now(tz).date()
    _server_today_lbl = _date_to_lbl(_server_today).upper()

    # 1) Fechas expl√≠citas presentes
    explicit_dates: list[str] = []
    for ln in (tail_blocks or []):
        if not ln:
            continue
        if "contenido nuevo" in safe_lower(ln):
            continue
        norm_ln = normalize_date_label_es(ln)
        if not norm_ln:
            continue
        try:
            yr = int(norm_ln[-4:])
        except Exception:
            yr = None
        if yr and yr >= MIN_MEDIA_YEAR:
            explicit_dates.append(norm_ln.upper())
    explicit_dates = sorted(list(dict.fromkeys(explicit_dates)), key=_dmy_key)
    _inferred_latest_explicit = explicit_dates[0] if explicit_dates else None

    # 2) Inferir fecha para "Contenido Nuevo" seg√∫n la primera fecha expl√≠cita que aparezca DESPU√âS.
    _contenido_nuevo_label: str | None = None
    cn_idx = None
    for i, ln in enumerate(tail_blocks or []):
        if "contenido nuevo" in safe_lower(ln):
            cn_idx = i
            break
    if cn_idx is not None:
        older_lbl = None
        for ln in (tail_blocks or [])[cn_idx + 1:]:
            if not ln:
                continue
            if "contenido nuevo" in safe_lower(ln):
                continue
            cand = normalize_date_label_es(ln)
            if cand:
                older_lbl = cand.upper()
                break
        if older_lbl:
            od = _lbl_to_date(older_lbl)
            if od:
                guess = od + timedelta(days=1)
                if guess <= _server_today:
                    _contenido_nuevo_label = _date_to_lbl(guess).upper()
                else:
                    _contenido_nuevo_label = _server_today_lbl
        if not _contenido_nuevo_label:
            _contenido_nuevo_label = _server_today_lbl

    # 3) Default para √≠tems sin fecha
    _default_date_label = (_contenido_nuevo_label or _inferred_latest_explicit or _server_today_lbl)

    def _norm_heading(raw_heading: str) -> str:
        s = strip_weird_symbols_keep_plus(raw_heading or "")
        # quita emojis/bullets al inicio
        s = re.sub(r"^[\s‚ú®‚≠êüåü‚Ä¢¬∑\-\‚Äì\‚Äî\*]+", "", s).strip()
        # quita ellipsis/puntos al final
        s = re.sub(r"[.¬∑‚Ä¢‚Ä¶]+$", "", s).strip()
        s = s.strip(":").strip()
        s = re.sub(r"\s{2,}", " ", s)

        # Canonicaliza algunos g√©neros para salida consistente (Series y Pel√≠culas)
        # Ej: "ciencia ficcion", "cienciaficcion", "ciencia-ficcion" -> "Ciencia Ficcion"
        low = safe_lower(s)
        low_nfkd = unicodedata.normalize("NFKD", low)
        low_nfkd = "".join(ch for ch in low_nfkd if not unicodedata.combining(ch))
        low_compact = re.sub(r"[\s\-]+", "", low_nfkd)
        if low_compact == "cienciaficcion":
            s = "Ciencia Ficcion"
        return s

    def _expand_media_lines(lines: list[str]) -> list[str]:
        """Wix a veces pega encabezados + t√≠tulo en una sola l√≠nea.
        Ej: 'Comedia‚Ä¶ Como llegar...' o '19 de Febrero NUEVAS SERIES'.
        Aqu√≠ los separamos para que el parser no meta todo en 'Novedades'.
        """
        out: list[str] = []
        for raw_line in (lines or []):
            if not raw_line:
                continue
            s0 = (raw_line or "").strip()
            if not s0:
                continue
            # Normaliza caracteres especiales SOLO para detecci√≥n de splits
            s = s0.replace("‚Ä¶", "...").replace("¬∑", ".")

            # 1) 'Contenido Nuevo' pegado con m√°s texto
            if re.search(r"(?i)\bcontenido\s+nuevo\b", s) and safe_lower(s).strip() not in {"contenido nuevo", "nuevo contenido"}:
                m = re.search(r"(?i)\bcontenido\s+nuevo\b", s)
                pre = (s[:m.start()] or "").strip(" \t:-‚Äî‚Äì")
                post = (s[m.end():] or "").strip(" \t:-‚Äî‚Äì")
                if pre:
                    out.append(pre)
                out.append("Contenido Nuevo")
                if post:
                    out.append(post)
                continue

            # 2) Fecha al inicio con contenido pegado
            m_date = re.match(
                r"^\s*(\d{1,2}\s+(?:de\s+)?[A-Za-z√Å√â√ç√ì√ö√ë]+(?:\s+(?:de|del)\s+\d{4})?)\s+(.+)$",
                s,
                re.I,
            )
            if m_date and normalize_date_label_es(m_date.group(1)):
                out.append(m_date.group(1).strip())
                rest = (m_date.group(2) or "").strip()
                if rest:
                    out.append(rest)
                continue

            # 3) Super-secciones pegadas (NUEVAS SERIES / NUEVOS EPISODIOS / NUEVAS PELICULAS)
            m_super = re.match(r"(?i)^\s*(nuevas?\s+pel[i√≠]culas)\s+(.+)$", s)
            if m_super:
                out.append("NUEVAS PELICULAS")
                out.append(m_super.group(2).strip())
                continue
            m_super = re.match(r"(?i)^\s*(nuevas?\s+series)\s+(.+)$", s)
            if m_super:
                out.append("NUEVAS SERIES")
                out.append(m_super.group(2).strip())
                continue
            m_super = re.match(r"(?i)^\s*(nuevos?\s+(?:episodios?|cap[i√≠]tulos?))\s+(.+)$", s)
            if m_super:
                out.append("NUEVOS EPISODIOS")
                out.append(m_super.group(2).strip())
                continue

            # 4) G√©nero pegado al inicio: 'Comedia... T√≠tulo'
            m_head = re.match(
                r"^\s*(?:‚ú®\s*)?([A-Za-z√Å√â√ç√ì√ö√ë][A-Za-z√Å√â√ç√ì√ö√ë\s]{2,32})\s*(?:\.\.\.|:)\s*(.+)$",
                s,
            )
            if m_head:
                head = _norm_heading(m_head.group(1))
                head_low = safe_lower(head)
                if head_low in known_heading or head_low in set(genres):
                    out.append(head + "...")
                    rest = (m_head.group(2) or "").strip()
                    if rest:
                        out.append(rest)
                    continue

            out.append(s0)
        return out

    # Expandimos primero para preservar encabezados + fechas aunque vengan pegados.
    tail_blocks = _expand_media_lines(tail_blocks)

    def _looks_like_heading(raw_line: str) -> str | None:
        """
        Detecta encabezados tipo:
          ‚ú® Novedades...
          ‚ú® Terror...
          Estrenos...
        """
        if not raw_line:
            return None
        s0 = raw_line.strip()
        s = _norm_heading(s0)
        if not s:
            return None

        low = safe_lower(s)
        # no confundir con t√≠tulos
        if RX_YEAR.search(s) or RX_QUALITY.search(s) or RX_SERIE.search(s):
            return None
        if _is_support_garbage_media(s):
            return None
        if len(s) < 3 or len(s) > 32:
            return None

        trigger = s0.startswith("‚ú®") or s0.endswith("...") or s0.endswith(":")
        if trigger or low in known_heading:
            # Normaliza capitalizaci√≥n suave (no fuerza acentos)
            return s[:1].upper() + s[1:]
        return None

    def _reset_section_blocks():
        # No creamos grupo autom√°ticamente: evitamos duplicar 'Novedades' cuando el feed ya trae ese heading.
        nonlocal section_block_idx, current_section, current_section_group, section_has_items
        section_block_idx = 0
        current_section = "Novedades"
        current_section_group = None
        section_has_items = False

    def _bump_section(new_section: str):
        nonlocal section_block_idx, current_section, current_section_group, section_has_items
        section_block_idx += 1 if section_block_idx else 1
        current_section = (new_section or "Novedades").strip() or "Novedades"
        current_section_group = f"{(current_date_label or '')}|{(current_super or '')}|{section_block_idx:03d}"
        section_has_items = False

    def _ensure_section_group():
        # Se usa antes de agregar el primer √≠tem de un bloque (si el feed no trajo heading previo).
        nonlocal section_block_idx, current_section_group, section_has_items
        if current_section_group:
            return
        if not section_block_idx:
            section_block_idx = 1
        current_section_group = f"{(current_date_label or '')}|{(current_super or '')}|{section_block_idx:03d}"
        section_has_items = False

    def _set_bucket_super(bucket: str, super_name: str):
        nonlocal current_bucket, current_super
        changed = (current_bucket != bucket) or (current_super != super_name)
        current_bucket = bucket
        current_super = super_name
        if changed:
            _reset_section_blocks()

    def _set_date_label(new_lbl: str | None):
        nonlocal current_date_label
        if new_lbl and new_lbl != current_date_label:
            current_date_label = new_lbl
            _reset_section_blocks()

    # Split de headings pegados al final de una l√≠nea de t√≠tulo:
    # "Se√±al acustica 2023 sub Infantiles..."  ->  ("Se√±al acustica 2023 sub", "Infantiles")
    RX_TAIL_HEADING = re.compile(r"\s+([A-Za-z√Å√â√ç√ì√ö√ë][A-Za-z√Å√â√ç√ì√ö√ë\s]{2,})\s*\.\.\.\s*$")

    def _split_trailing_heading(line: str) -> tuple[str, str | None]:
        s = (line or "").strip()
        m = RX_TAIL_HEADING.search(s)
        if not m:
            return s, None
        head = _norm_heading(m.group(1))
        if not head:
            return s, None
        head_low = safe_lower(head)
        # solo si parece heading real
        if head_low in known_heading or head_low in set(genres):
            title_part = s[:m.start()].rstrip()
            # Si la l√≠nea ES el heading (ej. 'Comedia...'), no la partas: se procesar√° como heading normal.
            if not title_part:
                return s, None
            return title_part, (head[:1].upper() + head[1:])
        return s, None

    for raw0 in tail_blocks:
        # Segunda capa de limpieza (men√∫/footer)
        if is_noise(raw0, bad_phrases):
            continue

        raw = strip_weird_symbols_keep_plus(raw0)
        low = safe_lower(raw).strip()

        # navegaci√≥n
        if low in {"inicio", "kaelus", "soporte", "menu", "men√∫", "home"}:
            continue
        if "kaelus" in low and len(low) < 18:
            continue
        if "wix" in low and len(low) < 30:
            continue

        # bucket (encabezados principales)
        if re.fullmatch(r"(pel[i√≠]culas|cine|pel[i√≠]culas\s+recientes)", low) or ("seccion peliculas" in low):
            current_bucket = "Peliculas"
            _set_bucket_super("Peliculas", "Nuevas Peliculas")
            if _default_date_label:
                _set_date_label(_default_date_label)
            continue

        if re.fullmatch(r"(series|series\s+recientes)", low) or ("seccion series" in low):
            current_bucket = "Series"
            _set_bucket_super("Series", "Nuevas Series")
            if _default_date_label:
                _set_date_label(_default_date_label)
            continue

        # super headers (fuerzan bucket)
        if re.search(r"\bnuevas?\s+pel[i√≠]culas\b", low):
            _set_bucket_super("Peliculas", "Nuevas Peliculas")
            continue

        if re.search(r"\bnuevas?\s+series\b", low):
            _set_bucket_super("Series", "Nuevas Series")
            continue

        if ("nuevos episodios" in low) or ("nuevos capitulos" in low) or ("nuevos cap√≠tulos" in low):
            # NO contaminar pel√≠culas
            if current_bucket == "Peliculas":
                continue
            _set_bucket_super("Series", "Nuevos Episodios")
            continue

        # fecha (solo >= MIN_MEDIA_YEAR)
        # "Contenido Nuevo" se mapea a la fecha m√°s reciente inferida del feed
        # para no romper cuando el Wix no se actualiza el mismo d√≠a del servidor.
        norm = (_contenido_nuevo_label or _default_date_label) if ("contenido nuevo" in low) else normalize_date_label_es(raw)
        if norm:
            try:
                yr = int(norm[-4:])
            except Exception:
                yr = None
            if yr and yr >= MIN_MEDIA_YEAR:
                _set_date_label(norm.upper())
            continue

        if ("fecha" in low) or re.search(r"\b(lunes|martes|mi√©rcoles|miercoles|jueves|viernes|s√°bado|sabado|domingo)\b", low):
            # Ej: "Fecha: Miercoles 18 de Febrero del 2026"  ->  "MIERCOLES 18 DE FEBRERO DEL 2026"
            lbl = re.sub(r"(?i)^\s*fecha\s*[:\-]?\s*", "", raw).strip()
            lbl = re.sub(r"\s{2,}", " ", lbl)
            norm_lbl = normalize_date_label_es(lbl) or lbl
            yr = _extract_any_year(norm_lbl)
            if yr and yr >= MIN_MEDIA_YEAR:
                _set_date_label(norm_lbl.upper())
            continue

        # heading pegado al final
        pending_heading = None
        raw, pending_heading = _split_trailing_heading(raw)
        low = safe_lower(raw).strip()

        # encabezado de g√©nero/secci√≥n gen√©rico (‚ú® ... / ...:)
        h = _looks_like_heading(raw0) or _looks_like_heading(raw)
        if h:
            h_low = safe_lower(h)
            # super headings dentro de headings (por si vienen con ‚ú®)
            if ("nuevos episodios" in h_low) or ("nuevos capitulos" in h_low) or ("nuevos cap√≠tulos" in h_low):
                if current_bucket != "Peliculas":
                    _set_bucket_super("Series", "Nuevos Episodios")
                continue
            if "nuevas series" in h_low:
                _set_bucket_super("Series", "Nuevas Series")
                continue
            if "nuevas peliculas" in h_low or "nuevas pel√≠culas" in h_low:
                _set_bucket_super("Peliculas", "Nuevas Peliculas")
                continue

            # secci√≥n normal (Estrenos, Terror, etc.)
            if safe_lower(current_section or "") == h_low and not section_has_items:
                # Evita duplicar encabezado inicial ("Novedades" default + "Novedades..." del feed)
                current_section = h
                _ensure_section_group()
            else:
                _bump_section(h)
            continue

        # Basura de soporte/apps (PIN, FIREDL, URLs, etc.)
        if _is_support_garbage_media(raw):
            continue

        # t√≠tulo real (filtros estrictos)
        if len(raw) < 8:
            continue

        is_series_line = bool(RX_SERIE.search(raw))
        if is_series_line:
            if not _looks_like_series_title(raw):
                continue
        else:
            if not _looks_like_movie_title(raw):
                continue

        kind, meta = classify_movie_or_series(raw)
        bucket_for_item = "Series" if kind == "series" else "Peliculas"

        # fijar bucket por defecto para contexto si a√∫n no se detect√≥
        if current_bucket is None:
            current_bucket = bucket_for_item
            if current_bucket == "Peliculas":
                current_super = current_super or "Nuevas Peliculas"
            else:
                current_super = current_super or "Nuevas Series"
            if _default_date_label and not current_date_label:
                _set_date_label(_default_date_label)
            else:
                _reset_section_blocks()

        # super_section final por √≠tem
        if kind == "movie":
            super_for_item = "Nuevas Peliculas"
        else:
            if current_super and (("episod" in safe_lower(current_super)) or ("capit" in safe_lower(current_super))):
                super_for_item = "Nuevos Episodios"
            else:
                super_for_item = "Nuevas Series"

        # secci√≥n actual (nunca episodios para pel√≠culas)
        section_for_item = (current_section or "Novedades").strip()
        if kind == "movie" and "episod" in safe_lower(section_for_item):
            section_for_item = "Novedades"

        # Asegura section_group (sin duplicar headings)
        _ensure_section_group()

        title_clean = clean_title_for_search(raw)
        meta["title_clean"] = title_clean
        meta["bucket"] = bucket_for_item
        meta["section"] = section_for_item
        meta["section_group"] = current_section_group
        meta["date_label"] = (current_date_label or None)
        meta["super_section"] = super_for_item
        meta["order"] = order_counter
        order_counter += 1

        ai_score, ai_confidence = ai_score_media_item(meta)
        meta["ai_score"] = ai_score
        meta["ai_confidence"] = ai_confidence

        art = get_media_art_zuplo(title_clean, "tv" if kind == "series" else "movie")

        poster = (art.get("poster") or art.get("img") or art.get("url") or art.get("backdrop") or art.get("thumb") or "")
        if art:
            meta["art"] = art

        t = raw.strip()
        if len(t) > 80:
            t = t[:80]

        item_id = sha_id(kind.upper(), t, section_for_item or "", bucket_for_item or "")
        if item_id in seen_ids:
            continue
        seen_ids.add(item_id)

        u = f"{WEBAPP_URL}#item={item_id}"
        cat = "üé¨ PEL√çCULAS" if kind == "movie" else "üì∫ SERIES"

        items.append({
            "_id": item_id,
            "t": t,
            "u": u,
            "cat": cat,
            "img": poster or "",
            "meta": meta
        })
        section_has_items = True

        # Si el heading ven√≠a pegado al final del t√≠tulo, lo aplicamos para siguientes l√≠neas
        if pending_heading:
            _bump_section(pending_heading)

    return items

def sincronizar_una_vez():
    html = fetch_html()
    blocks = html_to_text_blocks(html)
    sections = build_sections_from_blocks(blocks)

    sports_items = parse_sports_items(sections["deportes_blocks"])
    vs_items = parse_movies_series_items(sections["tail_blocks"])

    final_items = sports_items + vs_items

    if not final_items:
        log.warning("‚ö†Ô∏è Parser no extrajo items. Se deja cache anterior.")
        return

    cache_col.update_one(
        {"_id": "cache"},
        {"$set": {"items": final_items, "ts": datetime.now(timezone.utc)}},
        upsert=True
    )
    log.info(f"‚úÖ Cache actualizada: deportes={len(sports_items)} pelis/series={len(vs_items)} total={len(final_items)}")


def sincronizar_loop():
    while True:
        try:
            sincronizar_una_vez()
        except Exception as e:
            log.error(f"Sync error: {e}")
        time.sleep(SYNC_SECONDS)


# ---------------- TELEGRAM ----------------
def test_telegram_token():
    if not bot:
        log.error("[TOKEN_CHECK] bot no inicializado (token vac√≠o)")
        return False
    try:
        me = bot.get_me()
        log.warning(f"[TOKEN_CHECK] get_me OK -> id={me.id} username=@{me.username}")
        return True
    except Exception as e:
        log.error(f"[TOKEN_CHECK] get_me FAILED -> {e}")
        return False


def user_is_vip(user_id: int) -> bool:
    u = users_col.find_one({"_id": user_id}) or {}
    return bool(u.get("premium")) or user_id == ADMIN_ID


def ensure_user(user):
    if not users_col.find_one({"_id": user.id}):
        users_col.insert_one({"_id": user.id, "premium": False, "nombre": user.first_name})


def vip_request_button():
    kb = telebot.types.InlineKeyboardMarkup()
    kb.add(telebot.types.InlineKeyboardButton("üì© Solicitar VIP", callback_data="vip_req"))
    return kb


def notify_admin_vip_request(user_id: int, name: str):
    if not ADMIN_ID:
        return
    kb = telebot.types.InlineKeyboardMarkup()
    kb.add(
        telebot.types.InlineKeyboardButton("‚úÖ Aprobar", callback_data=f"vip_ok:{user_id}"),
        telebot.types.InlineKeyboardButton("‚ùå Rechazar", callback_data=f"vip_no:{user_id}")
    )
    bot.send_message(
        ADMIN_ID,
        f"üì© **Solicitud VIP**\nüë§ {name}\nüÜî `{user_id}`\n\n¬øAprobar?",
        parse_mode="Markdown",
        reply_markup=kb
    )


def _md_escape(s: str) -> str:
    # Para Markdown "classic": escapamos solo lo que suele romper.
    if s is None:
        return ""
    s = str(s)
    return s.replace("_", "\\_").replace("*", "\\*").replace("`", "\\`")


def send_long_message(chat_id: int, text: str, parse_mode: str | None = None, reply_markup=None):
    """Telegram limita el tama√±o del mensaje. Esta funci√≥n lo parte por l√≠neas sin romper Markdown."""
    if not bot:
        return
    if text is None:
        return
    text = str(text)
    if not text.strip():
        return

    max_len = 3800 if (parse_mode and parse_mode.lower().startswith("markdown")) else 4000
    lines = text.split("\n")
    chunks: list[str] = []
    buf = ""
    for line in lines:
        candidate = (buf + ("\n" if buf else "") + line)
        if len(candidate) > max_len:
            if buf:
                chunks.append(buf)
                buf = line
            else:
                # l√≠nea demasiado grande, cortamos duro
                chunks.append(line[:max_len])
                buf = line[max_len:]
        else:
            buf = candidate
    if buf:
        chunks.append(buf)

    for i, ch in enumerate(chunks):
        try:
            bot.send_message(chat_id, ch, parse_mode=parse_mode, reply_markup=(reply_markup if i == 0 else None))
        except Exception as e:
            # Fallback: si Markdown falla (entidades inv√°lidas), reintenta sin parse_mode
            try:
                bot.send_message(chat_id, ch, reply_markup=(reply_markup if i == 0 else None))
            except Exception as e2:
                log.warning(f"send_message failed: {e} | fallback failed: {e2}")




def build_sports_chat_message(items: list[dict]) -> str:
    """Formatea la secci√≥n DEPORTES con el patr√≥n solicitado (Telegram HTML).

    Reglas clave:
      - Titular por competencia en negrita.
      - Separador s√≥lido grueso (una sola l√≠nea) y un salto de l√≠nea.
      - Instancias (Jornada/Fase/Ronda/IDA/Vuelta/etc.) en negrita.
      - Rivales en negrita (üì£).
      - La palabra "Horario" en negrita.
      - No inventar competencias; solo mostrar lo scrapeado.
    """

    if not items:
        return ""

    # ------------------------------------------------------------
    # Limpieza defensiva (DEPORTES)
    #
    # En Wix es com√∫n que un encabezado de competencia (p.ej. "LPGA Tourüáπüá≠‚õ≥Ô∏è")
    # se cuele como si fuera un "t√≠tulo de evento" dentro de otra competencia
    # (p.ej. dentro de PGA Tour). Eso provoca que se vea duplicado:
    #   üì£Ô∏é LPGA Tourüáπüá≠‚õ≥
    #   LPGA Tour‚õ≥
    #
    # Regla: si un item NO tiene meta √∫til (hora/canales/sede/global/instancia)
    # y su t√≠tulo coincide con el nombre de una competencia conocida,
    # se descarta como "header fantasma".
    # ------------------------------------------------------------
    def _fold_key(s: str) -> str:
        s2 = safe_lower(strip_weird_symbols_keep_plus(s or ""))
        try:
            s2 = unicodedata.normalize("NFD", s2)
            s2 = "".join(ch for ch in s2 if unicodedata.category(ch) != "Mn")
        except Exception:
            pass
        s2 = re.sub(r"[^a-z0-9 ]+", " ", s2)
        s2 = re.sub(r"\s+", " ", s2).strip()
        return s2

    def _is_empty_meta(meta: dict) -> bool:
        if not meta:
            return True
        keys = [
            # horas
            "hora_text", "hora", "hora_original", "start_utc",
            # TV
            "canales", "canal", "categoria",
            # contexto
            "sede", "global", "jornada", "ronda",
        ]
        for k in keys:
            v = meta.get(k)
            if isinstance(v, list) and v:
                return False
            if isinstance(v, str) and v.strip():
                return False
        return True

    _known_comp_titles_fold: set[str] = set()

    def _h(s: str) -> str:
        return html.escape(str(s), quote=False)

    # L√≠nea s√≥lida (m√°s corta para que no se ‚Äúdoble‚Äù en Telegram)
    # L√≠nea s√≥lida gruesa: mantenla corta para evitar que Telegram la parta en 2 l√≠neas.
    THICK_DIVIDER = "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"

    # Meg√°fono en variante "texto" (monocromo) para evitar el emoji dorado en algunas UIs.
    MEGAPHONE_LIGHT = "üì£\uFE0E"


    # Nombres can√≥nicos -> t√≠tulo de presentaci√≥n (incluye emojis donde aplica)
    comp_display = {
        "Juegos Ol√≠mpicos de Invierno": "Juegos Ol√≠mpicos de InviernoüáÆüáπ‚õ∑Ô∏è",
        "Juegos Ol√≠mpicos de Invierno (Italia 2026)": "Juegos Ol√≠mpicos de InviernoüáÆüáπ‚õ∑Ô∏è",
        "Copa de Campeones CONCACAF": "Copa de Campeones CONCACAF‚öΩ",
        "CONCACAF Champions Cup": "Copa de Campeones CONCACAF‚öΩ",
        "UEFA Champions League": "UEFA Champions Leagueüá™üá∫‚öΩÔ∏è",
        "Partidos CIBACOPA": "Partidos CIBACOPAüá≤üáΩüèÄ",
        "LPGA Tour": "LPGA Tourüáπüá≠‚õ≥Ô∏è",
        "PGA Tour": "PGA Tourüá∫üá∏‚õ≥Ô∏è",
        "DP World Tour": "DP World Tour‚õ≥Ô∏è",
        "Premier League": "Premier Leagueüá¨üáß‚öΩÔ∏è",
        "Liga Escocesa": "Liga Escocesaüè¥‚öΩÔ∏è",
        "Liga de Escocia": "Liga Escocesaüè¥‚öΩÔ∏è",  # compat

        "Copa Alemana": "Copa Alemanaüá©üá™‚öΩÔ∏è",
        "Copa de Alemania": "Copa de Alemaniaüá©üá™‚öΩÔ∏è",
        "Copa de Italia": "Copa de ItaliaüáÆüáπ‚öΩÔ∏è",
        "Copa IT": "Copa de ItaliaüáÆüáπ‚öΩÔ∏è",
        "Copa Libertadores": "Copa Libertadores‚öΩÔ∏è",
        "Liga MX Femenil": "Liga Mx Femenilüá≤üáΩ‚öΩÔ∏è",
        "Liga MX": "Liga MXüá≤üáΩ‚öΩÔ∏è",
        "Liga de Campeones AFC": "Liga de Campeones de la AFC‚öΩ",
        "Liga de Campeones de la AFC": "Liga de Campeones de la AFC‚öΩ",
        "Copa AFC": "Copa AFC‚öΩ",
        "Eliminatorias Sub-17 CONCACAF": "Eliminatorias Sub-17 CONCACAF‚öΩ",
        "Segunda Liga": "Liga de Segunda Divisi√≥n de Portugalüáµüáπ‚öΩÔ∏è",
        # En algunas rutas el parser deja este canonical alternativo (sin acento) ‚Äî mapear ambos.
        "Liga de Segunda Division de Portugal": "Liga de Segunda Divisi√≥n de Portugalüáµüáπ‚öΩÔ∏è",
        "Liga de Segunda Divisi√≥n de Portugal": "Liga de Segunda Divisi√≥n de Portugalüáµüáπ‚öΩÔ∏è",
        "Primeira Liga": "Primeira Ligaüáµüáπ‚öΩÔ∏è",
        "Liga Expansi√≥n MX": "Liga Expansi√≥n MXüá≤üáΩ‚öΩÔ∏è",
        "Serie B": "Serie BüáÆüáπ‚öΩ",
        "Copa del Rey": "Copa del Reyüá™üá∏‚öΩÔ∏è",
        "UEFA Champions League Femenil": "UEFA Champions League Femenilüá™üá∫‚öΩÔ∏è",
        "UEFA Womens Europa Cup": "UEFA Womens Europa Cupüá™üá∫‚öΩÔ∏è",
        "Copa Argentina": "Copa Argentinaüá¶üá∑‚öΩÔ∏è",
        "Torneo de Reserva Argentina": "Torneo de Reservaüá¶üá∑‚öΩÔ∏è",
        "Primera A (Colombia)": "Primera A Colombiaüá®üá¥‚öΩÔ∏è",
        "Primera Divisi√≥n Honduras": "Primera Divisi√≥n Hondurasüá≠üá≥‚öΩ",
        "Liga Nacional Honduras": "Liga Nacional Hondurasüá≠üá≥‚öΩÔ∏è",
        "Partidos NBA": "Partidos NBAüá∫üá∏üèÄ",
        "Unrivaled Basketball": "Unrivaled Basketballüá∫üá∏üèÄ",
        "NCAA Basketball": "NCAA Basketballüá∫üá∏üèÄ",
        "NCAA Womens Basketball": "NCAA Womens Basketballüá∫üá∏üèÄ",
        "NCAA Womens Gymnastics": "NCAA Womens Gymnasticsüá∫üá∏ü§∏‚Äç‚ôÇÔ∏è",
        "WWE Wrestling": "WWE Wrestlingüá∫üá∏ü§º‚Äç‚ôÇÔ∏è",        "FA Cup": "FA Cupüá¨üáß‚öΩÔ∏è",
        "Championship": "Championshipüá¨üáß‚öΩÔ∏è",
        "FA Womens Cup": "FA Womens Cupüá¨üáß‚öΩÔ∏è",
        "Torneo de las 6 Naciones": "Torneo de las 6 Nacionesüèâ",
        "UFC Fight Night": "UFC Fight Nightüá∫üá∏ü§º‚Äç‚ôÇÔ∏è",
        "Partidos MLS": "Partidos MLSüá∫üá∏üá®üá¶‚öΩ",
        "Pretemporada MLB": "Pretemporada MLBüá∫üá∏‚öæ",
        "Womens Super League": "Womens Super Leagueüá¨üáß‚öΩÔ∏è",
        # League One = tercera divisi√≥n de Inglaterra (EFL)
        "League One": "League Oneüá¨üáß‚öΩÔ∏è",
        "A-League": "A-Leagueüá¶üá∫‚öΩ",
        "A-League Women": "A-League Womenüá¶üá∫‚öΩ",

        "Ekstraklasa": "Ekstraklasaüáµüá±‚öΩÔ∏è",
        "LaLiga": "LaLigaüá™üá∏‚öΩÔ∏è",
        "Serie A": "Serie AüáÆüáπ‚öΩÔ∏è",
        "Ligue 1": "Ligue 1üá´üá∑‚öΩÔ∏è",
        "Super Lig": "Super Ligüáπüá∑‚öΩÔ∏è",
        "Super Liga Argentina": "Super Liga Argentinaüá¶üá∑‚öΩÔ∏è",
        "Liga Profesional": "Liga Profesionalüá¶üá∑‚öΩÔ∏è",
        "Saudi Pro League": "Pro Leagueüá∏üá¶‚öΩÔ∏è",
        "La Liga Femenil": "La Liga Femenilüá™üá∏‚öΩÔ∏è",
        "Bundesliga 2": "Bundesliga 2üá©üá™‚öΩÔ∏è",
        "Bundesliga": "Bundesligaüá©üá™‚öΩÔ∏è",
        "NASCAR Cup Series": "NASCAR Cup Seriesüá∫üá∏üèéÔ∏è",
        "NASCAR Craftsman Truck Series": "NASCAR Craftsman Truck Seriesüá∫üá∏üèéÔ∏è",
        "NASCAR Auto Parts Series": "NASCAR Auto Parts Seriesüá∫üá∏üèéÔ∏è",
        "Segunda Divisi√≥n": "Segunda Divisionüá™üá∏‚öΩ",
        "NCAA Softball": "NCAA Softballüá∫üá∏ü•é",
        "Liga Mexicana de Softbol": "Liga Mexicana de Softbolüá≤üáΩü•é",
        "Major Arena Soccer League": "Major Arena Soccer Leagueüá∫üá∏‚öΩ",
        "Rugby Seis Naciones": "Rugby Seis Nacionesüèâ",
        "All Elite Wrestling": "All Elite Wrestlingüá¶üá∫ü§º‚Äç‚ôÇÔ∏è",
        "PGA Tour Golf": "PGA Tour Golfüá∫üá∏‚õ≥Ô∏è",
        "Campeonato Paulista": "Campeonato Paulistaüáßüá∑‚öΩ",
        "Eredivisie": "Eredivisieüá≥üá±‚öΩÔ∏è",
        "Primera A Colombia": "Primera A Colombiaüá®üá¥‚öΩÔ∏è",
        "Primera A Ecuador": "Liga de Ecuadorüá™üá®‚öΩÔ∏è",
        "Primera Divisi√≥n Per√∫": "Primera Divisi√≥nüáµüá™‚öΩÔ∏è",
        "Primera Divisi√≥n Costa Rica": "Primera Divisionüá®üá∑‚öΩÔ∏è",
        "Primera Divisi√≥n Chile": "Primera Divisi√≥nüá®üá±‚öΩÔ∏è",
        "Evento BOX (Alemania)": "Evento BOXüá©üá™ü•ä",
        "Evento BOX (M√©xico)": "Evento BOXüá≤üáΩü•ä",
        "Evento BOX (Estados Unidos)": "Evento BOXüá∫üá∏ü•ä",
        "Evento BOX (Reino Unido)": "Evento BOX üè¥ü•ä",
        "Evento BOX": "Evento BOXü•ä",

    }

    # Completa set de competencias conocidas para descartar headers fantasma.
    for k, v in comp_display.items():
        _known_comp_titles_fold.add(_fold_key(k))
        _known_comp_titles_fold.add(_fold_key(v))

    # Canonicaliza nombres de competencia para evitar duplicados por variantes
    # (p.ej. "LPGA Tour‚õ≥" vs "LPGA Tour").
    _CANON_COMP_BY_FOLD = {
        "lpga tour": "LPGA Tour",
        "pga tour": "PGA Tour",
        "dp world tour": "DP World Tour",
        "ufc fight night": "UFC Fight Night",
        "partidos mls": "Partidos MLS",
        "pretemporada mlb": "Pretemporada MLB",
        "serie b": "Serie B",
        "championship": "Championship",
        "league one": "League One",
    }

    def _canon_comp_name(s: str) -> str:
        fk = _fold_key(s or "")
        return _CANON_COMP_BY_FOLD.get(fk, (s or "").strip())

    cleaned_items: list[dict] = []
    for it in items:
        t = (it.get("t") or "").strip()
        meta = it.get("meta") or {}
        # Canonicaliza competencia en el meta (evita llaves distintas para la misma liga)
        if meta.get("competition"):
            meta["competition"] = _canon_comp_name(str(meta.get("competition")))
            it["meta"] = meta
        # No filtrar partidos reales
        if re.search(r"(?i)\bvs\b", t):
            cleaned_items.append(it)
            continue
        # Caso cr√≠tico: LPGA se ha colado como "evento" dentro de PGA.
        # Si el t√≠tulo es LPGA Tour pero la competencia NO es LPGA Tour, es un header fantasma.
        if re.search(r"(?i)\blpga\s*tour\b", t):
            if safe_lower(str(meta.get("competition") or "")) != "lpga tour":
                continue
        # Header fantasma: t√≠tulo == competencia conocida y sin meta √∫til
        if t and _fold_key(t) in _known_comp_titles_fold and _is_empty_meta(meta):
            continue
        # Filtro extra: LPGA Tour a veces se cuela como "evento" sin meta dentro de otra gira.
        if re.search(r"(?i)\blpga\s*tour\b", t) and _is_empty_meta(meta):
            continue
        cleaned_items.append(it)

    items = cleaned_items
    # Emojis por deporte: si el titular NO trae uno representativo, se a√±ade autom√°ticamente.
    SPORT_EMOJIS = ["‚öΩ", "üèÄ", "üèà", "‚öæ", "ü•é", "‚õ≥", "üéæ", "ü•ä", "üèí", "ü§º", "‚õ∑", "üèÖ", "üèé", "üèê", "üèè", "üé≥", "üèä", "üö¥", "üèÉ"]
    SPORT_EMOJI_BY_KIND = {
        "soccer": "‚öΩÔ∏è",
        "basketball": "üèÄ",
        "american_football": "üèà",
        "baseball": "‚öæÔ∏è",
        "softball": "ü•é",
        "golf": "‚õ≥",
        "tennis": "üéæ",
        "combat": "ü•ä",
        "hockey": "üèí",
    }

    def _has_sport_emoji(s: str) -> bool:
        s = s or ""
        return any(e in s for e in SPORT_EMOJIS)

    def _ensure_sport_emoji(title: str, sport_kind: str | None) -> str:
        if not title:
            return title
        if _has_sport_emoji(title):
            return title
        ek = SPORT_EMOJI_BY_KIND.get(str(sport_kind or "").strip().lower())
        if not ek:
            # fallback por texto
            ek = SPORT_EMOJI_BY_KIND.get(sport_kind_from_text(safe_lower(title)))
        return f"{title}{ek}" if ek else title

    def _has_flag_emoji(s: str) -> bool:
        if not s:
            return False
        if "üè¥" in s:
            return True
        for ch in s:
            o = ord(ch)
            if 0x1F1E6 <= o <= 0x1F1FF:
                return True
        return False

    # Asegura bandera para ligas/torneos claramente nacionales.
    # (Si ya existe una bandera, no se toca.)
    _FLAG_BY_COMP = {
        "Primeira Liga": "üáµüáπ",
        "Segunda Liga": "üáµüáπ",
        "Liga de Segunda Division de Portugal": "üáµüáπ",
        "Liga de Segunda Divisi√≥n de Portugal": "üáµüáπ",
        "Liga MX": "üá≤üáΩ",
        "Liga MX Femenil": "üá≤üáΩ",
        "LaLiga": "üá™üá∏",
        "Serie A": "üáÆüáπ",
        "Ligue 1": "üá´üá∑",
        "Bundesliga": "üá©üá™",
        "Bundesliga 2": "üá©üá™",
        "Ekstraklasa": "üáµüá±",
        "Primera A Ecuador": "üá™üá®",
        "Primera A Colombia": "üá®üá¥",
        "Liga Nacional Honduras": "üá≠üá≥",
        "Primera Divisi√≥n Honduras": "üá≠üá≥",
        # Inglaterra (EFL)
        "Championship": "üá¨üáß",
        "League One": "üá¨üáß",
        "FA Womens Cup": "üá¨üáß",
    }

    def _ensure_flag(title: str, comp_key: str) -> str:
        if not title:
            return title
        if _has_flag_emoji(title):
            return title
        flag = _FLAG_BY_COMP.get(comp_key)
        if not flag:
            # fallback por texto
            low = safe_lower(title)
            if "portugal" in low:
                flag = "üáµüáπ"
            elif "ecuador" in low:
                flag = "üá™üá®"
            elif "honduras" in low:
                flag = "üá≠üá≥"
            elif "mex" in low or "m√©x" in low:
                flag = "üá≤üáΩ"
            elif "espa" in low or "spain" in low:
                flag = "üá™üá∏"
            elif "ital" in low:
                flag = "üáÆüáπ"
            elif "fran" in low:
                flag = "üá´üá∑"
            elif "alem" in low or "german" in low:
                flag = "üá©üá™"
            elif "arg" in low:
                flag = "üá¶üá∑"
            elif "colombia" in low or "col" in low:
                flag = "üá®üá¥"
            elif "peru" in low or "per√∫" in low:
                flag = "üáµüá™"
            elif "chile" in low:
                flag = "üá®üá±"
        return f"{title}{flag}" if flag else title


    def _stage_text(meta: dict) -> str | None:
        # Prioridad: jornada -> ronda
        jornada = (meta.get("jornada") or "").strip()
        ronda = (meta.get("ronda") or "").strip()

        if jornada:
            return jornada

        if ronda:
            # WWE: convertir promo a texto
            if "wwe" in safe_lower(meta.get("competition","")) and ronda.upper() in {"NXT", "RAW", "SMACKDOWN", "DYNAMITE"}:
                return f"Promoci√≥n {ronda.title()}"
            return ronda
        return None

    # Agrupa por competencia preservando orden de aparici√≥n
    grouped: dict[str, list[dict]] = {}
    order: list[str] = []
    for it in items:
        m = it.get("meta") or {}
        comp = _canon_comp_name((m.get("competition") or it.get("cat") or "").strip())
        if not comp:
            continue
        if comp not in grouped:
            grouped[comp] = []
            order.append(comp)
        grouped[comp].append(it)

    if not grouped:
        return ""

    out: list[str] = ["‚ú®‚ú®<b>DEPORTES</b>‚ú®‚ú®", ""]

    for comp in order:
        comp_items = grouped[comp]

        # ------------------------------------------------------------
        # FIX: algunos encabezados de competencia se cuelan como si fueran
        # un "evento" dentro de la MISMA competencia.
        # Ejemplo real reportado:
        #   LPGA Tourüáπüá≠‚õ≥Ô∏è  (titular)
        #   üì£Ô∏é LPGA Tourüáπüá≠‚õ≥  (primer item)
        #
        # Regla: si el t√≠tulo del item es el mismo nombre de la competencia
        # (ignorando emojis/acentos), se descarta. Si al descartarlo el bloque
        # queda vac√≠o, se conserva el original (por seguridad).
        # ------------------------------------------------------------
        _comp_fold = _fold_key(comp)
        _disp_fold = _fold_key(comp_display.get(comp, comp))
        _filtered_comp_items: list[dict] = []
        for _it in comp_items:
            _t = str((_it.get("t") or "")).strip()
            if _t:
                _tf = _fold_key(_t)
                if _tf and (_tf == _comp_fold or _tf == _disp_fold):
                    continue
            _filtered_comp_items.append(_it)
        if _filtered_comp_items:
            comp_items = _filtered_comp_items
        base_disp = comp_display.get(comp, comp)
        base_disp = _ensure_flag(base_disp, comp)
        sk = ((comp_items[0].get("meta") or {}).get("sport_kind") if comp_items else None)
        disp = _ensure_sport_emoji(base_disp, sk)

        out.append(f"<b>{_h(disp)}</b>")
        out.append(f"<b>{THICK_DIVIDER}</b>")
        out.append("")
        # Si todo el bloque comparte una sola sede, impr√≠mela una sola vez justo despu√©s del titular.
        # Aplica a BOX y UFC (en UFC toda la cartelera comparte la sede).
        is_box_group = safe_lower(comp).startswith("evento box")
        is_ufc_group = (comp == "UFC Fight Night")
        group_sede: str | None = None
        if is_box_group or is_ufc_group:
            sedes = []
            for _it in comp_items:
                _m = _it.get("meta") or {}
                _s = str(_m.get("sede") or "").strip()
                if _s:
                    sedes.append(_s)
            if sedes:
                uniq = {safe_lower(s) for s in sedes}
                if len(uniq) == 1:
                    group_sede = sedes[0].strip()

        if group_sede:
            out.append(f"üìç {_h(group_sede)}")
            out.append("")

        # Si casi todos comparten la misma instancia (ej. Jornada#26), la mostramos una sola vez
        stage_counts: dict[str, int] = {}
        stages_for_items: list[str | None] = []
        for it in comp_items:
            st = _stage_text(it.get("meta") or {})
            stages_for_items.append(st)
            if st:
                stage_counts[st] = stage_counts.get(st, 0) + 1

        group_stage: str | None = None
        if stage_counts:
            top_stage, top_cnt = max(stage_counts.items(), key=lambda kv: kv[1])
            if top_cnt >= max(2, int(len(comp_items) * 0.70)):
                group_stage = top_stage

        if group_stage:
            out.append(f"üóìÔ∏è <b>{_h(group_stage)}</b>")
            out.append("")

        ufc_prev_group: str | None = None

        for it, st in zip(comp_items, stages_for_items):
            meta = it.get("meta") or {}


            # UFC: imprime sub-secciones (Preliminares / Principales / Estelar)
            if comp == "UFC Fight Night":
                ug = (meta.get("ufc_group") or "").strip()
                if ug and (ug != ufc_prev_group):
                    out.append(f"ü•ä <b>{_h(ug)}</b>")
                    ufc_prev_group = ug
            if st and (st != group_stage):
                out.append(f"üóìÔ∏è <b>{_h(st)}</b>")

            sede = (meta.get("sede") or "").strip()
            if sede and (not group_sede):
                out.append(f"üìç {_h(sede)}")

            gtxt = (meta.get("global") or "").strip()
            if gtxt:
                out.append(f"üß© {_h(gtxt)}")

            title = (it.get("t") or "").strip()


            # UFC: si el t√≠tulo coincide con el encabezado de sub-secci√≥n (ya impreso), no repetirlo.
            if comp == "UFC Fight Night":
                ug = (meta.get("ufc_group") or "").strip()
                if ug and title and safe_lower(title) == safe_lower(ug):
                    title = ""
            # Evita l√≠neas redundantes en WWE (el promo ya se muestra como instancia).
            if "wwe" in safe_lower(comp):
                promo_m = re.search(r"(?i)\b(nxt|raw|smackdown|dynamite)\b", title)
                if promo_m:
                    # 1) "NXT a partir de..." o similares
                    if re.search(r"(?i)\ba\s+partir\b", title):
                        title = ""
                    # 2) Si el t√≠tulo es solo el promo (p.ej. "Dynamite") no lo repitas
                    elif not re.search(r"(?i)\bvs\b", title):
                        title = ""

            # WWE: NO tratar plataformas/servicios como "rivales" (ej. 'Smackdown vs Netflix').
            # Si el segundo "equipo" es una plataforma, dejamos solo el nombre del evento/promoci√≥n.
            if "wwe" in safe_lower(comp) and title:
                ta_plat, tb_plat = _extract_match_teams(title)
                if ta_plat and tb_plat:
                    tb_low = safe_lower(tb_plat)
                    PLATFORM_OPPONENTS = {
                        "netflix", "peacock", "hulu", "max", "hbo max", "hbomax",
                        "prime", "prime video", "amazon", "amazon prime", "disney", "disney+", "disney plus",
                        "apple tv", "apple tv+", "paramount", "paramount+", "youtube", "tubi"
                    }
                    if tb_low in PLATFORM_OPPONENTS:
                        title = ta_plat.strip()


            # Evita repetir el nombre del titular como si fuera un evento
            # (caso reportado: LPGA Tour aparece como primer item dentro de LPGA Tour).
            if title:
                _tf = _fold_key(title)
                if _tf and (_tf == _comp_fold or _tf == _disp_fold):
                    title = ""
            if title:
                out.append(f"{MEGAPHONE_LIGHT} <b>{_h(title)}</b>")

            hora = (meta.get("hora_text") or meta.get("hora_original") or "").strip()
            if hora:
                live = bool(meta.get("is_live_now"))
                prefix = "üî¥ " if live else ""
                out.append(f"{prefix}üïí <b>Horario:</b> {_h(hora)}")

            canales = meta.get("canales") or []
            if isinstance(canales, str):
                canales = [canales]
            canales = [c for c in canales if str(c).strip()]
            categoria = (meta.get("categoria") or "").strip()

            # Normaliza canales/categor√≠a para evitar duplicados tipo "y y por Categor√≠a ..."
            def _clean_tv(canales_list, cat):
                clean = []
                cat = (cat or "").strip()
                for c in (canales_list or []):
                    s = str(c).strip()
                    if not s:
                        continue
                    s = re.sub(r"\s+", " ", s)
                    if re.fullmatch(r"(?i)y", s):
                        continue
                    mcat = re.match(r"(?i)^(?:y\s+)?(?:por\s+)?categor[i√≠]a\s*(.+)$", s)
                    if mcat:
                        cand = (mcat.group(1) or "").strip()
                        if cand and (not cat):
                            cat = cand
                        continue
                    clean.append(s)
                # dedupe preservando orden
                seen = set()
                uniq = []
                for s in clean:
                    k = s.lower()
                    if k in seen:
                        continue
                    seen.add(k)
                    uniq.append(s)
                return uniq, cat

            canales, categoria = _clean_tv(canales, categoria)
            tv_line = ", ".join(map(str, canales)).strip()
            if categoria:
                # Evita a√±adir Categor√≠a si ya viene incluida en los canales
                if not re.search(r"(?i)\bcategor[i√≠]a\s+" + re.escape(categoria) + r"\b", tv_line):
                    tv_line = re.sub(r"(?i)\s+y\s*$", "", tv_line).strip()
                    tv_line = (tv_line + (" y por Categor√≠a " if tv_line else "Por Categor√≠a ") + categoria).strip()
            tv_line = re.sub(r"(?i)\by\s+y\b", "y", tv_line)
            if tv_line:
                out.append(f"üì∫: {_h(tv_line)}")

            out.append("")

    return "\n".join(out).strip()


def build_media_chat_message(items: list[dict], want: str) -> str:
    """
    Formato (Telegram, Markdown):

    üé¨ *PEL√çCULAS*
    üóìÔ∏è *FECHA: MIERCOLES 18 DE FEBRERO DEL 2026*

    üé• *NUEVAS PELICULAS*

    ‚ú® *Estrenos...*
    üçø ...

    üé¨ *SERIES*
    üóìÔ∏è *FECHA: ...*

    üé• *NUEVAS SERIES*
    ‚ú® *Comedia...*
    üçø ...

    üé• *NUEVOS EPISODIOS*
    ‚ú® *Anime...*
    üçø ...

    Reglas clave:
    - El encabezado de *NUEVAS PELICULAS* va DENTRO de cada fecha (igual para series).
    - Se muestran las 2 fechas m√°s recientes (MAX_MEDIA_DATES, default=2).
    - Preserva el orden del feed y permite repetir secciones usando meta.section_group.
    """
    is_movies = (want == "peliculas")

    MONTHS = ["ENERO","FEBRERO","MARZO","ABRIL","MAYO","JUNIO","JULIO","AGOSTO","SEPTIEMBRE","SETIEMBRE","OCTUBRE","NOVIEMBRE","DICIEMBRE"]
    DOW = ["LUNES","MARTES","MIERCOLES","MI√âRCOLES","JUEVES","VIERNES","SABADO","S√ÅBADO","DOMINGO"]

    def _extract_dmy(lbl: str):
        """Devuelve (yyyy, mm, dd) si puede, si no None.
        Soporta:
          - 18 DE FEBRERO DE 2026
          - 18 DE FEBRERO DEL 2026
          - MIERCOLES 18 DE FEBRERO DEL 2026
          - FECHA: MIERCOLES 18 DE FEBRERO DEL 2026
        """
        up = (lbl or "").strip().upper()
        up = re.sub(r"^\s*FECHA\s*:\s*", "", up).strip()
        up = re.sub(r"\s{2,}", " ", up)
        up = re.sub(r"[^0-9A-Z√Å√â√ç√ì√ö√ë\s]+$", "", up).strip()
        up = re.sub(r"\s{2,}", " ", up)
        up = re.sub(r"[^0-9A-Z√Å√â√ç√ì√ö√ë\s]+$", "", up).strip()
        up = re.sub(r"\s{2,}", " ", up)

        # Quita d√≠a de la semana si viene
        m = re.match(rf"^({'|'.join(DOW)})\s+(.+)$", up)
        if m:
            up = m.group(2).strip()

        m2 = re.match(r"^(\d{1,2})\s+DE\s+([A-Z√Å√â√ç√ì√ö√ë]+)\s+(?:DE|DEL)\s+(\d{4})$", up)
        if not m2:
            return None
        dd = int(m2.group(1))
        mes = m2.group(2).upper().replace("SETIEMBRE", "SEPTIEMBRE")
        yyyy = int(m2.group(3))
        mm = (MONTHS.index(mes) + 1) if mes in MONTHS else 0
        if mm <= 0:
            return None
        return (yyyy, mm, dd)

    def _date_key(lbl: str):
        dmy = _extract_dmy(lbl)
        if not dmy:
            return (9999, 99, 99, (lbl or ""))
        yyyy, mm, dd = dmy
        return (-yyyy, -mm, -dd, (lbl or ""))

    def _norm_sec(sec: str) -> str:
        s = (sec or "Novedades").strip()
        s = re.sub(r"[.¬∑‚Ä¢‚Ä¶]+$", "", s).strip()
        s = s.strip(":").strip()
        return s or "Novedades"

    def _get_order(it, fallback: int) -> int:
        m = it.get("meta", {}) or {}
        try:
            return int(m.get("order"))
        except Exception:
            return fallback

    def _pick_dates(arr_items: list[dict]) -> list[str]:
        all_dates = []
        seen = set()
        for it in arr_items:
            d = ((it.get("meta", {}) or {}).get("date_label") or "").strip().upper()
            if not d:
                continue
            if d not in seen:
                seen.add(d)
                all_dates.append(d)
        all_dates.sort(key=_date_key)
        keep_n = max(1, int(MAX_MEDIA_DATES or 2))
        return all_dates[:keep_n]

    if is_movies:
        out = ["üé¨ *PEL√çCULAS*", ""]

        filtered = [it for it in items if (it.get("meta", {}) or {}).get("type") == "movie" or "PEL" in (it.get("cat", "") or "")]
        if not filtered:
            return "\n".join(out + ["‚ö†Ô∏è No hay pel√≠culas a√∫n."]).strip()

        dates_keep = _pick_dates(filtered)
        if not dates_keep:
            dates_keep = [""]

        by_date: dict[str, list[tuple[int, dict]]] = {}
        for i, it in enumerate(filtered):
            meta = it.get("meta", {}) or {}
            d = (meta.get("date_label") or "").strip().upper()
            if dates_keep and d not in dates_keep:
                continue
            by_date.setdefault(d, []).append((i, it))

        dates_order = [d for d in dates_keep if d in by_date]
        dates_order.sort(key=_date_key)

        wrote_any = False
        for date_label in dates_order:
            arr = by_date.get(date_label, [])
            arr.sort(key=lambda pair: _get_order(pair[1], pair[0]))

            if wrote_any:
                out.append("")
            if date_label:
                out.append(f"üóìÔ∏è *FECHA: {_md_escape(date_label)}*")
                out.append("")
            out.append("üé• *NUEVAS PELICULAS*")
            out.append("")

            # Agrupa por g√©nero sin repetir encabezados.
            # Mantiene el orden de aparici√≥n del g√©nero en el feed (por meta.order).
            sec_order: list[str] = []
            sec_map: dict[str, list[dict]] = {}
            for _, it in arr:
                meta = it.get("meta", {}) or {}
                sec = _norm_sec(meta.get("section") or "Novedades")
                if sec not in sec_map:
                    sec_map[sec] = []
                    sec_order.append(sec)
                sec_map[sec].append(it)

            for idx, sec in enumerate(sec_order):
                if idx > 0:
                    out.append("")
                out.append(f"‚ú® *{_md_escape(sec)}...*")
                for it in sec_map.get(sec, []):
                    out.append(f"üçø {_md_escape(it.get('t',''))}")

            wrote_any = True

        out.append("")
        return "\n".join(out).strip()

    # SERIES
    out = ["üé¨ *SERIES*", ""]
    filtered = [it for it in items if (it.get("meta", {}) or {}).get("type") == "series" or "SERIES" in (it.get("cat", "") or "")]
    if not filtered:
        return "\n".join(out + ["‚ö†Ô∏è No hay series a√∫n."]).strip()

    dates_keep = _pick_dates(filtered)
    if not dates_keep:
        dates_keep = [""]

    by_date: dict[str, dict[str, list[tuple[int, dict]]]] = {}
    for i, it in enumerate(filtered):
        meta = it.get("meta", {}) or {}
        date_label = (meta.get("date_label") or "").strip().upper()
        if dates_keep and date_label not in dates_keep:
            continue
        super_section = (meta.get("super_section") or "Nuevas Series").strip().upper()
        super_section = "NUEVOS EPISODIOS" if ("EPISOD" in super_section or "CAPIT" in super_section) else "NUEVAS SERIES"
        by_date.setdefault(date_label, {}).setdefault(super_section, []).append((i, it))

    dates = [d for d in dates_keep if d in by_date]
    dates.sort(key=_date_key)

    for date_label in dates:
        if date_label:
            out.append(f"üóìÔ∏è *FECHA: {_md_escape(date_label)}*")
            out.append("")
        day = by_date.get(date_label, {})

        for super_section in ["NUEVAS SERIES", "NUEVOS EPISODIOS"]:
            if super_section not in day:
                continue

            out.append(f"üé• *{_md_escape(super_section)}*")
            out.append("")

            arr = day.get(super_section, [])
            arr.sort(key=lambda pair: _get_order(pair[1], pair[0]))

            # Agrupa por g√©nero sin repetir encabezados.
            sec_order: list[str] = []
            sec_map: dict[str, list[dict]] = {}
            for _, it in arr:
                meta = it.get("meta", {}) or {}
                sec = _norm_sec(meta.get("section") or "Novedades")
                if sec not in sec_map:
                    sec_map[sec] = []
                    sec_order.append(sec)
                sec_map[sec].append(it)

            for idx, sec in enumerate(sec_order):
                if idx > 0:
                    out.append("")
                out.append(f"‚ú® *{_md_escape(sec)}...*")
                for it in sec_map.get(sec, []):
                    out.append(f"üçø {_md_escape(it.get('t',''))}")

            out.append("")

        out.append("")

    return "\n".join(out).strip()

if bot:
    @bot.message_handler(commands=["start"])
    def cmd_start(m):
        ensure_user(m.from_user)
        log.warning(f"[TG] /start uid={m.from_user.id} chat={m.chat.id}")

        kb = telebot.types.ReplyKeyboardMarkup(resize_keyboard=True)
        kb.add("‚öΩ DEPORTES", "üé¨ PEL√çCULAS", "üì∫ SERIES")
        kb.add("üèÜ WEBPREMIUM", "üë§ MI ESTADO", "üìò MANUAL")

        if m.from_user.id == ADMIN_ID:
            kb.add("üì¢ DIFUSI√ìN", "üßæ PENDIENTES VIP")

        msg_md = """üëã Bienvenido a *BCNTV Premium*.

Usa los botones para ver categor√≠as filtradas.
üèÜ *WEBPREMIUM* es la WebApp visual.
"""
        msg_plain = """üëã Bienvenido a BCNTV Premium.

Usa los botones para ver categor√≠as filtradas.
üèÜ WEBPREMIUM es la WebApp visual.
"""

        try:
            bot.send_message(m.chat.id, msg_md, reply_markup=kb, parse_mode="Markdown")
        except Exception as e:
            log.error(f"[TG] /start send_message failed: {e}")
            bot.send_message(m.chat.id, msg_plain, reply_markup=kb)

    @bot.message_handler(func=lambda m: _normalize_btn_text(m.text) in (_normalize_btn_text("üìò MANUAL"), "manual"))
    def manual_usuario(m):
        log.warning(f"[TG] btn_manual uid={m.from_user.id} chat={m.chat.id} text={(m.text or '')!r}")
        bot.send_message(
            m.chat.id,
            "üìñ *Manual*\n\n"
            "‚öΩ DEPORTES / üé¨ PEL√çCULAS / üì∫ SERIES: muestran listas filtradas.\n"
            "üèÜ WEBPREMIUM: abre la WebApp con mejor interfaz.\n"
            "‚≠ê En la WebApp puedes guardar favoritos.\n"
            "‚è∞ En deportes de la WebApp puedes descargar alarmas (.ics) 5/10/15 min antes.\n",
            parse_mode="Markdown"
        )

    @bot.message_handler(func=lambda m: _normalize_btn_text(m.text) in (_normalize_btn_text("üë§ MI ESTADO"), "mi estado", "estado"))
    def mi_estado(m):
        log.warning(f"[TG] btn_estado uid={m.from_user.id} chat={m.chat.id} text={(m.text or '')!r}")
        vip = "üåü VIP" if user_is_vip(m.from_user.id) else "üÜì Gratis"
        bot.send_message(m.chat.id, f"üë§ *Tu Perfil*\nüÜî `{m.from_user.id}`\nüíé {vip}", parse_mode="Markdown")

    @bot.message_handler(func=lambda m: _normalize_btn_text(m.text) in (_normalize_btn_text("üßæ PENDIENTES VIP"), "pendientes vip") and m.from_user.id == ADMIN_ID)
    def pendientes_vip(m):
        pend = list(vip_req_col.find({"status": "pending"}).sort("ts", -1).limit(25))
        if not pend:
            bot.send_message(m.chat.id, "‚úÖ No hay solicitudes pendientes.")
            return
        lines = ["üßæ *Pendientes VIP*"]
        for r in pend:
            lines.append(f"‚Ä¢ `{r.get('_id')}` ‚Äî {r.get('name','(sin nombre)')}")
        bot.send_message(m.chat.id, "\n".join(lines), parse_mode="Markdown")

    @bot.message_handler(commands=["invitar"])
    def admin_invitar(m):
        if m.from_user.id != ADMIN_ID:
            return
        parts = (m.text or "").split()
        if len(parts) < 2:
            bot.send_message(m.chat.id, "Uso: /invitar <user_id>")
            return
        try:
            uid = int(parts[1])
            users_col.update_one({"_id": uid}, {"$set": {"premium": True}}, upsert=True)
            vip_req_col.update_one({"_id": uid}, {"$set": {"status": "approved", "ts": datetime.now(timezone.utc)}}, upsert=True)
            bot.send_message(m.chat.id, f"‚úÖ VIP activado para `{uid}`", parse_mode="Markdown")
            try:
                bot.send_message(uid, "‚úÖ Tu acceso VIP fue activado. Ya puedes usar Deportes/Pel√≠culas/Series y WebPremium.")
            except Exception as e:
                log.warning(f"No pude notificar al usuario {uid}: {e}")
        except Exception:
            bot.send_message(m.chat.id, "‚ùå user_id inv√°lido.")

    @bot.message_handler(func=lambda m: _normalize_btn_text(m.text) in (_normalize_btn_text("üèÜ WEBPREMIUM"), "webpremium", "web premium", "webapp"))
    def webpremium(m):
        ensure_user(m.from_user)
        log.warning(f"[TG] btn_webpremium uid={m.from_user.id} chat={m.chat.id} text={(m.text or '')!r}")
        if user_is_vip(m.from_user.id):
            url = WEBAPP_URL + "?vip=1"
            if PUBLIC_URL:
                sep = "&" if "?" in url else "?"
                url += f"{sep}api={quote_plus(PUBLIC_URL)}"
            kb = telebot.types.InlineKeyboardMarkup().add(
                telebot.types.InlineKeyboardButton("üì∫ ABRIR WEBAPP", web_app=telebot.types.WebAppInfo(url))
            )
            bot.send_message(m.chat.id, "‚úÖ Acceso concedido.", reply_markup=kb)
        else:
            bot.send_message(m.chat.id, "üîí Zona VIP. Solicita acceso al administrador.", reply_markup=vip_request_button())

    @bot.callback_query_handler(func=lambda c: True)
    def callbacks(c):
        try:
            if c.data == "vip_req":
                user = c.from_user
                ensure_user(user)
                uid = user.id
                name = user.first_name or "Usuario"
                vip_req_col.update_one(
                    {"_id": uid},
                    {"$set": {"status": "pending", "name": name, "ts": datetime.now(timezone.utc)}},
                    upsert=True
                )
                bot.answer_callback_query(c.id, "Solicitud enviada ‚úÖ")
                bot.send_message(uid, "üì© Solicitud enviada. El admin la revisar√° pronto.")
                notify_admin_vip_request(uid, name)
                return

            if c.data.startswith("vip_ok:") and c.from_user.id == ADMIN_ID:
                uid = int(c.data.split(":", 1)[1])
                users_col.update_one({"_id": uid}, {"$set": {"premium": True}}, upsert=True)
                vip_req_col.update_one({"_id": uid}, {"$set": {"status": "approved", "ts": datetime.now(timezone.utc)}}, upsert=True)
                bot.answer_callback_query(c.id, "Aprobado ‚úÖ")
                bot.send_message(ADMIN_ID, f"‚úÖ VIP aprobado para `{uid}`", parse_mode="Markdown")
                try:
                    bot.send_message(uid, "‚úÖ Tu acceso VIP fue aprobado. Ya puedes usar Deportes/Pel√≠culas/Series y WebPremium.")
                except Exception as e:
                    log.warning(f"No pude notificar al usuario {uid}: {e}")
                return

            if c.data.startswith("vip_no:") and c.from_user.id == ADMIN_ID:
                uid = int(c.data.split(":", 1)[1])
                vip_req_col.update_one({"_id": uid}, {"$set": {"status": "rejected", "ts": datetime.now(timezone.utc)}}, upsert=True)
                bot.answer_callback_query(c.id, "Rechazado ‚ùå")
                bot.send_message(ADMIN_ID, f"‚ùå VIP rechazado para `{uid}`", parse_mode="Markdown")
                try:
                    bot.send_message(uid, "‚ùå Tu solicitud VIP fue rechazada. Puedes contactar al admin si fue un error.")
                except Exception as e:
                    log.warning(f"No pude notificar al usuario {uid}: {e}")
                return

        except Exception as e:
            log.warning(f"Callback error: {e}")

    def send_category_list(chat_id: int, user_id: int, want: str):
        d = cache_col.find_one({"_id": "cache"}) or {}
        items = d.get("items", []) if isinstance(d.get("items"), list) else []

        if want == "deportes":
            filtered = [x for x in items if (x.get("meta", {}).get("type") == "sport") or ("DEPORT" in (x.get("cat", "")))]
        elif want == "peliculas":
            filtered = [x for x in items if x.get("meta", {}).get("type") == "movie" or "PEL" in x.get("cat", "")]
        else:
            filtered = [x for x in items if x.get("meta", {}).get("type") == "series" or "SERIES" in x.get("cat", "")]

        if not filtered:
            bot.send_message(chat_id, "‚ö†Ô∏è No hay datos a√∫n. Intenta en unos minutos.")
            return

        if want == "deportes":
            msg = build_sports_chat_message(filtered)
            send_long_message(chat_id, msg, parse_mode="HTML")
            return

        msg = build_media_chat_message(filtered, want)
        send_long_message(chat_id, msg, parse_mode="Markdown")

    @bot.message_handler(func=lambda m: _normalize_btn_text(m.text) in (_normalize_btn_text("‚öΩ DEPORTES"), "deportes"))
    def deportes(m):
        ensure_user(m.from_user)
        log.warning(f"[TG] btn_deportes uid={m.from_user.id} chat={m.chat.id} text={(m.text or '')!r}")
        if not user_is_vip(m.from_user.id):
            bot.send_message(m.chat.id, "üîí Zona VIP. Solicita acceso al admin.", reply_markup=vip_request_button())
            return
        send_category_list(m.chat.id, m.from_user.id, "deportes")

    @bot.message_handler(func=lambda m: _normalize_btn_text(m.text) in (_normalize_btn_text("üé¨ PEL√çCULAS"), "peliculas", "pel√≠culas"))
    def peliculas(m):
        ensure_user(m.from_user)
        log.warning(f"[TG] btn_peliculas uid={m.from_user.id} chat={m.chat.id} text={(m.text or '')!r}")
        if not user_is_vip(m.from_user.id):
            bot.send_message(m.chat.id, "üîí Zona VIP. Solicita acceso al admin.", reply_markup=vip_request_button())
            return
        send_category_list(m.chat.id, m.from_user.id, "peliculas")

    @bot.message_handler(func=lambda m: _normalize_btn_text(m.text) in (_normalize_btn_text("üì∫ SERIES"), "series"))
    def series(m):
        ensure_user(m.from_user)
        log.warning(f"[TG] btn_series uid={m.from_user.id} chat={m.chat.id} text={(m.text or '')!r}")
        if not user_is_vip(m.from_user.id):
            bot.send_message(m.chat.id, "üîí Zona VIP. Solicita acceso al admin.", reply_markup=vip_request_button())
            return
        send_category_list(m.chat.id, m.from_user.id, "series")

    @bot.message_handler(func=lambda m: _normalize_btn_text(m.text) == _normalize_btn_text("üì¢ DIFUSI√ìN") and m.from_user.id == ADMIN_ID)
    def difusion_help(m):
        bot.send_message(
            m.chat.id,
            "üì¢ *Difusi√≥n (Admin)*\n\n"
            "Usa:\n"
            "‚Ä¢ /broadcast_all <mensaje>\n"
            "‚Ä¢ /broadcast_vip <mensaje>\n"
            "‚Ä¢ /broadcast_user <id> <mensaje>\n",
            parse_mode="Markdown"
        )

    @bot.message_handler(commands=["broadcast_all"])
    def broadcast_all(m):
        if m.from_user.id != ADMIN_ID:
            return
        msg = (m.text or "").split(" ", 1)
        if len(msg) < 2:
            bot.send_message(m.chat.id, "Uso: /broadcast_all <mensaje>")
            return
        text = msg[1]
        enviados = 0
        fallidos = 0
        for u in users_col.find({}, {"_id": 1}):
            try:
                bot.send_message(int(u["_id"]), text)
                enviados += 1
            except Exception:
                fallidos += 1
            if BROADCAST_SLEEP > 0:
                time.sleep(BROADCAST_SLEEP)
        bot.send_message(m.chat.id, f"‚úÖ Enviado a todos. OK: {enviados} Fallidos: {fallidos}")

    @bot.message_handler(commands=["broadcast_vip"])
    def broadcast_vip(m):
        if m.from_user.id != ADMIN_ID:
            return
        msg = (m.text or "").split(" ", 1)
        if len(msg) < 2:
            bot.send_message(m.chat.id, "Uso: /broadcast_vip <mensaje>")
            return
        text = msg[1]
        enviados = 0
        fallidos = 0
        for u in users_col.find({"premium": True}, {"_id": 1}):
            try:
                bot.send_message(int(u["_id"]), text)
                enviados += 1
            except Exception:
                fallidos += 1
            if BROADCAST_SLEEP > 0:
                time.sleep(BROADCAST_SLEEP)
        bot.send_message(m.chat.id, f"‚úÖ Enviado a VIPs. OK: {enviados} Fallidos: {fallidos}")

    @bot.message_handler(commands=["broadcast_user"])
    def broadcast_user(m):
        if m.from_user.id != ADMIN_ID:
            return
        parts = (m.text or "").split(" ", 2)
        if len(parts) < 3:
            bot.send_message(m.chat.id, "Uso: /broadcast_user <id> <mensaje>")
            return
        try:
            uid = int(parts[1])
            text = parts[2]
            bot.send_message(uid, text)
            bot.send_message(m.chat.id, "‚úÖ Enviado.")
        except Exception:
            bot.send_message(m.chat.id, "‚ùå Error enviando.")


# ---------------- API ----------------
@app.route("/")
def root():
    return "OK", 200


@app.route("/api/health")
def health():
    return jsonify({"ok": True, "ts": datetime.now(timezone.utc).isoformat()}), 200


@app.route("/api/links")
def api_links():
    if API_KEY and API_PROTECT_LINKS:
        auth = request.headers.get("x-api-key", "").strip()
        if auth != API_KEY:
            return jsonify({"ok": False, "error": "unauthorized"}), 401
    d = cache_col.find_one({"_id": "cache"}) or {}
    items = d.get("items", [])
    if not isinstance(items, list):
        items = []

    # Back-compat: normaliza _id y meta.type para que la WebPremium pueda filtrar/agruparear.
    for it in items:
        try:
            if isinstance(it, dict) and '_id' not in it and 'id' in it:
                it['_id'] = str(it.get('id'))

            m = it.get("meta", {}) if isinstance(it, dict) else {}
            if not isinstance(m, dict):
                m = {}

            t = (m.get("type") or "").strip().lower()
            if not t:
                cat = str(it.get("cat") or "")
                cat_u = cat.upper()

                # deportes por se√±ales fuertes
                if any(k in cat_u for k in ["DEPORT", "LALIGA", "LIGA MX", "NBA", "NCAA", "COMBATE", "TENIS", "GOLF"]):
                    t = "sport"
                elif any(k in cat_u for k in ["PEL", "üé¨"]):
                    t = "movie"
                elif any(k in cat_u for k in ["SERIE", "üì∫"]):
                    t = "series"
                else:
                    # se√±ales por campos
                    if any(m.get(k) for k in ["competition", "start_utc", "hora_text", "hora_original", "categoria"]):
                        t = "sport"
                    elif m.get("se"):
                        t = "series"
                    else:
                        # default conservador para VOD
                        t = "movie"

                m["type"] = t
                it["meta"] = m
        except Exception:
            continue

    # Refrescar banderas "En Vivo" al vuelo (sin depender del √∫ltimo sync)
    now_utc = datetime.now(timezone.utc)
    for it in items:
        try:
            # Compatibilidad: items antiguos pod√≠an traer 'id' en vez de '_id'
            if isinstance(it, dict) and '_id' not in it and 'id' in it:
                it['_id'] = str(it.get('id'))
            m = it.get("meta", {}) or {}
            if m.get("type") != "sport":
                continue

            start_iso = m.get("start_utc")
            start_utc = None
            if start_iso:
                try:
                    start_utc = datetime.fromisoformat(start_iso)
                    if start_utc.tzinfo is None:
                        start_utc = start_utc.replace(tzinfo=timezone.utc)
                except Exception:
                    start_utc = None

            duration_min = int(m.get("duration_min") or LIVE_DURATION_NORMAL_MIN)
            live = bool(is_live(now_utc, start_utc, duration_min)) if start_utc else False
            m["live"] = live
            m["is_live_now"] = live
            it["meta"] = m
        except Exception:
            continue
    return jsonify(items), 200



@app.route("/api/poster")
def api_poster():
    """Devuelve artes (poster/backdrop/logo/thumb) para una pel√≠cula/serie desde Zuplo.

    Par√°metros:
      - title: t√≠tulo limpio (idealmente meta.title_clean)
      - type: movie | tv | series
    """
    # Protecci√≥n opcional
    if API_KEY and API_PROTECT_LINKS:
        auth = request.headers.get("x-api-key", "").strip()
        if auth != API_KEY:
            return jsonify({"ok": False, "error": "unauthorized"}), 401

    try:
        title = (request.args.get("title") or "").strip()
        if not title:
            return jsonify({"ok": False, "error": "missing_title"}), 400

        t = (request.args.get("type") or "movie").strip().lower()
        if t in ("series", "tv"):
            t = "tv"
        else:
            t = "movie"

        title_clean = clean_title_for_search(title) or title

        # Nunca dejamos que un fallo de Zuplo tumbe el endpoint
        art = {}
        try:
            art = get_media_art_zuplo(title_clean, t) or {}
        except Exception as e:
            log.warning(f"get_media_art_zuplo failed: {e}")
            art = {}

        # URL estable para <img> (no requiere headers). El endpoint proxy hace la consulta a Zuplo.
        poster_img = f"{request.url_root.rstrip('/')}/api/poster_img?title={quote_plus(title_clean)}&type={t}"

        payload = {"ok": True, "title_clean": title_clean, "type": t, "poster_img": poster_img, **art}

        # Elegimos el mejor candidato (prioridad: poster_img expl√≠cito, poster, img, url, backdrop)
        poster_val = str(payload.get("poster_img") or payload.get("poster") or payload.get("img") or payload.get("url") or payload.get("backdrop") or "").strip()

        # Si Zuplo no devolvi√≥ un poster claramente utilizable (vac√≠o, dominio base, logo, o no parece imagen),
        # forzamos a usar el proxy /api/poster_img (que NO requiere headers desde el navegador).
        unusable = (
            (not poster_val)
            or (POSTER_API_URL and poster_val.rstrip("/") == POSTER_API_URL.rstrip("/"))
            or is_logo_url(poster_val)
            or (not re.search(r"\.(jpg|jpeg|png|webp|gif|bmp|svg)(\?|#|$)", poster_val, re.I) and ("/api/poster_img" not in poster_val))
        )
        if unusable:
            poster_val = poster_img

        # Compatibilidad: muchos frontends leen .poster, .img o .url
        payload["poster"] = poster_val
        payload["img"] = poster_val
        payload["url"] = poster_val

        return jsonify(payload), 200

    except Exception as e:
        # No devolvemos 500 para que la WebApp no se quede sin imagen; devolvemos fallback estable.
        log.exception("api_poster crashed")
        title_raw = (request.args.get("title") or "").strip() or "Unknown"
        t_raw = (request.args.get("type") or "movie").strip().lower()
        t = "tv" if t_raw in ("series", "tv") else "movie"
        title_clean = clean_title_for_search(title_raw) or title_raw
        poster_img = f"{request.url_root.rstrip('/')}/api/poster_img?title={quote_plus(title_clean)}&type={t}"
        return jsonify({
            "ok": False,
            "error": "poster_failed",
            "detail": (str(e) or "unknown")[:200],
            "title_clean": title_clean,
            "type": t,
            "poster_img": poster_img,
            "poster": poster_img,
            "img": poster_img,
            "url": poster_img,
        }), 200


@app.route("/api/poster_img")

def api_poster_img():
    """Proxy de imagen de p√≥ster desde Zuplo para poder usarlo en <img src="..."> sin headers.

    Devuelve SIEMPRE una imagen:
      - Si encuentra un poster real (Zuplo/TMDB), lo retransmite.
      - Si no, devuelve un placeholder SVG (para evitar 404 en la WebApp).

    Par√°metros:
      - title: t√≠tulo (limpio o no)
      - type: movie | tv | series
    """
    try:
            title = (request.args.get("title") or "").strip()
            if not title:
                return Response(status=400)
        
            t = (request.args.get("type") or "movie").strip().lower()
            if t in ("series", "tv"):
                t = "tv"
                label = "SERIE"
                emoji = "üì∫"
            else:
                t = "movie"
                label = "PEL√çCULA"
                emoji = "üé¨"
        
            title_clean = clean_title_for_search(title) or title
        
            base = (POSTER_API_URL or "").rstrip("/")
            q = f"title={quote_plus(title_clean)}&type={t}"
        
        
            candidates = []
            if base:
                q_variants = [
                    f"title={quote_plus(title_clean)}&type={t}",
                    f"q={quote_plus(title_clean)}&type={t}",
                    f"query={quote_plus(title_clean)}&type={t}",
                    f"search={quote_plus(title_clean)}&type={t}",
                    f"name={quote_plus(title_clean)}&type={t}",
                ]
                type_variants = [t] + (["tv", "series"] if t == "tv" else ["movie", "film"])
                q_variants += [f"title={quote_plus(title_clean)}&type={tv}" for tv in type_variants]
                q_variants += [f"q={quote_plus(title_clean)}&type={tv}" for tv in type_variants]
        
                path_variants = [
                    "/poster_img", "/posterimg", "/poster-image",
                    "/poster", "/posters", "/image", "/img", "/search", "/tmdb/poster", "/tmdb/search",
                    "/api/poster", "/v1/poster"
                ]
        
                for pth in path_variants:
                    for qv in q_variants:
                        candidates.append(f"{base}{pth}?{qv}")
                for qv in q_variants:
                    candidates.append(f"{base}?{qv}")
        
        
            headers = _poster_auth_headers()
        
            def _return_image(resp: requests.Response):
                ct = (resp.headers.get("content-type") or "").split(";")[0].strip().lower()
                if not ct.startswith("image/"):
                    return None
                out = Response(resp.content, status=200, mimetype=ct)
                out.headers["Cache-Control"] = "public, max-age=86400"
                return out
        
            def _tmdb_img_from_path(p: str, kind: str = "poster") -> str:
                if not p or not isinstance(p, str):
                    return ""
                p = p.strip()
                if not p.startswith("/"):
                    return ""
                size = "w500" if kind == "poster" else "w780"
                return f"https://image.tmdb.org/t/p/{size}{p}"
        
            def _deep_find_first_path(obj):
                if isinstance(obj, dict):
                    for k in ("poster_path", "posterPath"):
                        v = obj.get(k)
                        if isinstance(v, str) and v.strip().startswith("/"):
                            return ("poster", v.strip())
                    for k in ("backdrop_path", "backdropPath"):
                        v = obj.get(k)
                        if isinstance(v, str) and v.strip().startswith("/"):
                            return ("backdrop", v.strip())
                    for k in ("results", "data", "items"):
                        found = _deep_find_first_path(obj.get(k))
                        if found:
                            return found
                    for v in obj.values():
                        found = _deep_find_first_path(v)
                        if found:
                            return found
                elif isinstance(obj, list):
                    for it in obj:
                        found = _deep_find_first_path(it)
                        if found:
                            return found
                return None
        
            def _pick_image_url(d) -> str:
                """Extrae una referencia de imagen (URL o ruta) desde JSON de Zuplo.

                Soporta:
                  - URLs absolutas http(s)
                  - rutas relativas (se unen con POSTER_API_URL)
                  - TMDB poster_path/backdrop_path
                  - data:image/*
                """
                if not isinstance(d, (dict, list)):
                    return ""

                # Muchos gateways envuelven en {data:{...}}
                if isinstance(d, dict) and isinstance(d.get("data"), dict):
                    d = d.get("data") or d

                def _looks_img(s: str) -> bool:
                    s = str(s or "").strip()
                    if not s:
                        return False
                    if s.startswith("data:image/"):
                        return True
                    if "image.tmdb.org/t/p/" in s:
                        return True
                    if re.search(r"\.(jpg|jpeg|png|webp|gif|bmp|svg)(\?|#|$)", s, re.I):
                        return True
                    return False

                def _norm(s: str, key_hint: str = "") -> str:
                    s = str(s or "").strip()
                    if not s:
                        return ""
                    # data uri
                    if s.startswith("data:image/"):
                        return s
                    # esquema relativo
                    if s.startswith("//"):
                        s = "https:" + s
                    # URL absoluta
                    if s.startswith("http://") or s.startswith("https://"):
                        if base and s.rstrip("/") == base.rstrip("/"):
                            return ""
                        return s
                    # TMDB paths
                    kh = key_hint.lower()
                    if kh.endswith("_path") or "poster_path" in kh or "backdrop_path" in kh:
                        if s.startswith("/"):
                            return _tmdb_img_from_path(s, "poster")
                        if re.search(r"\.(jpg|jpeg|png|webp)$", s, re.I):
                            return _tmdb_img_from_path("/" + s.lstrip("/"), "poster")
                    # ruta relativa al gateway
                    if s.startswith("/"):
                        return (base + s) if base else ""
                    # ruta relativa sin slash pero parece imagen
                    if _looks_img(s) and base:
                        return base + "/" + s.lstrip("/")
                    return ""

                # Prioridad por keys t√≠picas
                if isinstance(d, dict):
                    key_order = (
                        "poster_img", "posterImage", "poster_image",
                        "poster", "poster_url", "posterUrl",
                        "img", "image", "image_url", "imageUrl",
                        "url", "file", "file_url", "src",
                        "backdrop", "backdrop_url",
                        "thumb", "thumbnail", "cover", "cover_url",
                        "poster_path", "backdrop_path",
                        "path", "href",
                    )
                    for k in key_order:
                        if k not in d or d.get(k) is None:
                            continue
                        v = d.get(k)
                        if isinstance(v, str):
                            u = _norm(v, k)
                            if u:
                                return u

                        # dict/list con url absoluta dentro
                        u_abs = _zuplo_pick_first_url(v)
                        if isinstance(u_abs, str) and u_abs:
                            u2 = _norm(u_abs, k)
                            if u2:
                                return u2

                        # b√∫squeda recursiva
                        u3 = _pick_image_url(v)
                        if u3:
                            return u3

                # poster_path/backdrop_path en cualquier profundidad
                found = _deep_find_first_path(d)
                if found:
                    kind, path = found
                    if kind == "poster":
                        return _tmdb_img_from_path(path, "poster")
                    return _tmdb_img_from_path(path, "backdrop")

                # Escaneo profundo: cualquier string que parezca imagen o key sugerente
                def _scan(obj, key_hint=""):
                    if isinstance(obj, dict):
                        for kk, vv in obj.items():
                            r = _scan(vv, str(kk))
                            if r:
                                return r
                    elif isinstance(obj, list):
                        for it2 in obj:
                            r = _scan(it2, key_hint)
                            if r:
                                return r
                    elif isinstance(obj, str):
                        if _looks_img(obj) or ("poster" in key_hint.lower()) or ("image" in key_hint.lower()):
                            return _norm(obj, key_hint)
                    return ""

                return _scan(d) or ""

            # 0) Intento r√°pido usando el extractor 'art'
            try:
                art = get_media_art_zuplo(title_clean, t)
                u0 = str(art.get("poster") or art.get("img") or art.get("url") or "").strip()
                if u0 and (not (POSTER_API_URL and u0.rstrip("/") == POSTER_API_URL.rstrip("/"))):
                    try:
                        r0 = SESSION.get(u0, headers=headers, timeout=POSTER_TIMEOUT, allow_redirects=True)
                        if r0.status_code < 400:
                            img0 = _return_image(r0)
                            if img0 is not None:
                                return img0
                    except Exception:
                        pass
            except Exception:
                pass
        
            # 1) Intentar que Zuplo ya devuelva la imagen o un JSON con una URL/path
            for u in candidates:
                try:
                    r = SESSION.get(u, headers=headers, timeout=POSTER_TIMEOUT, allow_redirects=True)
                    if r.status_code >= 400:
                        continue
        
                    img_resp = _return_image(r)
                    if img_resp is not None:
                        return img_resp
        
                    ct = (r.headers.get("content-type") or "")
                    if "json" in (ct or "").lower():
                        try:
                            j = r.json() or {}
                        except Exception:
                            j = {}
        
                        img_url = _pick_image_url(j)
                        if img_url and (not (POSTER_API_URL and img_url.rstrip("/") == POSTER_API_URL.rstrip("/"))):
                            try:
                                r2 = SESSION.get(img_url, headers=headers, timeout=POSTER_TIMEOUT, allow_redirects=True)
                                if r2.status_code < 400:
                                    img2 = _return_image(r2)
                                    if img2 is not None:
                                        return img2
                            except Exception:
                                pass
                except Exception:
                    continue
        

            # 1b) Algunos gateways usan POST (no GET) para b√∫squedas. Probamos POST con payload JSON.
            if base:
                post_targets = [
                    f"{base}/poster", f"{base}/posters", f"{base}/search",
                    f"{base}/tmdb/search", f"{base}/api/poster", f"{base}/v1/poster",
                    f"{base}/poster_img", f"{base}/poster-image",
                    f"{base}",
                ]
                post_payloads = [
                    {"title": title_clean, "type": t},
                    {"q": title_clean, "type": t},
                    {"query": title_clean, "type": t},
                    {"search": title_clean, "type": t},
                    {"name": title_clean, "type": t},
                ]
                post_headers = dict(headers)
                post_headers["Content-Type"] = "application/json"
                for target in post_targets:
                    for payload0 in post_payloads:
                        try:
                            r = SESSION.post(target, headers=post_headers, json=payload0, timeout=POSTER_TIMEOUT, allow_redirects=True)
                            if r.status_code >= 400:
                                continue

                            img_resp = _return_image(r)
                            if img_resp is not None:
                                return img_resp

                            ct = (r.headers.get("content-type") or "")
                            if "json" in ct.lower():
                                try:
                                    j = r.json() or {}
                                except Exception:
                                    j = {}
                                img_url = _pick_image_url(j)
                                if img_url and (not (POSTER_API_URL and img_url.rstrip("/") == POSTER_API_URL.rstrip("/"))):
                                    try:
                                        r2 = SESSION.get(img_url, headers=headers, timeout=POSTER_TIMEOUT, allow_redirects=True)
                                        if r2.status_code < 400:
                                            img2 = _return_image(r2)
                                            if img2 is not None:
                                                return img2
                                    except Exception:
                                        pass
                        except Exception:
                            continue

            # 2) Placeholder SVG (evita 404 en el frontend)
            safe_title = html.escape(title_clean[:60])
            svg = f"""<svg xmlns='http://www.w3.org/2000/svg' width='400' height='600' viewBox='0 0 400 600'>
              <defs>
                <linearGradient id='g' x1='0' x2='0' y1='0' y2='1'>
                  <stop offset='0' stop-color='#1b1b2e'/>
                  <stop offset='1' stop-color='#0f0f18'/>
                </linearGradient>
              </defs>
              <rect width='400' height='600' fill='url(#g)'/>
              <text x='200' y='250' text-anchor='middle' font-size='72' font-family='Arial, sans-serif' fill='#ffffff'>{emoji}</text>
              <text x='200' y='315' text-anchor='middle' font-size='26' font-family='Arial, sans-serif' fill='#ffffff'>{label}</text>
              <text x='200' y='370' text-anchor='middle' font-size='18' font-family='Arial, sans-serif' fill='#b7b7c6'>{safe_title}</text>
            </svg>"""
            out = Response(svg, status=200, mimetype="image/svg+xml")
            out.headers["Cache-Control"] = "public, max-age=60"
            return out
        
        
    except Exception as e:
        log.exception("api_poster_img crashed")
        title_f = (request.args.get("title") or "").strip() or "Unknown"
        t_raw = (request.args.get("type") or "movie").strip().lower()
        if t_raw in ("series", "tv"):
            label = "SERIE"
            emoji = "üì∫"
        else:
            label = "PEL√çCULA"
            emoji = "üé¨"
        safe_title = html.escape(title_f[:60])
        svg = f"""<svg xmlns='http://www.w3.org/2000/svg' width='400' height='600' viewBox='0 0 400 600'>
  <defs>
    <linearGradient id='g' x1='0' x2='0' y1='0' y2='1'>
      <stop offset='0' stop-color='#1b1b2e'/>
      <stop offset='1' stop-color='#0f0f18'/>
    </linearGradient>
  </defs>
  <rect width='400' height='600' fill='url(#g)'/>
  <text x='200' y='250' text-anchor='middle' font-size='72' font-family='Arial, sans-serif' fill='#ffffff'>{emoji}</text>
  <text x='200' y='315' text-anchor='middle' font-size='26' font-family='Arial, sans-serif' fill='#ffffff'>{label}</text>
  <text x='200' y='370' text-anchor='middle' font-size='18' font-family='Arial, sans-serif' fill='#b7b7c6'>{safe_title}</text>
</svg>"""
        out = Response(svg, status=200, mimetype="image/svg+xml")
        out.headers["Cache-Control"] = "public, max-age=300"
        return out
@app.route("/api/ics/<item_id>")
def api_ics(item_id: str):
    """
    Descarga un recordatorio ICS con alarma (VALARM) X minutos antes.
    Usado por la WebPremium (bot√≥n ‚è∞ 5/10/15 min).
    """
    if API_KEY and API_PROTECT_LINKS:
        auth = request.headers.get("x-api-key", "").strip()
        if auth != API_KEY:
            return jsonify({"ok": False, "error": "unauthorized"}), 401

    mins_txt = (request.args.get("mins") or "10").strip()
    try:
        mins = int(mins_txt)
    except Exception:
        mins = 10
    if mins < 1 or mins > 240:
        mins = 10

    d = cache_col.find_one({"_id": "cache"}) or {}
    items = d.get("items", [])
    if not isinstance(items, list):
        items = []

    it = None
    for x in items:
        if str(x.get("_id") or x.get("id")) == str(item_id):
            it = x
            break
    if not it:
        return jsonify({"ok": False, "error": "not_found"}), 404

    m = it.get("meta", {}) or {}
    if m.get("type") != "sport":
        return jsonify({"ok": False, "error": "not_sport"}), 400

    start_iso = m.get("start_utc")
    if not start_iso:
        return jsonify({"ok": False, "error": "no_start_time"}), 400

    try:
        start_utc = datetime.fromisoformat(start_iso)
        if start_utc.tzinfo is None:
            start_utc = start_utc.replace(tzinfo=timezone.utc)
        start_utc = start_utc.astimezone(timezone.utc)
    except Exception:
        return jsonify({"ok": False, "error": "bad_start_time"}), 400

    duration_min = int(m.get("duration_min") or LIVE_DURATION_NORMAL_MIN)
    end_utc = start_utc + timedelta(minutes=duration_min)

    def _ics_escape(s: str) -> str:
        s = (s or "").replace("\\", "\\\\")
        s = s.replace("\r", "").replace("\n", "\\n")
        s = s.replace(",", "\\,").replace(";", "\\;")
        return s

    def _ics_dt(dt: datetime) -> str:
        # Formato UTC: YYYYMMDDTHHMMSSZ
        dtu = dt.astimezone(timezone.utc)
        return dtu.strftime("%Y%m%dT%H%M%SZ")

    title = it.get("t") or "BCNTV"
    comp = m.get("competition") or "Deportes"
    sede = m.get("sede") or ""

    canales = m.get("canales") or []
    canal_one = m.get("canal")
    if canal_one and canal_one not in canales:
        canales = [canal_one] + list(canales)
    canales = _reorder_channels_for_display([str(c) for c in canales if c])

    web_url = f"{WEBAPP_URL}#item={item_id}"
    desc_lines = [
        f"{title}",
        f"Competici√≥n: {comp}",
    ]
    if sede:
        desc_lines.append(f"Sede: {sede}")
    if canales:
        desc_lines.append("Canales: " + ", ".join(canales[:25]))
    desc_lines.append("Web: " + web_url)

    desc = "\n".join(desc_lines)

    uid = f"bcntv-{item_id}-{mins}m@bcntv"
    dtstamp = _ics_dt(datetime.now(timezone.utc))

    lines = [
        "BEGIN:VCALENDAR",
        "VERSION:2.0",
        "PRODID:-//BCNTV//WebPremium//ES",
        "CALSCALE:GREGORIAN",
        "METHOD:PUBLISH",
        "BEGIN:VEVENT",
        f"UID:{_ics_escape(uid)}",
        f"DTSTAMP:{dtstamp}",
        f"DTSTART:{_ics_dt(start_utc)}",
        f"DTEND:{_ics_dt(end_utc)}",
        f"SUMMARY:{_ics_escape(title)}",
        (f"LOCATION:{_ics_escape(sede)}" if sede else None),
        f"DESCRIPTION:{_ics_escape(desc)}",
        f"URL:{_ics_escape(web_url)}",
        "BEGIN:VALARM",
        f"TRIGGER:-PT{mins}M",
        "ACTION:DISPLAY",
        f"DESCRIPTION:{_ics_escape('Recordatorio: ' + title)}",
        "END:VALARM",
        "END:VEVENT",
        "END:VCALENDAR",
        "",
    ]
    ics = "\r\n".join([x for x in lines if x])

    filename = f"bcntv_{item_id}_{mins}m.ics"
    return Response(
        ics,
        mimetype="text/calendar",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'}
    )

@app.route("/api/poster_diag")
def api_poster_diag():
    """Diagn√≥stico r√°pido de la integraci√≥n de posters (Zuplo).
    NO expone la API key, solo indica si existe y qu√© devuelve cada endpoint candidato.

    A√±ade snippet de respuesta para identificar r√°pidamente 401/403/404/ruta/params.
    """
    # Protecci√≥n opcional
    if API_KEY and API_PROTECT_LINKS:
        auth = request.headers.get("x-api-key", "").strip()
        if auth != API_KEY:
            return jsonify({"ok": False, "error": "unauthorized"}), 401

    title = (request.args.get("title") or "").strip()
    if not title:
        return jsonify({"ok": False, "error": "missing_title"}), 400

    t = (request.args.get("type") or "movie").strip().lower()
    if t in ("series", "tv"):
        t = "tv"
    else:
        t = "movie"

    title_clean = clean_title_for_search(title) or title

    base = (POSTER_API_URL or "").rstrip("/")
    headers = _poster_auth_headers()

    tried = []
    found = ""

    def _snip(resp: requests.Response) -> str:
        try:
            # No snip para imagen
            ct = (resp.headers.get("content-type") or "").lower()
            if ct.startswith("image/"):
                return ""
            s = resp.text or ""
            s = s.replace("\n", " ").replace("\r", " ").strip()
            return s[:800]
        except Exception:
            return ""

    def _record(method: str, url: str, resp: requests.Response):
        ct = (resp.headers.get("content-type") or "").split(";")[0].strip().lower()
        tried.append({
            "method": method,
            "url": url,
            "status": resp.status_code,
            "ct": ct,
            "snip": _snip(resp),
        })

    # GET candidates (rutas comunes + root)
    candidates = []
    if base:
        q_variants = [
            f"title={quote_plus(title_clean)}&type={t}",
            f"q={quote_plus(title_clean)}&type={t}",
            f"query={quote_plus(title_clean)}&type={t}",
            f"search={quote_plus(title_clean)}&type={t}",
            f"name={quote_plus(title_clean)}&type={t}",
        ]
        path_variants = [
            "/poster_img", "/posterimg", "/poster-image",
            "/poster", "/posters", "/image", "/img", "/search",
            "/tmdb/poster", "/tmdb/search", "/api/poster", "/v1/poster", "/v1/posters",
        ]
        for pth in path_variants:
            for qv in q_variants:
                candidates.append(f"{base}{pth}?{qv}")
        for qv in q_variants:
            candidates.append(f"{base}?{qv}")

    # Probamos GET (hasta 25)
    for u in candidates[:25]:
        try:
            r = SESSION.get(u, headers=headers, timeout=min(POSTER_TIMEOUT, 8), allow_redirects=True)
            _record("GET", u, r)
            ct = (r.headers.get("content-type") or "").split(";")[0].strip().lower()
            if r.status_code < 400 and ct.startswith("image/"):
                found = u
                break
        except Exception as e:
            tried.append({"method": "GET", "url": u, "error": (str(e) or "err")[:160]})

    # Probamos POST si no encontramos imagen directa (hasta 10*5)
    if base and not found:
        post_targets = [
            f"{base}/poster", f"{base}/posters", f"{base}/search",
            f"{base}/tmdb/search", f"{base}/api/poster", f"{base}/v1/poster", f"{base}/v1/posters",
            f"{base}",
        ]
        post_payloads = [
            {"title": title_clean, "type": t},
            {"q": title_clean, "type": t},
            {"query": title_clean, "type": t},
            {"search": title_clean, "type": t},
            {"name": title_clean, "type": t},
        ]
        post_headers = dict(headers)
        post_headers["Content-Type"] = "application/json"
        for target in post_targets[:10]:
            for payload0 in post_payloads:
                try:
                    r = SESSION.post(target, headers=post_headers, json=payload0, timeout=min(POSTER_TIMEOUT, 8), allow_redirects=True)
                    _record("POST", target, r)
                    ct = (r.headers.get("content-type") or "").split(";")[0].strip().lower()
                    if r.status_code < 400 and ct.startswith("image/"):
                        found = target
                        break
                except Exception as e:
                    tried.append({"method": "POST", "url": target, "error": (str(e) or "err")[:160]})
            if found:
                break

    # Hints
    hint = ""
    statuses = [x.get("status") for x in tried if isinstance(x, dict) and "status" in x]
    if not base:
        hint = "POSTER_API_URL no est√° configurado en Render."
    elif not POSTER_API_KEY:
        hint = "ZUPLO1_API_KEY/POSTER_API_KEY no est√° configurado."
    elif statuses and all(s in (401, 403) for s in statuses):
        hint = "Zuplo responde 401/403: revisa la API key o el esquema (Bearer vs x-api-key)."
    elif statuses and all(s == 404 for s in statuses):
        hint = "Todos 404: la ruta del gateway no coincide (revisa rutas en Zuplo/Developer Portal)."
    elif found:
        hint = "Se detect√≥ endpoint que devuelve image/*."
    else:
        hint = "No se detect√≥ image/*; revisa 'snip' para ver el error exacto (params/ruta/auth)."

    return jsonify({
        "ok": True,
        "title_clean": title_clean,
        "type": t,
        "poster_api_url_set": bool(base),
        "poster_api_key_set": bool(POSTER_API_KEY),
        "found_image_endpoint": found,
        "tried": tried,
        "hint": hint,
    }), 200

@app.route("/api/debug_cache")
def debug_cache():
    if API_KEY:
        auth = request.headers.get("x-api-key", "").strip()
        if auth != API_KEY:
            return jsonify({"ok": False, "error": "unauthorized"}), 401
    d = cache_col.find_one({"_id": "cache"}) or {}
    ts = d.get("ts")
    items = d.get("items", [])
    return jsonify({
        "items_count": len(items) if isinstance(items, list) else 0,
        "ts": ts.isoformat() if ts else None,
        "sync_seconds": SYNC_SECONDS,
        "webapp_url": WEBAPP_URL,
        "scrap_url": SCRAP_URL,
        "poster_api_url": POSTER_API_URL,
        "live_duration_normal_min": LIVE_DURATION_NORMAL_MIN,
        "live_duration_combat_min": LIVE_DURATION_COMBAT_MIN,
        "user_tz_default": USER_TZ
    }), 200


@app.route("/api/force_sync")
def force_sync():
    if API_KEY:
        auth = request.headers.get("x-api-key", "").strip()
        if auth != API_KEY:
            return jsonify({"ok": False, "error": "unauthorized"}), 401
    try:
        sincronizar_una_vez()
        d = cache_col.find_one({"_id": "cache"}) or {}
        items = d.get("items", [])
        return jsonify({"ok": True, "items": len(items) if isinstance(items, list) else 0}), 200
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500


# ---------------- WORKER (anti-409 robusto) ----------------
_worker_lock_fh = None


def acquire_worker_lock() -> bool:
    """
    Evita que 2 procesos del mismo deploy hagan polling a la vez (anti-409).
    NO elimina nada; solo blinda.
    """
    global _worker_lock_fh
    try:
        import fcntl  # linux
        lock_path = "/tmp/bcntv_worker.lock"
        fh = open(lock_path, "w")
        try:
            fcntl.flock(fh, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except BlockingIOError:
            log.error("‚ö†Ô∏è Worker lock ya est√° tomado. No iniciar√© polling/sync en este proceso.")
            return False
        fh.write(str(os.getpid()))
        fh.flush()
        _worker_lock_fh = fh
        log.warning(f"‚úÖ Worker lock adquirido pid={os.getpid()}")
        return True
    except Exception as e:
        log.warning(f"No se pudo adquirir lock (seguimos igual): {e}")
        return True


def delete_webhook_hard():
    """
    Asegura que no haya webhook activo (por si alguien lo activ√≥ antes).
    """
    if not TOKEN:
        return
    try:
        SESSION.get(
            f"https://api.telegram.org/bot{TOKEN}/deleteWebhook?drop_pending_updates=true",
            timeout=8
        )
    except Exception:
        pass


def set_webhook_hard(url: str):
    # Configura webhook v√≠a API (m√°s confiable que s√≥lo bot.set_webhook).
    if not TOKEN:
        return
    try:
        r = SESSION.get(
            f'https://api.telegram.org/bot{TOKEN}/setWebhook',
            params={'url': url, 'drop_pending_updates': 'true'},
            timeout=10
        )
        try:
            data = r.json()
        except Exception:
            data = {'ok': False, 'raw': r.text}
        if not data.get('ok'):
            log.error(f'‚ö†Ô∏è setWebhook fall√≥: {data}')
        else:
            log.warning('‚úÖ setWebhook OK')
    except Exception as e:
        log.error(f'‚ö†Ô∏è setWebhook exception: {e}')


def run_worker():
    if not acquire_worker_lock():
        while True:
            time.sleep(60)
    # ====== Lock global s√≥lo aplica al modo polling ======
    use_webhook = USE_WEBHOOK and bool(PUBLIC_URL)

    global_ok = False
    if not use_webhook:
        global_ok = acquire_global_polling_lock(GLOBAL_LOCK_TTL)
        if global_ok:
            threading.Thread(target=keepalive_global_polling_lock, args=(GLOBAL_LOCK_TTL,), daemon=True).start()
    else:
        log.warning('üåê USE_WEBHOOK=1: modo webhook activo (sin polling / sin getUpdates)')

    ok = test_telegram_token()

    if ok:
        try:
            if use_webhook:
                hook_url = f"{PUBLIC_URL}/telegram_webhook/{TG_WEBHOOK_SECRET}"
                # Asegura estado limpio y luego setea webhook
                delete_webhook_hard()
                set_webhook_hard(hook_url)
                log.warning(f"‚úÖ Webhook configurado: {hook_url}")
            else:
                bot.remove_webhook()
                delete_webhook_hard()
                time.sleep(1)
        except Exception as e:
            log.warning(f'webhook/polling setup warning: {e}')

    # Sync inicial
    try:
        sincronizar_una_vez()
    except Exception as e:
        log.error(f'Sync inicial fall√≥: {e}')

    # Sync loop
    threading.Thread(target=sincronizar_loop, daemon=True).start()

    # Si estamos en webhook, s√≥lo mantenemos vivo el worker para sync/broadcast (sin polling).
    if ok and use_webhook:
        while True:
            time.sleep(60)

    # Polling robusto (s√≥lo si tenemos lock global)
    if ok and global_ok:
        while True:
            try:
                bot.infinity_polling(skip_pending=True, timeout=60, long_polling_timeout=60)
            except Exception as e:
                msg = str(e)
                log.error(f'Polling cay√≥: {e}')
                if '409' in msg or 'Conflict' in msg:
                    delete_webhook_hard()
                    time.sleep(25)
                else:
                    time.sleep(5)
    else:
        # Sin lock global (o token inv√°lido): no hacemos polling, pero nos quedamos vivos
        while True:
            time.sleep(60)


if __name__ == "__main__":
    if WORKER_MODE:
        run_worker()
